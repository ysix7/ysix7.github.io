<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2Fp%2F0.html</url>
    <content type="text"><![CDATA[mean,imply,indicate,represent,denote,signify,suggest 这些动词均含有“表示……的意思”之意。 mean最普通用词。指文字或符号等所表示的各种明确的或含蓄的意义。 imply侧重用文字或符号表示的联想，暗示。 indicate指明显的表示。 represent指体现或代表。 denote指某一词字面或狭义的意思，或指某些符号或迹象的特指含义。 signify指用文字、说话或表情等表示单纯的意思。 suggest通常指暗含地、隐晦地表达意思。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2Fp%2F0.html</url>
    <content type="text"><![CDATA[各类表达 is of the order of 10^{-3}]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2Fp%2F0.html</url>
    <content type="text"><![CDATA[Foodie Travel艾格吃饱了：上海馆子 艾格吃饱了：杭州馆子]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2Fp%2F0.html</url>
    <content type="text"><![CDATA[October 生词 utmost to the utmost：极大限度的 the utmost noun.:最大程度的什么，可翻译为非常 drain 喝光，喝干； 使（精力、金钱等）耗尽； 使流出； 排掉水 e.g. I drain the pasta, then I share it out between two plates justfication 认为有道理 e.g. A justiﬁcation for the choice of SSIM as a good ﬁdelity measure instead of the traditional PSNR can be seen in Fig. 4. denote We denote apple by a. == a denote Apple. apple is denoted by a we denote by a apple denote that[表示，指示]: These signs denote that a crisis is approaching. Denote: Dark clouds denote a coming storm 词组The University security personnel are under instruction to discharge their duty having regard to this principle.指示下、履行职责、鉴于这个准则 Given the escalating turmoil and violence across Hong Kong, as far as: (1) as far as I’m concerned 在我看来。 就什么而论 (2) 就。。的程度=so far as as far as I know 据我所知；as far as the eye could see 目之所及；’as far as sb can/could 竭尽所能 not as far as I remember 我不记得 (3) 远至 I read as far as the third chapter. We walk as far as the edge of the forest. 语法 一个语法网站 when,while,as的区别 doing… while doing../ doing when do 当 转折]]></content>
  </entry>
  <entry>
    <title><![CDATA[数学_聚类算法]]></title>
    <url>%2Fp%2Fd0a2.html</url>
    <content type="text"><![CDATA[主成分分析(未完成) 作用 数据降维后可视化、数据压缩。 两种定义：最大化方差，使投影数据尽可能分散；最小化投影误差 最小化投影误差 最大化方差 Find $v_i$ 一些名词 投影误差： 谱聚类算法(问题)谱聚类算法 问题 特征值和特征向量的物理意义 谱聚类算法中 Reference https://blog.csdn.net/zhongkelee/article/details/44064401 《机器学习最后一章》]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2Fp%2F0.html</url>
    <content type="text"><![CDATA[香薰灯 秋天的香水]]></content>
  </entry>
  <entry>
    <title><![CDATA[数学_优化]]></title>
    <url>%2Fp%2F68e3.html</url>
    <content type="text"><![CDATA[性质coercive：]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2Fp%2F0.html</url>
    <content type="text"><![CDATA[日本签注 https://zhuanlan.zhihu.com/p/25683802 西北角攻略 http://f1.intocity.cn/share/dist/module/articleDetailMore.html?id=161 网上预约 https://www.vfsglobal.com/Japan/Hongkong/Book-an-Appointment.html 申请表格 https://www.hk.emb-japan.go.jp/jp/docs/2012_visa_app_info.pdf]]></content>
  </entry>
  <entry>
    <title><![CDATA[摄影_Lightroom调色]]></title>
    <url>%2Fp%2F5c30.html</url>
    <content type="text"><![CDATA[莫兰迪色 x 城市风光]]></content>
  </entry>
  <entry>
    <title><![CDATA[摄影_VSCO滤镜]]></title>
    <url>%2Fp%2Fd5da.html</url>
    <content type="text"><![CDATA[Reference：五款滤镜推荐 AV8]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2Fp%2F0.html</url>
    <content type="text"><![CDATA[checkpoint = utility.checkpoint(args) self.loader_train = MSDataLoader( args, trainset, batch_size=args.batch_size, shuffle=True, pin_memory=not args.cpu ) while not t.terminate(): t.train() t.test() def terminate(self): if self.args.test_only: self.test() return True else: epoch = self.scheduler.last_epoch + 1 return epoch &gt;= self.args.epochs 训练的例子参数设置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147cd Train/# MSRN x2 LR: 48 * 48 HR: 96 * 96python main.py --template MSRN --save MSRN_X2 --scale 2 --reset --save_results --patch_size 96 --ext sep_reset# MSRN x3 LR: 48 * 48 HR: 144 * 144python main.py --template MSRN --save MSRN_X3 --scale 3 --reset --save_results --patch_size 144 --ext sep_resetif args.template.find('MSRN') &gt;= 0: args.model = 'MSRN' args.n_blocks = 8 args.n_feats = 64 args.chop = True parser.add_argument('--debug', action='store_true', help='Enables debug mode')parser.add_argument('--template', default='.', help='You can set various templates in option.py')# Hardware specificationsparser.add_argument('--n_threads', type=int, default=8, help='number of threads for data loading')parser.add_argument('--cpu', action='store_true', help='use cpu only')parser.add_argument('--n_GPUs', type=int, default=4, help='number of GPUs')parser.add_argument('--seed', type=int, default=1, help='random seed')# Data specificationsparser.add_argument('--dir_data', type=str, default='dataset', help='dataset directory')parser.add_argument('--dir_demo', type=str, default='test', help='demo image directory')# data_train, 'DIV2K'parser.add_argument('--data_train', type=str, default='DIV2K', help='train dataset name')# data_test, parser.add_argument('--data_test', type=str, default='DIV2K', help='test dataset name')parser.add_argument('--data_range', type=str, default='1-800/896-900', help='train/test data range')parser.add_argument('--ext', type=str, default='sep', help='dataset file extension')parser.add_argument('--scale', type=str, default='4', help='super resolution scale')# patch_sizeparser.add_argument('--patch_size', type=int, default=128, help='output patch size')parser.add_argument('--rgb_range', type=int, default=255, help='maximum value of RGB')parser.add_argument('--n_colors', type=int, default=3, help='number of color channels to use')parser.add_argument('--chop', action='store_true', help='enable memory-efficient forward')parser.add_argument('--no_augment', action='store_true', help='do not use data augmentation')# Model specifications 怎么找到这个模型？parser.add_argument('--model', default='EDSR', help='model name')parser.add_argument('--act', type=str, default='relu', help='activation function')parser.add_argument('--pre_train', type=str, default='.', help='pre-trained model directory')parser.add_argument('--extend', type=str, default='.', help='pre-trained model directory')parser.add_argument('--n_resblocks', type=int, default=16, help='number of residual blocks')parser.add_argument('--n_feats', type=int, default=64, help='number of feature maps')parser.add_argument('--res_scale', type=float, default=1, help='residual scaling')parser.add_argument('--shift_mean', default=True, help='subtract pixel mean from the input')parser.add_argument('--dilation', action='store_true', help='use dilated convolution')parser.add_argument('--precision', type=str, default='single', choices=('single', 'half'), help='FP precision for test (single | half)')# Option for Residual channel attention network (RCAN)parser.add_argument('--n_resgroups', type=int, default=1, help='number of residual groups')parser.add_argument('--reduction', type=int, default=16, help='number of feature maps reduction')# Training specificationsparser.add_argument('--reset', action='store_true', help='reset the training')parser.add_argument('--test_every', type=int, default=1000, help='do test per every N batches')parser.add_argument('--epochs', type=int, default=1000, help='number of epochs to train')parser.add_argument('--batch_size', type=int, default=16, help='input batch size for training')parser.add_argument('--split_batch', type=int, default=1, help='split the batch into smaller chunks')parser.add_argument('--self_ensemble', action='store_true', help='use self-ensemble method for test')parser.add_argument('--test_only', action='store_true', help='set this option to test the model')parser.add_argument('--gan_k', type=int, default=1, help='k value for adversarial loss')# Optimization specificationsparser.add_argument('--lr', type=float, default=1e-4, help='learning rate')parser.add_argument('--lr_decay', type=int, default=200, help='learning rate decay per N epochs')parser.add_argument('--decay_type', type=str, default='step', help='learning rate decay type')parser.add_argument('--gamma', type=float, default=0.5, help='learning rate decay factor for step decay')parser.add_argument('--optimizer', default='ADAM', choices=('SGD', 'ADAM', 'RMSprop'), help='optimizer to use (SGD | ADAM | RMSprop)')parser.add_argument('--momentum', type=float, default=0.9, help='SGD momentum')parser.add_argument('--beta1', type=float, default=0.9, help='ADAM beta1')parser.add_argument('--beta2', type=float, default=0.999, help='ADAM beta2')parser.add_argument('--epsilon', type=float, default=1e-8, help='ADAM epsilon for numerical stability')parser.add_argument('--weight_decay', type=float, default=0, help='weight decay')# Loss specificationsparser.add_argument('--loss', type=str, default='1*L1', help='loss function configuration')parser.add_argument('--skip_threshold', type=float, default='1e6', help='skipping batch that has large error')# Log specificationsparser.add_argument('--save', type=str, default='test', help='file name to save')parser.add_argument('--load', type=str, default='.', help='file name to load')parser.add_argument('--resume', type=int, default=0, help='resume from specific checkpoint')parser.add_argument('--save_models', action='store_true', help='save all intermediate models')parser.add_argument('--print_every', type=int, default=100, help='how many batches to wait before logging training status')parser.add_argument('--save_results', action='store_true', help='save output results') Template.py 12 datasetdataset/DIV2K/DIV2K_train_HR; dataset/DIV2K/DIV2K_train_LR_bicubic 设定模型12module = import_module('model.' + args.model.lower()) # import model.msrn.pyself.model = module.make_model(args).to(self.device) # msrn.py的make_model: 返回该文件model]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2Fp%2F0.html</url>
    <content type="text"><![CDATA[名词国内A股 H股：在香港上市的国企股。如，中国银行、工商银行、中国人寿。 股票是什么股票除了股息，还有分红，这是另一个话题，有空再讲。 股票其实是一个“吸金手段”，公司用较高的股息，拿股票换取现金，以求发展。 股票价格股票价格的形成: 集合竞价(开盘价)+连续竞价：最大成交量，来获得最大佣金 集合竞价最后的价格，由“王子灰姑娘”的交易方法决定，具体会因为不同的证券市场而有不同。 集合竞价的交易时间 股票的种类http://www.csrc.gov.cn/pub/gansu/xxfw/tzzzsyd/200608/t20060821_92670.htm A股和H股的区别 问题H股：指的是指大陆上市公司在香港卖的股票？还是大陆公司又在香港上市，发行股票？那么香港本地上市的的本地公司算什么股票类型？]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2Fp%2F0.html</url>
    <content type="text"><![CDATA[参考 文献综述参考自链接： 深度学习在图像去噪方面最近有哪些进展，与传统方法相比效果如何, 作者：徐君 对deep learning很好的总结：flyywh/Image-Denoising-State-of-the-art。这里可以找到很全的各种state-of-the-art算法的链接。 高斯噪声去噪文献综述 (2009 NIPS) Jain, Viren, and Sebastian Seung. “Natural image denoising with convolutional networks.” Advances in neural information processing systems. 2009. 效果一般。只是比BLS-GSM, FoE的PSNR稍微好一点。 (2012 MLP) Burger, Harold C., Christian J. Schuler, and Stefan Harmeling. “Image denoising: Can plain neural networks compete with BM3D?.” 2012 IEEE conference on computer vision and pattern recognition. IEEE, 2012. 网络都不是很深，只有3层左右。效果也就是和BM3D差不多的水平。 2014-2015年，出现了一些判别学习的方法出现，比如CSF和TNRD，这些方法的PSNR/SSIM基本超过了BM3D，在速度上也不分伯仲。 (2014 CSF) Schmidt, Uwe, and Stefan Roth. “Shrinkage fields for effective image restoration.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2014. (2015 TNRD) Chen, Yunjin, Wei Yu, and Thomas Pock. “On learning optimized reaction diffusion processes for effective image restoration.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2015. 然后Shrinkage Field和TNRD这两篇文章是属于对sparse coding的传统iterative method进行unfolding成为feed forward network的工作，我觉得做得很赞。我最近的工作也在follow这里两个工作。我觉得这种approach可以给更多的inverse problem提供一种principled approach，便于更好地更理智地设计网络结构来解决一切特殊的imaging问题。我近期的工作在扩展这个思路来解决一些高维度问题。作者：Bihan Wen 链接：https://www.zhihu.com/question/66359919/answer/241936523 2016-2017 (2016 NIPS) Mao, Xiaojiao, Chunhua Shen, and Yu-Bin Yang. “Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections.” Advances in neural information processing systems. 2016. (2017 DnCNN) Zhang, Kai, et al. “Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising.” IEEE Transactions on Image Processing 26.7 (2017): 3142-3155. code ResNet+BN简单有效。 NTIRE 2019 Challenge on Real Image Denoising: Methods and Results 真实噪声去噪文献综述 (2016 CVPR) Nam, Seonghyeon, et al. “A holistic approach to cross-channel image noise modeling and its application to image denoising.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016. 用之前提到的MLP训练得到真实噪声图里的噪声分布情况,然后用 Bayesian Nonlocal Means Filter算法做去噪的。 (2017 ICCV) Xu, Jun, et al. “Multi-channel weighted nuclear norm minimization for real color image denoising.” Proceedings of the IEEE International Conference on Computer Vision. 2017. 考虑到真实噪声图里R,G,B三个通道理的噪声分布不一样的情形,并基于传统的低秩模型提出了一个新的模型 (2019 CVPR 左老师组合)Guo, Shi, et al. “Toward convolutional blind denoising of real photographs.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019. (Roth老师组发表在NeurIPS2018的工作paper+code) Tobias Plötz and Stefan Roth, Neural Nearest Neighbors Networks, Advances in Neural Information Processing Systems (NeurIPS), 2018 (2019 CVPR) Brooks, Tim, et al. “Unprocessing images for learned raw denoising.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019. 难点1: 数据集 多次拍摄取平均作为ground truth (2016 A holistic approach ….) 用该方法做了15张512*512图片 (2018 PolyU dataset) Xu, Jun, et al. “Real-world noisy image denoising: A new benchmark.” arXiv preprint arXiv:1804.02603 (2018).github 用低iso和高iso来拍摄同一场景 (2017 德国Roth组) Plotz, Tobias, and Stefan Roth. “Benchmarking denoising algorithms with real photographs.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017. 一共50张大图，每张分割成20张512*512的小图，共1000张 难点2: 噪声分布真是图像的噪声和信号想关，难以用一个分布来模拟。 (2018 ECCV) Xu, Jun, Lei Zhang, and David Zhang. “A trilateral weighted sparse coding scheme for real-world image denoising.” Proceedings of the European Conference on Computer Vision (ECCV). 2018. 提出了对不同的图像通道和不同的图像快都采取不同方差的高斯分布去拟合真实噪声。 噪声级别估计文献综述 (2013) Liu, Xinhao, Masayuki Tanaka, and Masatoshi Okutomi. “Single-image noise level estimation for blind denoising.” IEEE transactions on image processing 22.12 (2013): 5226-5237. https://arxiv.org/pdf/1712.03381.pdf Noise Level Estimation for Overcomplete Dictionary Learning Based on Tight Asymptotic Bounds https://www.itread01.com/content/1544715003.html 基於pytorch的噪聲估計網路 问题 构造噪声level的时候如如何]]></content>
  </entry>
  <entry>
    <title><![CDATA[深度_卷积神经网络]]></title>
    <url>%2Fp%2F6a2b.html</url>
    <content type="text"><![CDATA[通用近似定理只有参数足够多，一层网络可以近似任何函数。但是训练难度大，维度可能💥。 卷积 卷积的特点：局部连接、权值共享、空间或时间上的采样 卷积的作用：减少参数、提取局部不变性特征 Motivation：一个视神经元的感受野，来激活该神经元 互相关是什么 滑动步长和填充 不同滑动步长和填充的作用 根据输出长度：窄卷积、等宽卷机 卷积网络的结构用卷机层代替全连接网络。 Reference: CNN中卷积层的计算细节、卷积层与池化层、池化层的实现、反向传播 卷机层的特点：局部感知、参数共享、池化 汇聚层/池化层/pool层减少神经元个数。池化层的输入一般来源于上一个卷积层，主要作用是提供了很强的鲁棒性（例如max-pooling是取一小块区域中的最大值，此时若此区域中的其他值略有变化，或者图像稍有平移，pooling后的结果仍不变），并且减少了参数的数量，防止过拟合现象的发生。池化层一般没有参数，所以反向传播的时候，只需对输入参数求导，不需要进行权值更新。 bottle layer/1*1卷积？降维 Dropout?https://zhuanlan.zhihu.com/p/38200980 https://blog.csdn.net/stdcoutzyx/article/details/49022443 反卷积转置卷机：低维特征到高维特征？反卷积 输出维度的大小： 函数： 123456789101112class torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1)##in_channels(int) – 输入信号的通道数out_channels(int) – 卷积产生的通道数kerner_size(int or tuple) - 卷积核的大小stride(int or tuple,optional) - 卷积步长，即要将输入扩大的倍数。padding(int or tuple, optional) - 输入的每一条边补充0的层数，高宽都增加2*paddingoutput_padding(int or tuple, optional) - 输出边补充0的层数，高宽都增加paddinggroups(int, optional) – 从输入通道到输出通道的阻塞连接数bias(bool, optional) - 如果bias=True，添加偏置dilation(int or tuple, optional) – 卷积核元素之间的间距 空洞卷机目的增加输出单元的感受野，通过增加空洞来增加大小。 (如何增大感受野：增大卷机核大小，增加深度，再卷机之前进行汇聚操作) concat/ f= [a,b,c,d]12outputs = [branch1, branch2, branch3, branch4]return torch.cat(outputs, 1) # 按照维数1进行拼接，按列拼接。 batch normalization?上采样 在应用在计算机视觉的深度学习领域，由于输入图像通过卷积神经网络(CNN)提取特征后，输出的尺寸往往会变小，而有时我们需要将图像恢复到原来的尺寸以便进行进一步的计算(e.g.:图像的语义分割)，这个采用扩大图像尺寸，实现图像由小分辨率到大分辨率的映射的操作，叫做上采样(Upsample)。 上采样有3种常见的方法：双线性插值(bilinear)，反卷积(Transposed Convolution)，反池化(Unpooling)，我们这里只讨论反卷积。这里指的反卷积，也叫转置卷积，它并不是正向卷积的完全逆过程，用一句话来解释： 参考 https://zhuanlan.zhihu.com/p/67907490 https://zhuanlan.zhihu.com/p/48501100 插值法 最近邻插值(Nearest neighbor interpolation) 双线性插值(Bi-Linear interpolation) 双立方插值(Bi-Cubic interpolation) 卷积网络中的反向传播 更新W 更新b 计算反向误差 池化层的求导：池化层一般没有参数，所以反向传播的时候，只需对输入参数求导，不需要进行权值更新。 max-pooling mean-pooling 几个重要的卷积神经 CNN发展至今，已经有很多变种，其中有几个经典模型在CNN发展历程中有着里程碑的意义，它们分别是：LeNet、Alexnet、Googlenet、VGG、DRL等 LeNet, 1998 参考文献 Ann LeCun 在1998年发表了关于LeNet的经典论文《Gradient-Based Learning Applied to Document Recognition 》用于识别支票上的手写字体 大话CNN经典模型：LeNet 原论文代码复现: tiny-dnn/tiny-dnn、简化版本(主要体现在评论区说的s2到c3的过程上, 原paper之所以那样实现，也是受限于当时的计算资源。现简化版本也符合pytorch的实现框架。) 网络结构 S2，S4: 在汇聚层使用非线性激活函数$Y^{\prime d}=f\left(w^{d} \cdot Y^{d}+b^{d}\right)$。因此，在汇聚层也有两个参数需要训练。 C3层使用了一个局部连接： ​ F6层：84个神经元，可组成一张7*12的特征图像，可以对不同的字体进行编码 ​ 输出层： $y_{i}=\sum_{i}\left(x_{j}-w_{i j}\right)^{2}$ 激活函数： AlexNet, 2012 Reference：论文解读、 AlexNet赢得了2012 年 ImageNet图像分类竞赛的冠军 突破点：BigData+GPU 进行并行训练，采用了 ReLU 作为非线性激活函数，使用 Dropout 防止过拟合，使用数据增强来提高模型准确率等。 激活函数：ReLU，线性整流函数（Rectified Linear Unit, ReLU），又称修正线性单元 $\max \left(0, \mathbf{w}^{T} \mathbf{x}+b\right)$ AlexNet中的Pooling是重叠的，每次取3 * 3的像素做Max-Pooling，然后每次只越过2格，这样每个Pooling之间都会有一定得重叠，有利于更好地克服过拟合 减少过拟合 Data Augmentation Dropout Googlenet, 2014 Reference: 代码、Inception的发展 won ImageNet 2014. Inception结构 网络结构 两个辅助的softmax分支，作用有两点，一是为了避免梯度消失，用于向前传导梯度。反向传播时如果有一层求导为0，链式求导结果则为0。二是将中间某一层输出用作分类，起到模型融合作用。 VGG(待补充)ResNet Reference He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. InProceedings of the IEEE conference on computer vision and pattern recognition 2016 (pp. 770-778). Identity Mappings in Deep Residual Networks 1202层的ResNet出现了过拟合的问题，有待进一步改进。第二年，何的团队分析了ResNet成功的关键因素——residual block背后的算法，并对residual block以及after-addition activation进行改进，通过一系列的ablation experiments验证了，在residual block和after-addition activation上都使用identity mapping（恒等映射）时，能对模型训练产生很好的效果，通过这项改进，也成功的训练出了具有很好效果的ResNet-1001。 为什么要skip connect，深层网络难以训练的根本原因不是提速消失 为什么Residual网络效果好的原因，有很多论文在分析 何恺明的tutorial(https://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf) ResNet, 2015, 微软亚洲研究院的何凯明等人提出 主要贡献：网络层数从32层提到152层 特点： 网络较瘦，控制了参数数量； 存在明显层级，特征图个数逐层递进，保证输出特征表达能力； 使用了较少的池化层，大量使用下采样，提高传播效率； 没有使用Dropout，利用BN和全局平均池化进行正则化，加快了训练速度； 层数较高时减少了3x3卷积个数，并用1x1卷积控制了3x3卷积的输入输出特征图数量，称这种结构为“瓶颈”(bottleneck)。 残差单元： 克服了梯度消失的问题。DRN怎样解决了梯度消失问题？ Unet， 网络结构 右边有上采样： 上采样的方法 问题 叠加的时候concat为什么在维度1 https://blog.csdn.net/u012193416/article/details/79479935 https://blog.csdn.net/qq_39709535/article/details/80803003]]></content>
  </entry>
  <entry>
    <title><![CDATA[深度_可解释网络]]></title>
    <url>%2Fp%2F43c9.html</url>
    <content type="text"><![CDATA[参考文献 The Building Blocks of Interpretability https://arxiv.org/abs/1901.02413 https://arxiv.org/abs/1602.04938 https://arxiv.org/abs/1806.10574 http://www.vision.ee.ethz.ch/ntire18/ 会议链接]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2Fp%2F0.html</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title></title>
    <url>%2Fp%2F0.html</url>
    <content type="text"><![CDATA[K-means 字典学习K-SVD 字典学习]]></content>
  </entry>
  <entry>
    <title><![CDATA[tight frame去噪和KSVD]]></title>
    <url>%2Fp%2F9719.html</url>
    <content type="text"><![CDATA[ReferenceCai, J. F., Ji, H., Shen, Z., &amp; Ye, G. B. (2014). Data-driven tight frame construction and image denoising. Applied and Computational Harmonic Analysis, 37(1), 89-105. 算法框架和KSVD的关系 核心比较 与K-SVD的比较 There are images on which the K-SVD method performed better and there are some on which our approaches performed better. Overall, the performances of our proposed method and the K-SVD method are comparable in terms of PSNR value, and so is the visual quality.]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深度_Unet]]></title>
    <url>%2Fp%2Fc0c5.html</url>
    <content type="text"><![CDATA[图像语义分割 图像分割综述 U-net Reference Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. “U-net: Convolutional networks for biomedical image segmentation.” International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015. U-Net论文、U-Net论文笔记、U-Net论文笔记、U-Net论文笔记知乎 网络结构 各层作用 conv： pool： up-conv： conv 1*1： Reference: CNN中卷积层的计算细节、卷积层与池化层、池化层的实现、反向传播 up，downsample，deconvolution， batch normalization… normalization有很多方法，VGG-Net，Stride 反卷积：https://blog.csdn.net/Fate_fjh/article/details/52882134 Code链接 代码分析 Attention U-net Key Idea: Produce foreground masks via feature maps to filter out backgournd information 关注点在前景，比Unet多了attention gate Unet Attention Unet Regularized UNet for Automated Pancreas Segmentation不同Unet的对比 Holistically-nested Convolutional Neural NetworksHierarchical 3D fully Convolutional Networks for Multi-organ SegmentationA Fixed-Point Model for Pancreas Segmentation in Abdominal CT ScansA 3D Coarse-to-Fine Framework for Volumetric Medical Image SegmentationFully Automated Pancreas Segmentation with Two-stage 3D Convolutional Neural Networks胰脏分割从coarse到fine Attention U-net： 关注点在前景，比Unet多了attention gate]]></content>
  </entry>
  <entry>
    <title><![CDATA[咖啡]]></title>
    <url>%2Fp%2Facef.html</url>
    <content type="text"><![CDATA[咖啡的配方打奶https://www.youtube.com/watch?v=-kvY2AmbLOE 拿铁、拉花 https://www.youtube.com/watch?v=h3S_ClRFxX4]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[网络可视化]]></title>
    <url>%2Fp%2Fe804.html</url>
    <content type="text"><![CDATA[tensorboard利用tensorboard画图 通过logger文件可视化训练过程、官网、别人写的tensorflow下比较具体的可视化 出现的问题 端口冲突通过指定端口解决： 12tensorboard --logdir=/tmp --port=8008 #绝对路径tensorboard --logdir=./tmp --port=8008 #相对路径 tensorbard命令无法找到： 1python3 -m tensorboard.main --logdir=~/my/training/dir 进入目录： 123pip show tensorflowcd /home/abc/xy/.local/lib/python2.7/site-packagespython main.py --logdir=/path/to/log_file/ 路径的名称要小心，路径得是根目录, 不需要引号 1yyfang@mai:~$ ge/ERRNet_Code/logs --port=8008 可视化 1234567891011121314151617181920212223242526272829303132if iteration % 100 == 0: print("===&gt; Epoch[&#123;&#125;](&#123;&#125;/&#123;&#125;): Loss: &#123;:.6f&#125;".format(epoch, iteration, len(training_data_loader),loss.data.item())) info = &#123; 'loss': loss.data.item()&#125; itera = (epoch-1)*len(training_data_loader)+iteration for tag, value in info.items(): logger.scalar_summary(tag, value, itera) for tag, value in model.named_parameters(): # print(value.grad) logger.histo_summary(tag, to_np(value), itera) logger.histo_summary(tag+'/grad', to_np(value.grad), iteration) images = input * 255. # ?[0,1]?????[0,255]?? images[images &lt; 0] = 0 images[images &gt; 255.] = 255. a =to_np(images.view(-1, 64, 64)[:8]) # info_input = &#123;'input': to_np(images.view(-1, 64, 64)[:2])&#125; imagelabel = label * 255. # ?[0,1]?????[0,255]?? imagelabel[imagelabel &lt; 0] = 0 imagelabel[imagelabel &gt; 255.] = 255. b = to_np(imagelabel.view(-1, 64, 64)[:8]) c = np.hstack((a,b)) # print(images) info_label = &#123;'inpput/label': c&#125; for tag, images in info_label.items(): logger.image_summary(tag, images, itera) # for tag, images in info_label.items(): # logger.image_summary(tag, images, iteration) if iteration % 5000 == 0: number = opt.number save_checkpoint_iter(model, number) opt.number += 1]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2Fp%2F0.html</url>
    <content type="text"><![CDATA[K-SVD K-means SVD 论文 [11] M. Elad, M. Ahron, Image denoising via sparse and redundant representations over learned distionaries, IEEE Trans. on Image Processing 54 (12) (2006) 3736–3745. 2013_tight frame比较的这篇 算法框架 算法框架 比较 K-SVD和tight frame的关系 psnr, ksvd最佳，视觉效果，adaptive tiggt frame更高。两者比较结果在后者的文章中阐述了。 Reference [11] M. Elad, M. Ahron, Image denoising via sparse and redundant representations over learned distionaries, IEEE Trans. on Image Processing 54 (12) (2006) 3736–3745. 2013_tight frame比较的这篇 最先提出KSVD Aharon, M., Elad, M., &amp; Bruckstein, A. (2006). K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation. IEEE Transactions on signal processing, 54(11), 4311-4322. 论文的解读以及应用 Eric的个人网页 https://elad.cs.technion.ac.il/software/ KSVD的网页: http://www.cs.technion.ac.il/~ronrubin/software.html Matlab tool中的介绍 http://www.ux.uis.no/~karlsk/dle/ Code https://github.com/Deepayan137/K-svd https://github.com/jbhuang0604/SelfSimSR/tree/master/Lib/KSVD]]></content>
  </entry>
  <entry>
    <title><![CDATA[服务器和本地文件共享]]></title>
    <url>%2Fp%2Ff933.html</url>
    <content type="text"><![CDATA[服务器上访问本地文件(局域网)尚未实现 本地电脑访问服务器 方法一：安装samba服务 Ubuntu 16.04安装配置Samba服务 Mac上的连接方法 服务器上配置samba服务 方法二[最简易]：sshfs客户端直接安装 安装FUSE for Mac和SSHFS客户端 下载链接：https://osxfuse.github.io/ 测试sshfs 使用sshfs进行挂载 创建本地文件夹Server sshfs yyfang@mai.math.cuhk.edu.hk:/home/yyfang Server 终止挂载 umount Server 为了简化命令，方便挂载，改一下~/.ssh/config 1open ~/.ssh/config 在文件中添加： 1234Host []//ServerHostname ]mai.math.cuhk.edu.hkUser yyfangPort 22 简化后的命令 12ssh Serversshfs Server: Server 方法三：sshfs，按照github的步骤安装 Reference 如何从Mac OS访问Ubuntu服务器上的共享文件夹？ 利用 ssh 的用户配置文件 config 管理 ssh 会话 问题 如果出现服务器没反应的情况 1.首先 123 sudo diskutil umount force 或者 sudo umount Server 2.检查网络，看看其他ssh服务能否成功 3.重新挂载 出现无法umount的情况 https://github.com/osxfuse/osxfuse/issues/45 出现pn-206-40的情况（未查明） 12Last login: Thu Sep 5 23:51:46 on ttys001pn-206-40:~ yyf$ c 出现 Input/output error]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>服务器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Graph model]]></title>
    <url>%2Fp%2F3b7a.html</url>
    <content type="text"><![CDATA[Opening今天我讲的内容是第十六章，图模型。主要包含三个基本问题：图模型的结构、推断、学习。表示问题，利用图结构表示彼岸两之间的依赖关系 推断问题，推断预测变量的后验分布。在观测到部分变量e时候，计算其它变量的某个子集 q = {q 1 , q 2 , · · · , q n } 的后验概率 p(q|e)。以及学习问题, 图结构的学习和图中参数的学习。 在前面几周，已经介绍了一些推断的方法，以及参数的学习方法，例如EM算法。今天，我将首先补充一下图结构的知识。并通过举例一些实际应用例子，来看图结构如何协同推断和学习来解决一个问题，如同如文本分类、以及图像去噪。 有向图结构 图结构分为无向图和有向图 有向图：Directed Graphical model，也称为贝叶斯网络（Bayesian Network），或信念网络（Belief Network） 无向图： 有向图的联合分布 以图(a)为例，p(x1,x2,x3,x4)=p(x1)p(x2|x1)p(x3|x1,x2)p(x4|x1,x2,x3 )。 根据性质$X_{2} \perp X_{3} | X_{1}$, $p\left(x_{2} | x_{1}, x_{3}\right)=p\left(x_{2} | x_{1}\right)$, 以及图结构上的独立性， 我们有p(x2|x1)=p(x2|x1,x3) ，p(x4|x3,x2)=p(x4|x3,x2,x1) 因此右边可以简化为p(x1)p(x2|x1 )p(x3|x1)p(x4|x2, x3) For any acyclic Bayesian network G, the joint probability distribution of X can be decomposed into a multiplicative form of the local conditional probability of each random variable X. In this graph, then the 对于任意一个无环贝叶斯网络G，X 的联合概率分布可以分解为每个随机变量 $X_k$ 的局部条件概率的连乘形式 p(\mathbf{x})=\prod_{k=1}^{K} p\left(x_{k} | \mathbf{x}_{\pi_{k}}\right)其中$x_{\pi k}$表示节点k的父节点。P(Xk|Xπk)是局部条件概率 根据这个公式：我们遍历图（a）中的每一个节点，即可以得到联合分布 p(x)=p(x1)p(x3|x1)p(x2|x1)p(x4|x3,x2). ——和我们之前的结果一样。 现在我们来讨论用一个图结构来表示概率分布的优点。 假设X1 , X2 , X3 , X4 是二值变量，在没有图中变量依赖关系的情况下，可以用一个联合概率表来显式地记录每一种取值的概率P(X)，共需要2^4 −1 = 15个参数。如果我们用4 个表格来记录这 4 个局部条件概率的乘积，只需要 1 + 2 + 2 + 4 = 9 个独立参数。 因此当概率模型中的变量数量比较多时，其条件依赖关系也比较复杂。使用图结构的方式可以将概率模型可视化，，以一种直观、简单的方式描述出随机变量之间的条件独立性的性质，并可以将一个复杂的联合概率模型分解为一些简单条件概率模型的组合。 Therefore, when the number of variables in the probability model is relatively large, the conditional dependence is also complicated. The use of graph structure can visualize the probability model, describe the nature of conditional independence between random variables in an intuitive and simple way, and decompose a complex joint probability model into some simple conditional probability models. combination. 几种常见的有向图（不讲） 很多经典的机器学习模型可以使用有向图模型来描述，比如朴素贝叶斯分类器、隐马尔可夫模型、深度信念网络等。各自具有特定的结构，适用于不同的任务。 有向图—文本分类 图结构：一个常用的有向图结构，朴素贝叶斯模型。 参数的学习 后验分布可以分解为 这里我要强调一下两个概念。生成模型和判别模型的区别。 生成模型和判别模型的区别我们经常提到的生成模型和判别模型。我们可能通常错误的以为生成模型就是图像应用中数据重建的意思，其实不然。生成模型除了重建也可以用来判别。 简单来说。生成模型是计算一个p(x,y)的联合分布，而判别模型是生成p(y|x)， 两个模型的应用。生成模型和判别模型都可以用来进行分类问题。 两个模型的建模以及我的总结 有向图—高斯混合模型高斯混合模型（Gaussian Mixture Model，GMM)。假设样本 x 是从 K 个高斯分布中生成的。 高斯混合模型的概率密度函数为：$p(x)=\sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(x | \mu_{k}, \sigma_{k}\right)$ 有向图结构的定义：Y-&gt;X 在这个图中，我们定义$P(X|Y=k)=\mathcal{N}\left(x | \mu_{k}, \sigma_{k}\right)$, $P(Y=k)=\pi_k, \sum_{k=1}^{K} \pi_{k}=1$. 根据这个图：我们可以得到x的边际概率：$p(x)=\sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(x | \mu_{k}, \sigma_{k}\right)$ 参数估计：用最大似然的方法来进行参数估计，给定N 个训练样本D = {x (i) }, 1 ≤ i ≤ N，其训练集的对数边际似然为 $\mathcal{L}(\mathcal{D} | \theta)=\frac{1}{N} \sum_{i=1}^{N}\left(\log p\left(\mathbf{x}^{(i)}, \theta\right)\right.$,并通过EM算法来求解获得参数$u_k,\sigma_k,\pi_k$, 推断：计算后验分布 p(Y=k|x)$\pi_{k} \mathcal{N}\left(x^{(n)} | \mu_{k}, \sigma_{k}\right)$ 无向图结构 应用场景 我们可以看到，在有向图中，通常存在明确方向上的因果关系。当相互的作用并没 有本质性的指向，或者是明确的双向相互作用时，使用无向模型更加合适。作为一个这种情况的例子，假设我们希望对三个二值随机变量建模。X1-X2-X3。其中X1，X2，X3分别代表你的室友，你和你的同事是否😷。我们假设，你的室友和同事 并不认识，所以他们不太可能直接相互传染一些疾病，因而我们的模型中两者之间没有直接的连接。然而，你传染给你的室友和你的室友传染给你都是非常容易的，所以模型不存在一个明确的单向箭头。并且，你的室友和你很有可能其中之一将感冒传染给你，然后通过你再传染给了另一个人。 现在我们来考虑三者健康状况的联合分布。p(X1,X2,X3) 无向图的联合分布的分解方式 无向图模型，也称为马尔可夫随机场（Markov Random Field，MRF）或 马尔可夫网络（Markov Network） 无向图的联合分布方式：对于一个马尔可夫随机场G,当p(x)可以表示为,一系列定义在最大团上的非负函数的乘积形式,即p(\mathbf{x})=\frac{1}{Z} \prod_{c \in \mathcal{C}} \phi_{c}\left(\mathbf{x}_{c}\right)。其中：最大团（Maximal Clique）就是其中的点是全连接的子集，并且无法被一个更大的团包含。对于 图中的每一个团C，一个 因子（factor）ϕ(C)(也称为 团势能（clique potential)，衡量了团中变量每一种可能的联合状态所对应的密切程度，因而必须为0。为了归一化 例子中的计算方法 在我们这个模型中，我们可以为你和你的室友定义这样一个势能函数。 状态为 1 代表了健康的状态，相对的状态为 0 则表示不好的健康状态（即感染 了感冒）。你们两个通常都是健康的，所以对应的状态拥有最高的密切程度。两个人 中只有一个人是生病的密切程度是最低的，因为这是一个很罕见的状态。两个人都生病的状态（通过一个人来传染给了另一个人）有一个稍高的密切程度，尽管仍然 不及两个人都健康的密切程度。 同样地，我们可以为你和你的同事定义一个。 现在我们可以得到非归一化的概率分布。如hc=0，hy=1，hx=1的情况下，p；hc=0，hy=1，hx=1的情况下，p。和我们预设的情形相符合。 无向图中联合分布的能量表达方法 现在我们返回到这个联合分布的定义。我们注意到，为了有效定义每一种情况，势能函数必须大于0. 由于势能函数必须为正的，因此我们一般定义为$\phi_{c}\left(\mathbf{x}_{c}\right)=\exp \left(-E_{c}\left(\mathbf{x}_{c}\right)\right)$，其中E(xc)为能量函数（energy function) 。因此，无向图上定义的概率分布可以表示为： \begin{aligned} P(\mathbf{x}) &=\frac{1}{Z} \prod_{c \in \mathcal{C}} \exp \left(-E_{c}\left(\mathbf{x}_{c}\right)\right) \\ &=\frac{1}{Z} \exp \left(\sum_{c \in \mathcal{C}}-E_{c}\left(\mathbf{x}_{c}\right)\right)\\&=\frac{1}{Z} \exp \left(-E\left(\mathbf{x}\right)\right) \end{aligned}这种形式的分布又称为玻尔兹曼分布（Boltzmann Distribution）。任何一个无向图模型都可以用公式(11.21) 来表示其联合概率。服从式 (16.7) 形式的任意分布都是 玻尔兹曼分布（Boltzmann distribution） 的一个实例。正是基于这个原因， 我们把许多基于能量的模型称为玻尔兹曼机（Boltzmann Machine）。现在我们来讲一个基于无向图的应用，去噪。 常见的无向图(不讲) 总结 利用图模型的局部马尔可夫性，我们可以对多变量的联合概率进行简化，从而降低建模的复杂度。（可加到有向图的后面）Using the local Markov property of the graph model, we can simplify the joint probability of multivariates, thus reducing the complexity of modeling. (can be added to the back of the directed graph) 局部马尔可夫性质：对一个更一般的贝叶斯网络，其局部马尔可夫性质为：每个随机变量在给定父节点的情况下，条件独立于它的非后代节点。对与一个无向图而言，即一个变量 Xk 在给定它的邻居的情况下独立于所有其它变量。 无向图—图像去噪无向图：图像去噪。 基于马尔科夫随机场的图像去噪方法+python代码 马尔可夫去噪+matlab：能量函数有点不同 GMNN: Graph Markov Neural Networks 受限玻耳兹曼机A widely used model of Undirected graph is Boltzman’s machine。 下面我们有请Ms Tang，来介绍some details about the Boltzman’s machine and its extentions in deep learning models. 玻耳兹曼机是一个无向图 发展：1986Hinton发明了玻耳兹曼机-&gt;2002Hinton发明了CD(对比散度)来计算梯度-&gt;2006Hinton提出了受限玻耳兹曼机 模型构建：Introduction to Restricted Boltzmann Machines Using PyTorch、英文原文 计算原理：预备知识、梯度上升法、评价效果的方法 代码：受限玻耳兹曼姬重构：tf版本、受限玻耳兹曼姬重构：pytorch版本、Introduction to Restricted Boltzmann Machines Using PyTorch、pytorch-rbm、 受限玻耳兹曼姬和深度学习的关系未完待续 GMNN: Graph Markov Neural Networks]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[图像处理相关问题综述]]></title>
    <url>%2Fp%2F23e4.html</url>
    <content type="text"><![CDATA[deblur Learning a Discriminative Prior for Blind Image Deblurring \min _{I, k}\|I \otimes k-B\|_{2}^{2}+\gamma\|k\|_{2}^{2}+p(I) The key to the success of this framework lies on the latentimage prior p(I), which favors clear images over blurredimages when minimizing (2). Therefore, the image priorp(I) should have lower responses for clear images andhigher responses for blurred images. test $a+b=\beta$ super-resolution Deep Plug-and-Play Super-Resolution for Arbitrary Blur Kernels Reference在图像处理领域的不同的研究问题]]></content>
  </entry>
  <entry>
    <title><![CDATA[hexo博客搭建]]></title>
    <url>%2Fp%2F7ab7.html</url>
    <content type="text"><![CDATA[安装node.js https://nodejs.org/en/ sudo npm install —unsafe-perm —verbose -g hexo；hexo version 测试 自带了git，否则需要安装 初始化博客文件夹 123$ hexo init &lt;folder&gt;$ cd &lt;folder&gt;$ npm install 测试生成网页 123hexo s 重新编译，并在本地显示hdxo g 重新编译hexo d 上传至github 选主题https://hexo.io/themes/ 下载 git clone https://github.com/theme-next/hexo-theme-next 将它放到theme下，并重命名 修改 站点配置文件_config.yml中的主题 12345# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/# theme: landscapetheme: next 重新编译 修改主题风格 生成的github文件我的github链接：https://github.com/ysix7/ysix7.github.io/tree/master/2019 参考blog： https://hellozhaozheng.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/ 本地更新文件 Hero s 本地看 新增的功能分级目录：https://blog.csdn.net/wugenqiang/article/details/88609066 新增分类：http://www.iooeo.com/2017/07/20/Hexo-%E6%96%B0%E5%BB%BA%E8%8F%9C%E5%8D%95-menu-%E5%AD%98%E6%94%BE%E5%BD%92%E6%A1%A3/ 分类以及标签深度学习：pytorch，python, 机器学习，服务器 美食类：西安,香港.. 软件：latex 图形处理：denoising,segmentation 主页自定义12345678npm install --save hexo-generator-searchnpm install hexo-generator-searchdb --save 添加本地搜索npm install hexo-generator-sitemap --savenpm install hexo-generator-baidu-sitemap --save 生成sitemap，用来被百度谷歌收录npm install --save hexo-symbols-count-time 添加统计字数npm install hexo-generator-index --savenpm install hexo-generator-index-pin-top --save 博客置顶npm install hexo-abbrlink --save 优化链接 主题配置文件 头像 ：theme-source-imgs 页面左上角图标 链接：https://www.iconfont.cn/home/index?spm=a313x.7781069.1998910419.2 文件保存位置： 分类图标 链接：http://fontawesome.dashgame.com/ 开启站内搜索 修改页边距公式渲染问题 开启公式：scaffolds-post.md中修改默认模版 模板里面只写mathjax: ，只在需要公式的地方写true。模版中写true，会导致网页显示变慢。 解决mathjax默认渲染将一些符号转义的问题 新建草稿博客hexo n title： 默认用的是post布局 hexo n draft title： 会在post同级建一个draft文件夹放草稿source/_drafts 草稿的默认结构在scaffolds里的draft.md里面改 出现过的问题 文章不显示在列表：tags以及catogories的填写格式有问题 全局搜索不出现的问题]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[生成模型vs判别模型]]></title>
    <url>%2Fp%2F44f5.html</url>
    <content type="text"><![CDATA[不同的应用判别式模型，这种模型的形式主要是根据原始图像推测图像具备的一些性质，例如根据数字图像推测数字的名称，根据自然场景图像推测物体的边界；而生成模型恰恰相反，通常给出的输入是图像具备的性质，而输出是性质对应的图像。这种生成模型相当于构建了图像的分布，因此利用这类模型，我们可以完成图像自动生成（采样）、图像信息补全等工作。 生成模型可以用来重构、也可以用来判别。当用于判别时 生成模型可以是监督的，也可以是无监督的？ 常用的生成网络：VAE、GAN、GAN 问题生成模型的输入是什么？ 参考 十个生成模型(GANs)的最佳案例和原理 | 代码+论文 GNN论文分门别类，16大应用200+篇论文最新推荐]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[图形边缘提取]]></title>
    <url>%2Fp%2Fbe62.html</url>
    <content type="text"><![CDATA[参考资料 Edge detection ppt Edge detection blog Edge detection matlab 图像的二阶信息 传统数学方法发展过程 一阶、二阶、优点、缺点、鲁棒性 canny Sobel code++ I. Sobel. Camera models and machine perception. Technical report, DTIC Document, 1970. thresholding the gradient map. Canny：an extension of Sobel, more robust to noise. code++ J. Canny. A computational approach to edge detection. IEEE TPAMI, 8(6):679–698, 1986. Gaussian smoothing as a preprocessing step Laplacian: 用一个线性算子即可 Ansitropic nabla^2 = uxx+uyy. (右边取绝对值) 深度学习方法 Xie, S., Tu, Z.: Holistically-nested edge detection. In: Proceedings of the IEEE international conference on computer vision, pp. 1395–1403 (2015) Liu, Y., Cheng, M.M., Hu, X., Wang, K., Bai, X.: Richer convolutional features for edge detection. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3000–3009 (2017) Laplacian方法边缘加强去噪二阶算子或者一阶算子 边缘 二阶算子对应其他边缘 如canny边缘或者sobel边缘 能否有这个结果 实验记录08.30 原来的网络，更新了input和label的建立方法。 单独训练gx和gy，因为divergence可以通过gx和gy得到。 label: gx 1234image = im2double(uint8(image));image = double(image);[gx, gy] = gradient(image);edge = gy+0.5; Label:gx_YY 训练样本：detail_gradient15_gx.h5、detail_gradient15_gy.h5 将image归一化后求梯度，在DIV2数据中范围在[-0.4,+0.4] 将归一化后的g_15和gx作为输入和label 目前训练获得一个依然携带噪声的灰度图 下一步实验计划 目前问题 训练loss回升的原因 09.01 验证是否gx的信息也可以辅助去噪，不一定要二阶信息。 matlab自带的gradient操作 结论：原图的gx也能帮助图像复原，比gxgy的信息的效果稍差一点。 但是对gx的偏移或者缩放都会严重影响辅助效果，通常是没有效果。 计算是一同缩放或者平移的影响 一阶边缘和二阶边缘的区别 一阶边缘更像是一个浮雕图。 09/04不同的网络，不同的label，训练灰度图的结果 1.天蓝色：Unet, label-&gt;gradientX用的是自己写的gradient_x 2.深红: Ljc的Net，label-&gt;gradientX用的是自己写的gradient_x，网络的影响不大 3.深蓝色：Unet, label-&gt;gradientY用的是系统 4.黄色： Unet, label-&gt;gradientX用的是系统 12345678910111213141516171819202122232425262728% 一阶边缘%% 1.label: gradient matlab自带梯度image = im2double(uint8(image));image = double(image);[gx, gy] = gradient(image); %[-0.3, 0.3]edge1 = gx+0.5;%% 2.label: gradient_x 相邻两个像素点的差值edge = gradient_x(image) % [-0.6, 0.6]edge2 = edge/2.0+0.5 % [0.2, 0.8];% 二阶边缘%% zeng的edgegx = gx./sqrt(gx.^2+gy.^2+1);gy = gy./sqrt(gx.^2+gy.^2+1);edge = divergence(gx,gy); [-0.6, 0.6]%% 3.label 归一化的二阶边缘：训练出来的结果比shift好。但是网络并没有收敛的趋势？MaxValue = max(max(edge));MinValue = min(min(edge));edge2=(edge-MinValue)/(MaxValue-MinValue);%% 4.divgradient：算子的运算更直接, 对应divgradient_T转置edge3 = divgradient(ftrue); # [-0.6, 0.6]% label_divgradient: edge3 = edge3/2.0;% label_divgradient: edge3 = edge3/2.0+0.5;% 都采用DenoiseNet，再尝试Unet 正则项的改进 算子和网络edge匹配 缩放 平移 一阶gx下，gradient_x和label：一阶边缘系统gradient的psnr最高。视觉效果最好。 正则项的变形 $|x-edge_x|_{1}+|y-edge_y|_{2}$ 结果 视觉方面，噪声大的时候，新的edge能不能减少artifacts？ 噪声低的时候，psnr能不能升高 重复的纹理能不能提出来 使用divgradient+DenoiseNet，Set12，noise=15，psnr比过bm3d, sum(bm3d)=388.4312 alpha=0.1，beta=0.3, sum(psnr)= 390.5050 alpha=0.1，beta=0.4, sum(psnr)= 390.0549 alpha=0.1，beta=0.2, sum(psnr)=390.2907 alpha=0.07，beta=0.3, sum(psnr)=391.5980 09.09 问题一：噪声50下，原图的edge，也无法将图像复原。有斑驳。 解决办法： 检查有没有解错，beta很小的情况下，为什么会有斑驳，理应和没有beta是一致的 尝试两步解法，先tv，再edge 不太行：加上去的纹理像噪声 问题二：无法提取高噪声下的edge 解决办法：用新的训练代码，或者找另一个人一起训练，先验证15下的loss和模型是差不多的效果。 先尝试噪声20 25 30 论文要修改的地方 噪声等级和噪声方差之间的关系 $\sigma=15,35,50$ 标志着噪声等级。 对应均值为0，方差为$(\sigma/255)^2$的高斯白噪声。 噪声的合成由matlab的imnoise完成。 实验评价 比较的方法 NLM, BM3D, tight frame \cite{cai2014data} 参数的选择 the code of BM3D and Tight frame are downloaded from thier official website. and run directly.? we choose the parameters for the best performance. TV: 15, alpha=0.01, 35, alpha=0.28, 07_tv28.bmp 50, alpha=0.38, 07_tv38.bmp Edge TV: 15, alpha=0.08, beta=0.3 35, alpha=0.25, beta=0.9 50, alpha=0.30, beta=0.8 摘录 g, the introduction of the second order term leads to a signiﬁcant reduction of the staircasing effect resulting in piecewise smooth images rather than piecewise constant images when using the ROF model. The superiority of an approach that combines ﬁrst and second order regularisation rather than ﬁrst order regularisation only, is conﬁrmed quantitatively by the SSIM index. 图片表格的改进： 补充噪声级别和噪声variance之间的关系 （sigma/255）^2; therefore 15 corresponds to the 50 corresponds to 补充参数表格 将所有psnr参数用红色标出，把表格转置 The results are illustrated in Fig. 15 . And all the numerical mesurements of the results of Set 12 is listed in Table 7. the psnr and ssim of the proposed methods which outperform the BM3Dis marked red in 图后面可以加解释 tight frame和BM3D的参数不需要 for BM3D and tight frame，we use their code published on their and the parameters are tuned to the best performance 调参的可视化 评价 效率： For tight frame based methods, a sparse basis for transforming the noisy images to a sparser representation is the crucial step.a over-complete dictionary is calculated for each noisy image, where it can get a sparse representation. BM3D，block searching, time cost，Patch-based methods, searching the pixels in a similar block, and therefore cosume a lot of time p needs to be trained for different level of noise rather than different images. for images of diﬀerent types, 效果 these methods all work quite well in the case of noise 15. Therefore,we have to show you the the enlaegement to see the tiny difference. Compared to BM3D, EdgeTV is more sharp in the edge, as in the 海星。 It is much smoother in the smooth district, sometimes even smoother and smoothed but not at the loss of the detail. Actually, BM3D is still oversmoothed as in the mouth of parrot. Actually, the sharp the edg Comparisons of denoising results with other methods on Set12 论文实验部分参考 架构参考 3.3 Inpainting 3.4 Zooming 3.5 Denoising 阐明比较的方法；阐述各种方法中的超参，初始条件； 3.5.1 Synthetic Image 噪声的级别；算法的迭代次数；阐述结果的视觉效果。 3.5.2 Natural Image 噪声的级别；参数的设置,迭代次数；评价结果（先把表格和图标说明具体化：具体某列某个细节的描述方法） In all the experiments displayed in Fig. 15, we observe a similar phenomenon. The TV regularization contains staircasing effects and reduces the contrast or erases thin features,see the ropes in Boat. Some gray stripes on the NLTV denoising result become less notable, see the second row, fourth column in Fig. 16. NLTVG denoising results have similar problems because of an excessive blur. BM3D puts distracting artifact around ropes, see the ﬁrst row, sixth column in Fig. 16, and smooths out some of the textures on the scarf 图片的组成 单张图的所有噪声不同方法的结果；参数的设置；多张图的多种噪声结果；表格数值；放大区域比较 阐明处理的问题：denoising, images corrupted with Gaussian noise, thus L2 norm in the fidelity term is the most suitable. 比较方法: We compare our method with… 参数选择: We present examples of (5.2) for alpha=, beta=. 有额外说明的用Note that 测试的图像：the basic synthetic test image is shown in Fig. 3. 评价方法：Our main assessment for the quality of the reconstruction is the structural similarity index SSIM. The SSIM index also assesses the conservation of the structural information of the reconstructed image. N 实验的终止条件:固定迭代次数 评价： 词组摘录 visual quality, visual inspection, 噪声 We consider additive Gaussian white noise with standard deviation σ = 0.04, 0.06 and 0.08 The noise has been produced with MATLAB’s built in function imnoise. 参数选择 The setting of the parameters in RNLTV and NLTV is presented in Tables 5 and 6, respectively. The number of iterations 8 is both 600. BM3D does not have any input parameters (except the polluted image). The parameter of the TV model is tuned to ﬁnd the best restoration quality. The highest SSIM value for TV denoising is achieved for α = 0.12 (SSIM =0.8979) while the highest one for TV-TV 2 is achieved for α = 0.06, β = 0.03 (SSIM = 0.9081). Note, however, that this optimal combination of α and β in terms of SSIM does not always correspond to best visual result. In general, the latter corresponds to a slightly bigger β than the one chosen by SSIM, see Fig. 5(h). 看图说话 See Fig. 4 for an visual inspection of the results for the image “Barbara” by diﬀerent methods. Figure 5 depicts one denoising example, where the original image is corrupted with Gaussian noise of variance 0.005. For better visualisation, we include the middle row slices of all the reconstructions in Fig. 6. 对TV的评价 The TV regularization contains staircasing effects and reduces the contrast or erases thin features, see the ropes in Boat. —while being simple and efﬁciently numerically solvable. 不同方法的比较 the performances of the K-SVD method with patch size 8×8 and our approaches with ﬁlter 8×8, 16×16 are nearly the same with similar PSNR values. Overall, the performances of our proposed method and the K-SVD method are comparable in terms of PSNR value, and so is the visual quality. There are images on which the K-SVD method performed better and there are some on which our approaches performed better. 时间比较 Moreover, the computational effort needed for its numerical solution is not much more than the one needed for solving the standard ROF model [61]. For comparison the numerical solution for TGV regularisation is in general about ten times slower than this, see Table 1 at the end of the paper. 方法的延伸 Let us note that the algorithm (5.19)–(5.23) can be easily generalised to colour images, again see [57, 76].]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Proof_解的存在性]]></title>
    <url>%2Fp%2Fe1d.html</url>
    <content type="text"><![CDATA[目录 存在性：存在收敛的序列 唯一性，凸性 解存在性证明例子1 证明 $\inf _{\Omega} f \leq u^{*} \leq \sup _{\Omega} f$ 通过有界性说明 证明有界性 BV的有界性 解存在性证明例子2问题的转化1.原问题：The natural space for the functional H to be deﬁned in, is $W^{2,1}(\Omega)$ \begin{aligned} H(u)=& \frac{1}{2} \int_{\Omega}\left(u_{0}-T u\right)^{2} d x+\alpha \int_{\Omega} f(\nabla u) d x +\beta \int_{\Omega} g\left(\nabla^{2} u\right) d x \end{aligned}缺点：this space is not reﬂexive(自反空间), and thus existence of minimisers cannot be guaranteed. 每个有限维赋范向量空间都是自反空间。 所有的希尔伯特空间都是自反空间。比如说，$\mathrm{L}^2$空间是自反空间。 2.空间拓展上：extend the functional H in (3.5) to $\mathrm{BH}(\Omega)$ H_{e x}(u)=\left\{\begin{array}{l}{\frac{1}{2} \int_{\Omega}\left(u_{0}-T u\right)^{2} d x+\alpha \int_{\Omega} f(\nabla u) d x} \\ {+\beta \int_{\Omega} g\left(\nabla^{2} u\right) d x \quad \text { if } u \in W^{2,1}(\Omega)} \\ {+\infty \text { if } f \in \mathrm{BH}(\Omega) \backslash W^{2,1}(\Omega)}\end{array}\right.缺点：$H_{ex}$ is not sequentially lower semicontinuous with respect to the strict topology in BH(Ω) and hence it is neither with respect to the $weak^∗$ topology in $[\mathrm{BH}(\Omega)]^{n}$ 为什么需要lower semicontinuous? $BH$空间, $\mathrm{BH}(\Omega)=\left\{u \in W^{1,1}(\Omega) : \nabla u \in[\mathrm{BV}(\Omega)]^{2}\right\}$ the $weak^∗$ topology in BH(Ω) provides a good compactness property which is inherited from the weak ∗ topology in $[\mathrm{BV}(\Omega)]^{n}$ 3.函数上拓展：Relaxed functional 将$\nabla$改成可$D$, 再将$D$分解 \begin{aligned} \overline{H_{e x}}(u) :=& \frac{1}{2} \int_{\Omega}\left(u_{0}-T u\right)^{2} d x+\alpha \int_{\Omega} f(\nabla u) d x \\ &+\beta g\left(D^{2} u\right)(\Omega) \\=& \frac{1}{2} \int_{\Omega}\left(u_{0}-T u\right)^{2} d x+\alpha \int_{\Omega} f(\nabla u) d x \\ &+\beta \int_{\Omega} g\left(\nabla^{2} u\right) d x \\ &+\beta \int_{\Omega} g_{\infty}\left(\frac{D^{s} \nabla u}{\left|D^{s} \nabla u\right|}\right) d\left|D^{s} \nabla u\right| \end{aligned} Relaxed function： 优点： lower semicontinuous。 Theorem 3.6 The functional $\overline{H_{e x}}(u)$ is lower semicontinuous with respect to the strict topology in $\mathrm{BH}(\Omega)$. which means, $\overline{H_{e x}}(u)\le\lim_{k\to\infty}\inf\overline{H_{e x}}(u_k)$, when $u_k\to u$, in $\mathrm{BH}{(\Omega)}$ Also, $\overline{H_{e x}}(u)\le\lim_{k\to\infty}\inf\overline{H_{e x}}(u_k)$, when $u_k\rightarrow^{w^*} u$, in $\mathrm{BH}{(\Omega)}$ 证明步骤目标方程：$\inf _{u \in \mathrm{BH}(\Omega)} \overline{H_{e x}}(u)$ 取一个minimizing sequence $\{u_k\}$ 证明$\{u_k\}$ is bounded in BH(Ω). 利用BH的compactness, 得到$u_k\rightarrow u$ weakly 在BH空间 利用${H_{ex}}$的下半连续性，得到 $\overline{H_{e x}}(u)\le\lim_{k\to\infty}\inf\overline{H_{e x}}(u_k)$ which implies that(因为$u_k$minimizing): $u=\min _{u \in \mathrm{BH}(\Omega)} \overline{H_{e x}}(u)$ 利用 property of the relaxed functional：$\min _{x \in X} F(x)=\inf _{x \in X} F(x)$ 问题：？那不是只能说明F存在下界？ inf F(x) = min F(x) Bregman算法的收敛性证明 参考 “THE SPLIT BREGMAN METHOD FOR L1 REGULARIZED PROBLEMS” 参考 “A Combined First and Second Order Variational Approach for Image Reconstruction” 级数的收敛级数：一个有穷或无穷的序列的和称为级数。根据项数分为：有穷级数和无穷级数 无穷级数的收敛性：如果当$n$趋于正无穷大时，趋向一个有限的极限。如果当趋于正无穷大时，趋向一个有限的极限)： 条件收敛：如果任意项级数收敛，而级数发散，则称级数条件收敛。 绝对收敛：如果级数收敛，则称级数绝对收敛。绝对收敛-&gt;条件收敛 收敛级数的性质：当趋向无限大时，任何一个收敛级数的通项都趋于0： 序列的收敛性有界 定义 Thus a sequence f = (a0, a1, a2, …) is bounded if there exists a real number M such that 实数中的定义 如果存在一个实数 k，使得对于所有 S 中的 s 有 k ≥ s，实数集合 S 被称为“上有界”的，这个数 k 被称为 S 的上界。可用类似的定义术语“下有界”和下界。 度量空间中的定义 度量空間 (M, d) 的子集S 是有界的，如果它包含在有限半徑的球內，就是說如果對於所有 S 中的 s，存在 M 中的 x 並且 r &gt; 0，使得 d(x, s) &lt; r。M 是有界度量空間（或 d 是有界度量），如果 M 作為自身的子集是有界的。 空间 向量空间、函数空间 Lp空间 Lp空间是由p次可积函数组成的空间；对应的ℓp空间是由p次可和序列组成的空间，有时叫做勒贝格空间。 Sobolev spaces: $W^{1,1}$, $W^{2,1}$ Equipped with the norm becomes a Banach space. Banach空间：完备(柯西收敛即收敛)+赋范, 属于Hilbert空间 A Banach space is a vector space X over any scalar field K, which is equipped with a norm) and which is complete with respect to the distance function induced by the norm, that is to say, for every Cauchy sequence ${x_n}$ in X, there exists an element x in X such that $\lim _{n \rightarrow \infty} x_{n}=x$ BV空间 One variable Functions of bounded variation, BV functions), are functions whose distributional derivative is a finite[5] Radon measure. $|u|_{\mathrm{BV}(\Omega)} :=\int_{\Omega}|u| d x+|D u|(\Omega)$ $BV^2$空间 $|u|_{\mathrm{BV}(\Omega)} :=\int_{\Omega}|u| d x+|D u|(\Omega)$ BH空间, 属于Banach空间 $\mathrm{BH}(\Omega)=\left\{u \in W^{1,1}(\Omega) : \nabla u \in[\mathrm{BV}(\Omega)]^{2}\right\}$ $\mathrm{BH}(\Omega)$ is a Banach space equipped with the norm $|u|_{B H(\Omega)}=|u|_{B V(\Omega)}+\left|D^{2} u\right|(\Omega)$. Deﬁnition 3.1 ($Weak^∗$ Convergence in BH(Ω)) We say that $(u_k ),k\in N$ converges to $u$ $weakly^∗$ in $\mathrm{BH}(\Omega)$ if $u_{k} \rightarrow u, \quad \text {in} L^{1}(\Omega)$ and $\nabla u_{k} \rightarrow \nabla u \quad \text {weakly }^{*} \text { in }[\mathrm{BV}(\Omega)]^{2},\text{as } k \rightarrow \infty$ or in other words: \begin{array}{l}{\left\|u_{k}-u\right\|_{L^{1}(\Omega)} \rightarrow 0} \\ {\left\|\nabla u_{k}-\nabla u\right\|_{\left[L^{1}(\Omega)\right]^{2}} \rightarrow 0} \\ {\int_{\Omega} \phi d D^{2} u_{k} \rightarrow \int_{\Omega} \phi d D^{2} u, \quad \forall \phi \in C_{0}(\Omega)}\end{array} Theorem 3.2 (Compactness in $\mathrm{BH}(\Omega)$): 可根据bounded得到收敛的序列 Suppose that the sequence $\left(u_{k}\right)_{k \in \mathbb{N}}$ is bounded in $\mathrm{BH}(\Omega)$. Then there exists a subsequence $\left(u_{k_{\ell}}\right)_{\ell \in \mathbb{N}}$ and a function $u \in \mathrm{BH}(\Omega)$ such that $\left(u_{k_{\ell}}\right)_{\ell \in \mathbb{N}}$ converges to u $weakly^∗$ in $\mathrm{BH}(\Omega)$. Deﬁnition 3.3 (Strict Convergence in BH(Ω)) We say that $(u_k ),k\in N$ converges to $u$ $strictly$ in $\mathrm{BH}(\Omega)$ if $u_{k} \rightarrow u, \quad \text {in } L^{1}(\Omega)$ and the weak ∗ topology in BH(Ω) provides a good compactness property which is inherited from the weak ∗ topology in [BV(Ω)] n. Compactness：可根据bounded得到收敛的序列 Lebesgue space 就是Lp空间？ 各类空间 序列 柯西序列 定义： 和收敛序列的关系：任何收敛数列必然是柯西列，任何柯西列必然是有界序列。 无限维度中的有界序列具体含义：见“有界” 柯西列是否一定是收敛的？ 序列收敛的定义 收敛(强)：定义在norm 弱(weak)收敛：定义在内积 weak*收敛： 实数序列收敛的判别方法 Bolzano-Weierstrass: 任一有限维实向量空间$\mathbb {R} ^{n}$中的有界序列都至少包含一个收敛的子列。 有界不一定收敛 有界序列是什么意思？ 有界序列 Theorem 3.2 (Compactness in $\mathrm{BH}(\Omega)$) Suppose that the sequence $(u_k)_{k \in N}$ is bounded in $\mathrm{BH}(\Omega)$. Then there exists a subsequence $\left(u_{k_{\ell}}\right)_{\ell \in \mathbb{N}}$ and a function $u \in \mathrm{BH}(\Omega)$ such that $\left(u_{k_{\ell}}\right)_{\ell \in \mathbb{N}}$ converges to $u$ weakly∗ in $\mathrm{BH}(\Omega)$. 不同的收敛速度 converge sublinearly $\lim _{k \rightarrow \infty} \frac{\left|x_{k+1}-L\right|}{\left|x_{k}-L\right|}=1$ converge linearly $\lim _{k \rightarrow \infty} \frac{\left|x_{k+1}-L\right|}{\left|x_{k}-L\right|}=\mu, \mu \in(0,1)$ converge superlinearly $\lim _{k \rightarrow \infty} \frac{\left|x_{k+1}-L\right|}{\left|x_{k}-L\right|}=0$ Q-linear convergence: distinguish superlinear rates of convergence. $\lim _{k \rightarrow \infty} \frac{\left|x_{k+1}-L\right|}{\left|x_{k}-L\right|^{q}}&lt;M$ Monotone operator 不同的收敛模式 强收敛 弱收敛 补充Fatous lemma Lower semi-continuous A lower semi-continuous function. $f(x)=\lceil x \rceil$ is lower semi-continuous. An upper semi-continuous function. Distributional derivative weak derivative Reference Distributions and distributional derivatives Distributional Derivative 问题 证明中的问题 minimizing sequence就不是bounded in BH？ 分解的前提只需要g是连续的，convex，不需要coercivity condition 证明bounded in BH的充分条件是什么 BH空间的扩展中，从nabla到D的过程转变 K-SVD code 线性增长是否一定等价于下述不等式？作用是什么？以及分解的作用？ coercivity condition 是什么？一定等价于？在此处的作用是什么？ 问题 Linear growth 为什么需要$1+|x|$ $Du=\nabla u$ 当且仅当$u \in W^{1,1}$ $\nabla$替换成D，为什么可以将方程relaxed, 继而对g的分解 g分解的前提和依据？ minimizing sequence是任意取的未必在${W^{1,1}}$中，那么为什么m Du bounded, L1 bounded-&gt; bounded in BH? relaxed $\overline H$有解，为什么代表原问题有解？需要 $\min \overline{H}=\inf H$ 离散化Splitting bregman的收敛性]]></content>
      <categories>
        <category>数学</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[概率图模型]]></title>
    <url>%2Fp%2Fd0e9.html</url>
    <content type="text"><![CDATA[统计学知识 极大似然估计与最大后验概率估计 极大似然估计：p(X=头痛|Y=中风)=0.8 P(X=头痛|Y=感冒)=0.5 P(X=头痛|Y=) X=头痛 Y=argmaxP(X=头痛|Y) Y=中风 最大后验概率：Y = argmax p(Y|X=头痛)=P(X|Y)P(Y) 生成模型和判别模型的区别 判别模型 p(y|x), 给一个X，判断y=1的值。 生产模型 p(y|x)等价于p(x|y)p(y)=p(x,y)。需要构造一个p(x|y=1)p(y=1)的根据黄牛特征得到的黄牛模型，p(x|y=0)p(y=0)的水牛模型，比较联合概率密度。 深度学习中的生成模型：深度学习的三大生成模型：VAE、GAN、GAN、十个生成模型(GANs)的最佳案例和原理、 概率图模型 structured probabilistic models.概率图模型：用图论表现随机变量之间的条件依赖关系的建模方法。典型的概率图模型包括贝叶斯网络和马尔可夫随机场，分别对应着有向图模型和无向图模型。 它们提供了将概率模型的结构可视化的简单方式，而对图形的观察可以加深对模型性质的认识，其中最主要的性质就是变量之间的条件独立性。此外，概率图模型还可以表示学习和推断过程中的复杂计算，隐式地承载了图形背后的数学表达。 优点：图模型建模方式的优点是：多变量分布通常可以表示为一些局部函数（local functions）的乘积，而每个局部函数依赖于更小的变量子集。通过因子化（factorization）和条件独立性（conditional independence），使得复杂的多变量分布可以用少得多的参数进行刻画。 1、概率模型是利用训练样本数据，通过学习条件概率分布P(X|Y)来进行推断决策，而非概率模型是通过学习得到决策函数Y=f(X)来进行决策。 2、生成模型的目标是求联合概率分布P(X,Y)，然后由条件公式求取条件概率分布P(X|Y)。即P(X|Y) = P(X,Y) / P(X)。 3、判别模型是由训练数据直接求取决策函数Y=f(x)或者条件分布P(X|Y)。它并不需要关心X与Y之间的生成关系，它关心的是对于给定输入X应该得到怎么样的输出Y。 4、机器学习大部分模型都是判别模型，判别模型得到条件概率或者决策函数直接用于预测，准确率会更高；而生成模型用于数据预测，所以它的应用领域会更加广泛。 5、常见的判别模型有：K近邻、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、逻辑斯蒂回归、boosting、条件随机场 常见的生成模型有：朴素贝叶斯、隐马尔可夫模型、高斯混合模型、文档主题生成模型（LDA）、限制玻尔兹曼机 作者：decan5958来源：CSDN原文：https://blog.csdn.net/decan5958/article/details/76607082 深度学习和概率图模型的关系 参考 深度学习之外的人工智能——概率图模型 与深度学习的关系 两者者使用 同的基本计算工具：近似推断？损失函数？学习过程？ 深度学习潜变量：比可观察变量更多；不包含特定含义； 概率图模型潜变量：数量通常很少；通常被赋予一些特定含义； 深度学习的连接方式：其他单元组全连接 概率图的连接方式：具有非常少的连接， 并且每个变量的连接选择可以单独设计。 模型结构的设计与推断算法的选择紧密相关。图模型的传统方法通常旨在保持精确 推断的可解性。 推断方式：什么是推断？作用？ 图模型的传统方法通常旨在保持精确推断的可解性。 大规模图模型和深度图模型最大的区别之一就是深度学习中几乎从来不会使用环状信念传播。相反的，许多深度学习模型可以设计来加速 Gibbs 采样或者变分推断。 图模型如何用于深度学习的典型例子：受限玻尔兹曼机（Restricted Boltzmann Machine, RBM） 特点：它的单元被分成很大的组，这种组称作层，层之间 的连接由矩阵描述，连通性相对密集。该模型被设计为能够进行高效的 Gibbs 采样， 并且模型设计的重点在于以很高的自由度来学习潜变量。 标准的 RBM 是具有二值的可见和隐藏单元的基于能量的模型。其能量函数为$E(\boldsymbol{v}, \boldsymbol{h})=-\boldsymbol{b}^{\top} \boldsymbol{v}-\boldsymbol{c}^{\top} \boldsymbol{h}-\boldsymbol{v}^{\top} \boldsymbol{W h}$ 概率图(结构化)模型的优点 显著降低表示概率分布、学习和推断的成本。如有向图中的参数表示。1+4+4+9的例子 有向模型中采样还可以被加速，但是对于无向模型情况则较为复杂。？ 概率图模型的应用 MRF去噪：无向图中的马尔可夫模型？ 能否看到一个具体的学习、建模过程。 有向图模型/贝叶斯网络/信念网络 有向图模型以及联合分布 常见的有向图：sigmoid 信念网络、朴素贝叶斯分类器、隐马尔可夫模型(HMM) 一个能看懂的HMM例子：每天观察一个病人的状态 无向图/马尔可夫随机场/马尔可夫网 常见的无向图：对数线性模型、条件随机场(CRF） 无向图模型 无向图的联合概率可以分解为一系列定义在最大团上的非负函数的乘积形式。 组成：配分函数$Z=\int \tilde{p}(\mathbf{x}) d \mathbf{x}$。 常见的无向图：条件随机场, 词性分类 条件随机场进行词性分类 线性链条件随机场-tutorial（一） 线性链条件随机场-tutorial（二） 常见的无向图：去噪 基于马尔科夫随机场的图像去噪方法+python代码 马尔可夫随机场彩色图去噪 马尔可夫去噪+matlab：能量函数有点不同 采样去获取满足一个分布的样本。 例如：需要获得一个泊松分布，知道某事件符合此分布的话，可以通过采集这个事件的信息，来获得样本。 拒绝采样：利用一个容易获取样本的分布q，先获得一个样本，再通过判断$\alpha(\hat{x})=\frac{\hat{p}(\hat{x})}{k q(\hat{x})}$，来决定是否留下这个样本x Gibbs采样：一种满足稳态转移的马尔可夫采样法。 参考： 直观理解概率图模型中的采样(sampling)技术 浅谈Gibbs 一篇MCMC解释的不错的文章 问题：和推断的关系？ 蒙特卡洛采样、非数学话的解释蒙特卡洛采样、 推断利用图的结构，来计算出一些变量的后验信息 推断和最大似然任务的关系：在计算最大对数似然函数的时候，中间步骤需要一些变量的后验信息。 例子：EM算法 本质是最大化似然函数：$p(\mathbf{x|\theta})$ 通过隐含结点z，来表示x的编辑概率：$p(\mathbf{x} | \theta)=\sum_{\mathbf{Z}} p(\mathbf{x}, \mathbf{z} | \theta)$ 通过变分函数q(z)(z的先验信息)，来获得一个下界。 EM迭代q和$\theta$, 使得$p(x|\theta)$越来越小. 其中每一次迭代时的$ q(\mathbf{z})=p(\mathbf{z} | \mathbf{x}, \theta)$，因此需要计算$p(z|x)$。此时用到的就是推断。根据图的定义，$p(x|z)和p(x),p(z)$是已知的。 学习如上述的EM算法，学习参数的优化算法。 玻耳兹曼机(RBM)参考一个写的不错的受限玻尔兹曼机（RBM）学习笔记 博客：算法描述 对上一个博客的笔记：对比散度(CD)算法 code: RBM-for-MNIST 其他： 如何使用TensorFlow和VAE模型生成手写数字 free energy的推算 特点：它的单元被分成很大的组，这种组称作层，层之间 的连接由矩阵描述，连通性相对密集。该模型被设计为能够进行高效的 Gibbs 采样， 并且模型设计的重点在于以很高的自由度来学习潜变量。 马尔可夫去噪+matlab：能量函数有点不同 我对玻耳兹曼姬的理解：求出转移的条件概率以后，GIbbs采样，通过一系列的往返操作，可以采出满足p（V）分布的样本。再通过gibbs采样，获得满足p(v)的样本。 采样： 推断和采样的关系 什么时候需要采样 其他参考联合概率、边际概率、条件概率 一个完整的概率图模型笔记、这系列相关的一个博客 概率图模型的讲解 隐马尔可夫(HMM)模型 CPD(conditional probability distribution)概率图模型]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自编码器]]></title>
    <url>%2Fp%2F221.html</url>
    <content type="text"><![CDATA[参考 自编码器是什么？有什么用？这里有一份入门指南（附代码） 发展 自编码器 整个自编码器可以用函数g(f(x)) = r来描述，其中输出r与原始相近。 作用 如果自编码器的唯一目的是让输出值等于输入值，那这个算法将毫无用处。事实上，我们希望通过训练输出值等于输入值的自编码器，让潜在表征h将具有价值属性。自编码器能从数据样本中进行无监督学习，这意味着可将这个算法应用到某个数据集中，来取得良好的性能，且不需要任何新的特征工程，只需要适当地训练数据。 实现 限制h的维度使其小于输入x，这种情况下称作有损自编码器。通过训练有损表征，使得自编码器能学习到数据中最重要的特征。 正则 应用 第一是数据去噪，第二是为进行可视化而降维。设置合适的维度和稀疏约束，自编码器可以学习到比PCA等技术更有意思的数据投影。 种类 香草自编码器 多层自编码器 卷积自编码器 正则自编码器：使用损失函数来鼓励模型学习其他特性 常用两种正则自编码器 稀疏自编码器：通过对损失函数施加惩罚项 降噪自编码器：是通过改变损失函数的重构误差项来学习一些有用信息。 去噪自编码器DAE]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[淘宝收藏]]></title>
    <url>%2Fp%2F4dde.html</url>
    <content type="text"><![CDATA[裤裤 短裤 衣服 一家红裙子店]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[粤语口语]]></title>
    <url>%2Fp%2F39a8.html</url>
    <content type="text"><![CDATA[粤语学习笔记 广东话和普通话的区别https://wenku.baidu.com/view/33face69011ca300a6c390cf.html粤语口语https://hal.archives-ouvertes.fr/hal-00271141/document 学粤语的一个不错的网页，中文大学链接 https://www.ilc.cuhk.edu.hk/chinese/canton_express/others/download.html j带头的字 今 jin-&gt;gen 尽量 jin-&gt;zen 叫jiao-&gt;gao 静候 jing-&gt;zeng 自由：zi-&gt;zei you 坚持：jian-&gt;gin ci 枯枝-&gt;fuzi 收成：sou seng 这一次：zeiyici 你要：nei you 失守 sei sou 始终 si zhong 成-&gt;seng 记载-&gt;gei zai]]></content>
      <categories>
        <category>语言</category>
      </categories>
      <tags>
        <tag>粤语</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[presentation常用英文口语]]></title>
    <url>%2Fp%2F9de6.html</url>
    <content type="text"><![CDATA[英语pre摘录 It’s been a while 有阵子了 It all boils down to realizing that it is natural (and interesting!) to consider different topologies on the same set 𝑋, each of which comes with a notion of convergence.这一切归结为 我的声音够大吗 你可以重复一下这个问题吗 我没有想过这个问题，但是我觉得 介绍下一位演讲嘉宾 对这部一部分你们应该有更深的印象 术语Maximum a posteriori estimation MAP posteriori probability 后验概率 ReferencePresentation English(英语 演讲用到的各种表达) Presentation实用表达总结]]></content>
      <categories>
        <category>语言</category>
      </categories>
      <tags>
        <tag>English</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生活中的英文口语]]></title>
    <url>%2Fp%2Fef9c.html</url>
    <content type="text"><![CDATA[重读音动词+介词： 轻轻带过介词，如look at 重读介词如：take off 动词+名词: 日常摘录关于吃饭、点餐、结账6/13 续杯、点餐 split the fare I have a god feeling I could tell]]></content>
      <categories>
        <category>语言</category>
      </categories>
      <tags>
        <tag>English</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac/ipad/iphone软件清单]]></title>
    <url>%2Fp%2F44bc.html</url>
    <content type="text"><![CDATA[Mac软件工具Ipad软件工具做笔记功能 notability功能介绍]]></content>
      <categories>
        <category>软件</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ios软件清单]]></title>
    <url>%2Fp%2F44bc.html</url>
    <content type="text"><![CDATA[Mac软件工具Ipad软件工具做笔记功能 notability功能介绍]]></content>
      <categories>
        <category>软件</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[L1算法综述]]></title>
    <url>%2Fp%2F65b3.html</url>
    <content type="text"><![CDATA[lowercontinuity 一阶算法 Proximal point algorithm Gradient projection method. Dual-projection An Algorithm for Total Variation Minimization and Applications ALM和PPA ReferenceADMM（1975，1976，2011） Gabay, D., and Mercier, B., A dual algorithm for the solution of nonlinear variational problems via finite-element approximations, Comp. Math. Appl., 2 (1976), pp. 17-40. Glowinski, R., and Marrocco, A., Sur lapproximation par elements finis dordre un, et la resolution par penalisation-dualite dune classe de problemes de Dirichlet nonlineaires, Rev. Francaise dAut. Inf. Rech. Oper., R-2 (1975), pp. 41-76. Existing convergence theory for ADMM Eckstein, J., and Bertsekas, D., On the Douglas-Rachford splitting method and the proximal point algorithm for maximal monotone operators, Mathematical Programming 55, NorthHolland, 1992. Boyd, S., Parikh, N., Chu, E., Peleato, B., &amp; Eckstein, J. (2011). Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends® in Machine learning, 3(1), 1-122. Split Bregman Bregman distance图像复原. Osher, S., Burger, M., Goldfarb, D., Xu, J., &amp; Yin, W. (2005). An iterative regularization method for total variation-based image restoration. Multiscale Modeling &amp; Simulation, 4(2), 460-489. 也是利用了Bregman distance 处理denoising (首次提出) Goldstein, T., &amp; Osher, S. (2009). The split Bregman method for L1-regularized problems. SIAM journal on imaging sciences, 2(2), 323-343. (与ADMM的等价性) Esser, E. (2009). Applications of Lagrangian-based alternating direction methods and connections to split Bregman. CAM report, 9, 31. Split Bregman（1967，2009） 参考文献： Goldstein, T., &amp; Osher, S. (2009). The split Bregman method for L1-regularized problems. SIAM journal on imaging sciences, 2(2), 323-343. (下述文献编号的出处) Bregman证明（具体针对什么的证明） 组成部分 Bregman Iteration to unconstrained problem Bregman Iteration to constrained problem Split Bregman to two variable problem Bregman distance Bregman Iteration for unconstrained problem 目标方程(2.1)：$\min_u E(u)+\lambda H(u)$, with $H$ is differentiable and $\min_u H(u)=0$. Bregman Iteration: 由[4]提出 \begin{aligned} u^{k+1} &=\min _{u} D_{E}^{p}\left(u, u^{k}\right)+\lambda H(u) \\ &=\min _{u} E(u)-\left\langle p^{k}, u-u^{k}\right\rangle+\lambda H(u) \\ p^{k+1} &=p^{k}-\nabla H(u^{k+1}) \end{aligned}得到的解是（2.4）的最小值，且满足$H(u)=0$? 应该只是收敛到H(u)=0 $H\left(u^{k}\right) \rightarrow 0$ as $k\rightarrow\infty$. and $H(u^{k+1})\le H(u^k)$. Bregman Iteration的收敛性 $u^*$: $H\left(u^{k}\right) \rightarrow 0$ as $k\rightarrow\infty$. | Theorem 2.1 || —————————————————————————————— || 1) Monotonic decrease in $H$: $H(u^{k+1})\leq H(u^k)$2) Convergence to a minimizer of $H$ :$H(u^{k})\leq H(u^)+J(u^)/k$ | Proof：参考文献[21] Bregman iteration for constrained problem 目标方程(2.4) \min _{u} E(u) \text { such that } \mathrm{Au}=\mathrm{b}等价于无约束问题(2.5)当$\lambda\rightarrow\infty$，如PPA算法，但是$\lambda$很大时不好求解 \min _{u} E(u)+\frac{\lambda}{2}\|A u-b\|_{2}^{2} 对(2.5)使用Bregman iteration [30][21] \begin{aligned} u^{k+1} &=\min _{u} D_{E}^{p}\left(u, u^{k}\right)+\frac{\lambda}{2}\|A u-b\|_{2}^{2} \\ &=\min _{u} E(u)-\left\langle p^{k}, u-u^{k}\right\rangle+\frac{\lambda}{2}\|A u-b\|_{2}^{2} \\ p^{k+1} &=p^{k}-\lambda A^{T}\left(A u^{k+1}-b\right) \end{aligned}当$A$是线性的时候，可简化为 \begin{aligned} u^{k+1} &=\min _{u} E(u)+\frac{\lambda}{2}\left\|A u-b^{k}\right\|_{2}^{2} \\ b^{k+1} &=b^{k}+b-A u^{k} \end{aligned} 证明(2.5)的Bregman迭代可以获得(2.4)的解。（解的等价性证明） | Theorem 2.2 || —————————————————————————————— || Let $H:R^n \rightarrow R$ be convex. Let $A: R^n \rightarrow R^m$ be linear. Consider the algorithm (2.9-2.10). Suppose that some iterate, $u^∗$ , satisﬁes $Au^∗ = b$(利用theorem2.1). Then $u^∗$ is a solution to the original constrained problem (2.4). | Proof1: 参考theorem 2.2 Assumption: $Au_{k+1}=b$, Proof2: “A Combined First and Second Order Variational Approach for Image Reconstruction” 问题：意味着任何一个lambda下的2.5的迭代解，都等价于(2.4)的解？迭代所得的解未必是(2.5)的解 Bregman的优点：收敛快(原因见appendix); 可以保持$\lambda$是常数，不需要变大，因此稳定。 在其他$\lambda_k$增大的情况下，需要以一种极慢的步子增大，使得算法的效率变低。 Split Bregman for l1-regularized optimization problem (1.1) 目标方程 \min _{u}|\Phi(u)|+H(u)where |·| denotes the l1-norm, and both |Φ(u)| and H(u) are convex functions, Φ(·) to be diﬀerentiable. 引入一个变量d, 转化成一个约束问题 (3.1) $\min \limits_{u, d}|d|+H(u) \text { such that } d=\Phi(u)$转化成一个无约束问题 (3.2) $\min\limits_{u, d}|d|+H(u)+\frac{\lambda}{2}|d-\Phi(u)|_{2}^{2}$进一步转化成无约束问题 Bregman iteration \begin{aligned}\left(u^{k+1}, d^{k+1}\right) &=\min _{u, d}|d|+H(u)+\frac{\lambda}{2}\left\|d-\Phi(u)-b^{k}\right\|_{2}^{2} (3.7)\\ b^{k+1} &=b^{k}+\left(\Phi\left(u^{k+1}\right)-d^{k+1}\right) (3.8)\end{aligned} Splitting technique to solve (3.7) \begin{array}{l}{\text { Step } 1 : u^{k+1}=\min _{u} H(u)+\frac{\lambda}{2}\left\|d^{k}-\Phi(u)-b^{k}\right\|_{2}^{2}} \\ {\text { Step } 2 : d^{k+1}=\min _{d}|d|+\frac{\lambda}{2}\left\|d-\Phi\left(u^{k+1}\right)-b^{k}\right\|_{2}^{2}}\end{array} Generalized Split Bregman Iteration 内循环N=1即可，直观理解 Application ADMM Dual Ascent ADMM ADMM和Split bregman的关系 和Split Bregman的等价性proof Applications of Lagrangian-Based Alternating Direction Methods and Connections to Split Bregman [2009] ADMM 最早分别由 Glowinski &amp; Marrocco 及 Gabay &amp; Mercier 于 1975 年和 1976 年提出，并被 Boyd 等人于 2011 年重新综述并证明其适用于大规模分布式优化问题。由于 ADMM 的提出早于大规模分布式计算系统和大规模优化问题的出现，所以在 2011 年以前，这种方法并不广为人知。 ADMM是一种ALM的方法。PPA也是。 Related Algorithm 来自Dual ascent+approximate Primal ascent，因为函数不具有连续性，有很多断掉的极大值点。 PPA（1976）Primal-dual（2011） A First-Order Primal-Dual Algorithm for Convex Problems with Applications to Imaging 二阶算法Inexact Lagrangian + 牛顿法 A highly eﬃcient semismooth Newton augmented Lagrangian method for solving Lasso problems code++、SuiteLasso 无约束问题转化成约束问题 \text { (P) } \max -\{f(x)=h(\mathcal{A} x)-\langle c, x\rangle+ p(x)\} \text { (P) } \max -\{f(x)=h(\mathcal{A} x)-\langle c, x\rangle+ p(x)\} \text { (P) } \min \{f(x)=h(\mathcal{A} x)-\langle c, x\rangle+p(x)\} \text { (P) } \max -\{f(x)=h(\mathcal{A} x)-\langle c, x\rangle+ p(x)\} \text { (D) } \min \left\{h^{*}(y)+p^{*}(z) | \mathcal{A}^{*} y+z=c\right\} 从(P)到(D)的转化过程：与增广拉格朗日的关系？ Assumption: 算法基础：An inexact augmented Lagrangian method for (D)[42]: $\lambda$是越来越大的 l(y, z, x)=h^{*}(y)+p^{*}(z)-\left\langle x, \mathcal{A}^{*} y+z-c\right\rangle, \quad \forall(y, z, x) \in \mathcal{Y} \times \mathcal{X} \times \mathcal{X} \mathcal{L}_{\sigma}(y, z ; x) :=l(y, z, x)+\frac{\sigma}{2}\left\|\mathcal{A}^{*} y+z-c\right\|^{2}, \quad \forall(y, z, x) \in \mathcal{Y} \times \mathcal{X} \times \mathcal{X} 算法核心 将(18) 转化成 (22) 解决（22）：Ax = 0 的方法 牛顿法法 收敛性分析 应用：Yuan, Y., Sun, D., &amp; Toh, K. C. (2018). An efficient semismooth Newton based algorithm for convex clustering. arXiv preprint arXiv:1802.07091. Nonconvex + 牛顿法 目标问题：square-root regression problem \min _{\beta \in \mathbb{R}^{n}}\{g(\beta) :=\|X \beta-b\|+\lambda p(\beta)-q(\beta)\} 收敛性证明一般流程：解的存在性、唯一性到算法的收敛性 先证明解的存在性、通过严格凸问题得到解的唯一性、假设存在唯一解，然后收敛到最小解 例子： 存在性证明：A CONVEX VARIATIONAL MODEL FOR RESTORING BLURRED IMAGES WITH MULTIPLICATIVE NOISE; On the convex model of speckle reduction 唯一性：A CONVEX VARIATIONAL MODEL FOR RESTORING BLURRED IMAGES WITH MULTIPLICATIVE NOISE; On the convex model of speckle reduction 收敛性： 问题 牛顿法中，P和D之间与拉格朗日的关系 semismooth到底什么意思 在Split Bregman中，Split Bregman下的任何lambda都是原约束问题的解？]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[变分不等式的收敛性证明]]></title>
    <url>%2Fp%2Ff8c4.html</url>
    <content type="text"><![CDATA[变分不等式变分不等式(VI)的定义： 最优化问题的转化： ​ $x^{*}=\arg \min _{x \in X} f(x)​$ ​ 等价于满足$\operatorname{VI}(\nabla f, X)​$即 ​ $\nabla f\left(x^{}\right)^{T}\left(x^{\prime}-x^{}\right) \geq 0, \forall x^{\prime} \in X$ 优化问题 x^{*} \in \arg \min \{\theta(x)+f(x) | x \in \mathcal{X}\}​ 其中$f(x),\theta(x)$是convex function, $\theta$不一定可微, 等价于： x^{*} \in \mathcal{X}, \quad \theta(x)-\theta\left(x^{*}\right)+\left(x-x^{*}\right)^{T} \nabla f\left(x^{*}\right) \geq 0, \quad \forall x \in \mathcal{X} ​ ​ ​ 优化问题转化VI不等式问题将不同的优化问题转化成等价的VI问题： 一元约束问题$\min \{\theta(x) | A x=b, x \in \mathcal{X}\}$ 等价于拉格朗日：$L(x, \lambda)=\theta(x)-\lambda^{T}(A x-b), \quad(x, \lambda) \in \mathcal{X} \times \Re^{m}$ 转化后等于VI inequality：$w^{} \in \Omega, \quad \theta(x)-\theta\left(x^{}\right)+\left(w-w^{}\right)^{T} F\left(w^{}\right) \geq 0, \quad \forall w \in \Omega$ 二元三元。。 min-max问题: $\min _{x \in \mathcal{X}} \max _{y \in \mathcal{Y}}\left\{\mathcal{L}(x, y)=\theta_{1}(x)-y^{T} A x-\theta_{2}(y)\right\}$ PPA的收敛性用PI不等式证明方程： \min \{\theta(x)+f(x) | x \in \mathcal{X}\}where θ(x) and f(x) are convex but θ(x) is not necessary smooth, X is a closed convex set. 定义 Monotone operator 单调函数（x-y）（F（x）-F(y)）&gt;0， 则F是单调函数 凸函数满足的性质 (x-y)^{T}(\nabla f(x)-\nabla f(y)) \geq 0We say the gradient rf of the convex function f is a monotone operator. min max Lagrangian 和 max min Lagrangian的关系]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Semismooth牛顿法]]></title>
    <url>%2Fp%2F157d.html</url>
    <content type="text"><![CDATA[参考文献 A highly efficient semismooth Newton augmented Lagrangianmethod for solving Lasso problems，论文代码 A sparse semismooth Newton based proximalmajorization-minimization algorithm for nonconvexsquare-root-loss regression problems, h(Ax)+p(x) 论文中的模型：Primal/Dual \text { (P) } \max -\{f(x)=h(\mathcal{A} x)-\langle c, x\rangle+ p(x)\} \text { (P) } \min \{f(x)=h(\mathcal{A} x)-\langle c, x\rangle+p(x)\} \text { (D) } \min \left\{h^{*}(y)+p^{*}(z) | \mathcal{A}^{*} y+z=c\right\}where, $\mathcal{A} \in R^{MN}​$, $p, h : \mathcal{Y} \rightarrow \Re \text { and } p : \mathcal{X} \rightarrow(-\infty,+\infty]​$ are two closed proper convex functions. 对于Dual问题： $h^$ is essentially smooth with $\nabla h^{}$ is locally Lipschitz continuous and directionally diﬀerentiable on int $\operatorname{int}\left(\operatorname{dom} h^{*}\right)$. 对偶问题的拉格朗日$l​$和增广拉格朗日$\mathcal{L}​$ l(y, z, x)=h^{*}(y)+p^{*}(z)-\left\langle x, \mathcal{A}^{*} y+z-c\right\rangle, \quad \forall(y, z, x) \in \mathcal{Y} \times \mathcal{X} \times \mathcal{X} \begin{aligned} \mathcal{L}_{\sigma}(y, z ; x) &=l(y, z, x)+\frac{\sigma}{2}\left\|\mathcal{A}^{*} y+z-c\right\|^{2}, \quad \forall(y, z, x) \in \mathcal{Y} \times \mathcal{X} \times \mathcal{X}\\ &=h^{*}(y)+p^{*}(z) +\frac{\sigma}{2}\left\|\mathcal{A}^{*} y+z-c-\frac{x}{\sigma}\right\|^{2}-\frac{1}{2 \sigma}\|x\|^{2} \end{aligned} 算法框架（第一层循环） 将(18)中的拉格朗日方程转化成单变量拉格朗日方程 Define $\min _{y, z} \Psi(y, z) :=\mathcal{L}_{\sigma}(y, z ; \tilde{x})​$ Define $\psi(y) :=\inf _{z} \Psi(y, z)​$ （18）中的拉格朗日转化成单变量光滑方程 \begin{aligned} \Psi(y) &=\inf _{z}\{ h^{*}(y)+p^{*}(z) +\frac{\sigma}{2}\left\|\mathcal{A}^{*} y+z-c-\frac{x}{\sigma}\right\|^{2}-\frac{1}{2 \sigma}\|x\|^{2}\} \\ &=h^*(y)+\sigma \inf _{z}\left\{\frac{1}{\sigma} p^*(z)+\frac{1}{2}\left\|z-(c-A^*y+\frac{x}{\sigma})\right\|^{2}\right\}-\frac{1}{2 \sigma}\|x\|^{2} \\ &=h^*(y)+\sigma M_{\frac{1}{\sigma}}p^*(c-A^*y+\frac{x}{\sigma})-\frac{1}{2 \sigma}\|x\|^{2} \end{aligned})with $z = \arg\min_z p^(z)/\sigma+\frac{1}{2}\left|z-(c-A^y+\frac{x}{\sigma})\right|^{2}=\operatorname{Prox}_{p^{} / \sigma}\left(x / \sigma-\mathcal{A}^{} \overline{y}+c\right)$ . $\psi(y)$ is a smooth function whose gradient is derived as: \begin{aligned} \nabla\Psi(y) &= \nabla h^{*}(y)-\mathcal{A} \operatorname{Prox}_{\sigma p}\left(\tilde{x}-\sigma\left(\mathcal{A}^{*} y-c\right)\right) \end{aligned} 将 $z =\operatorname{Prox}_{p^{} / \sigma}\left(x / \sigma-\mathcal{A}^{} \overline{y}+c\right)​$ 代入 (18) 得到 \psi(y)=h^{*}(y)+p^{*}\left(\operatorname{Prox}_{p^{*} / \sigma}\left(x / \sigma-\mathcal{A}^{*} y+c\right)\right)+\frac{1}{2 \sigma}\left\|\operatorname{Prox}_{\sigma p}\left(x-\sigma\left(\mathcal{A}^{*} y-c\right)\right)\right\|^{2}-\frac{1}{2 \sigma}\|x\|^{2} Moreau-Yosida regularization and proximal mapping M_{\lambda} f(x) :=\min _{u} f(u)+\frac{1}{2 \lambda}\|u-x\|_{2}^{2} \nabla M_{\lambda} f(x)=\frac{1}{\lambda}\left(x-\operatorname{Prox}_{\lambda f}(x)\right) \operatorname{Prox}_{\lambda f}(x) :=\arg \min _{u \in \mathcal{X}} f(x)+\frac{1}{2 \lambda}\|u-x\|^{2} \operatorname{Prox}_{\lambda p}(x)+\lambda \operatorname{Prox}_{p^{*}(x) / \lambda}(x / \lambda)=x\operatorname{Prox}_{\lambda p}(x)+\lambda \operatorname{Prox}_{p^{*}(x) / \lambda}(x / \lambda)=x $\min _{y, z} \Psi(y, z) $等价于以下单变量非线形方程 (22) \nabla\Psi(y)=0whose the generalized Hessian of $\Psi$ at y is defined as \hat{\partial}^{2} \psi(y) :=\partial\left(\nabla h^{*}\right)(y)+\sigma \mathcal{A} \partial \operatorname{Prox}_{\sigma p}\left(\tilde{x}-\sigma\left(\mathcal{A}^{*} y-c\right)\right) \mathcal{A}^{*}Defining the $H \in \partial^{2} h^{}(y)$ and $U \in \partial \operatorname{Prox}_{\sigma p}\left(\tilde{x}-\sigma\left(\mathcal{A}^{} y-c\right)\right)$, then, we have V :=H+\sigma \mathcal{A} U \mathcal{A}^{*} 利用牛顿法和Hessian 矩阵求解非线形等式(22) (第二层循环) 寻找方向$d$：$V_{j} d+\nabla \psi\left(y^{j}\right)=0$（24） $V \in \hat{\partial}^{2} \psi(y)$, $\hat{\partial}^{2} \psi(y) :=\partial\left(\nabla h^{}\right)(y)+\sigma \mathcal{A} \partial \operatorname{Prox}_{\sigma p}\left(\tilde{x}-\sigma\left(\mathcal{A}^{} y-c\right)\right) \mathcal{A}^{*}$ $V :=H+\sigma \mathcal{A} U \mathcal{A}^{*}​$ 步长：set $\alpha_{j}=\delta^{m_{j}}​$ $y^{j}+\delta^{m} d^{j} \in \operatorname{int}\left(\operatorname{dom} h^{*}\right) \quad \text { and } \quad \psi\left(y^{j}+\delta^{m} d^{j}\right) \leq \psi\left(y^{j}\right)+\mu \delta^{m}\left\langle\nabla \psi\left(y^{j}\right), d^{j}\right\rangle​$ 更新：$y^{j+1}=y^{j}+\alpha_{j} d^{j}$ 迭代终止条件：$\nabla \psi\left(y^{j}\right)$ is sufficiently small 求解线性问题（24）$V_{j} d=-\nabla \psi\left(y^{j}\right)​$ 方法一：利用$V_j​$的稀疏性求出解析解 \left(H+\sigma A U A^{T}\right) d=-\nabla \psi(y), H = LL^T \left(I_{m}+\sigma\left(L^{-1} A\right) U\left(L^{-1} A\right)^{T}\right)\left(L^{T} d\right)=-L^{-1} \nabla \psi(y)考虑简化后的情况： \left(I_{m}+\sigma A U A^{T}\right) d=-\nabla \psi(y) \mbox{with } A U A^{T}=(A U)(A U)^{T}=A_{\mathcal{J}} A_{\mathcal{J}}^{T} \left(I_{m}+\sigma A U A^{T}\right)^{-1}=\left(I_{m}+\sigma A_{\mathcal{J}} A_{\mathcal{J}}^{T}\right)^{-1}=I_{m}-A_{\mathcal{J}}\left(\sigma^{-1} I_{r}+A_{\mathcal{J}}^{T} A_{\mathcal{J}}\right)^{-1} A_{\mathcal{J}}^{T}维度降低：利用U是一个对角由0和1组成的对角矩阵，$\mathcal{O}\left(m^{2} n\right)$-&gt;$\mathcal{O}\left(m^{2} r\right)$-&gt;$\mathcal{O}\left(r^{2} m\right)$ 方法二：Pcg 循环（第三层循环） Do $V_jd^j​$ Until $\left|V_{j} d^{j}+\nabla \psi\left(y^{j}\right)\right| \leq \min \left(\overline{\eta},\left|\nabla \psi\left(y^{j}\right)\right|^{1+\tau}\right)​$ 尝试过的加速办法： 与$\tau$的大小有关 将该循环写成c语言 每次牛顿迭代从上一次d的结果开始。在算法快收敛的时候可以加速。 速度分析 循环一：lnexact Lagrangian methd 循环二：牛顿法迭代求解问题（22） 影响循环二迭代次数的因素： 1.该循环的中止条件: $\left|\nabla \psi_{k}\left(y^{k+1}\right)\right|$ 足够小 2.循环三中的精度$\tau$：该循环的收敛速度与循环三中pcg的精度有关 \left\|y^{j+1}-\overline{y}\right\|=O\left(\left\|y^{j}-\overline{y}\right\|^{1+\tau}\right) 循环三(若使用pcg的方法求解(24))： 影响循环三迭代次数的因素: $\tau$的大小 1.该循环终止条件：$\left|V_{j} d^{j}+\nabla \psi\left(y^{j}\right)\right| \leq \min \left(\overline{\eta},\left|\nabla \psi\left(y^{j}\right)\right|^{1+\tau}\right),\tau=(0,1]$ h(x)+p(Bx) 目标方程： $\min|y-g|+\lambda|\nabla y|_1$ 无法套用论文中$h(Ay)+p(y)​$的Dual方法的原因： 如果$p(y)=\lambda|\nabla y|_1​$，那么p*的显式无法表达。 如果$h(Ay)=\lambda|\nabla y|_1, A=\nabla$, 那么依然无法将问题转化成单变量的smooth问题，因为h项会被保留 $\nabla$的矩阵形式 (补充) 提出模型 $h(y)+p(By), h(y)=|Ay-g|, p(Bx)=|\nabla y|_1​$，$B=\nabla​$ （P）$\min h(y)+p(By)​$, $h(y)=|Ay-g|​$, $p(Bx)=|\nabla y|_1​$ （D）$\min \left\{h^{}(y)+p^{}(z) | \mathcal{B}^{} z+y=c\right\}$, $h^{}(y)=\frac{1}{2}|b+y|^{2}-\frac{1}{2}|b|^{2}$, $p^{*}(z)=I\left\{|z|_{\infty} \leq \lambda\right\}$ 对于D问题，无法转化成单变量的smooth方程，推导见附录(待补充)。因此选择该模型的主问题。 对于(P) $h(y)+p(By)​$，转化成单变量的smooth方程 增广拉格朗日：$\mathcal{L}_{\sigma}(y, z ; x) :=h(y)+p(z)++\frac{\sigma}{2}|\mathcal{B} y-z|^{2}​$ ​ $=h(y)+p(z)+\frac{\sigma}{2}\left|\mathcal{B} y+\frac{x}{\sigma}-z\right|^{2}-\frac{1}{2 \sigma}|x|^{2}​$ 转化后的单变量方光滑方程： $\mathcal{L}_{\sigma}(y ; x) :=h(y)+p\left(\operatorname{Prox} \frac{p}{\sigma}\left(\mathcal{B} y+\frac{x}{\sigma}\right)\right)+\frac{1}{2 \sigma}\left|\operatorname{Prox}_{\sigma p^{*}}\left(\sigma\left(\mathcal{B} y+\frac{x}{\sigma}\right)\right)\right|^{2}-\frac{1}{2 \sigma}|x|^{2}, \overline{z}=\operatorname{Prox}_{\frac{p}{\sigma}}\left(\mathcal{B} y+\frac{x}{\sigma}\right)$ $\nabla \mathcal{L}_{\sigma}(y) :=\nabla h(y)+\mathcal{B}^{} \operatorname{Prox}_{\sigma p^{}}(\sigma(\mathcal{B} y+\frac{x}{\sigma})$ $\partial\left(\nabla \mathcal{L}_{\sigma}(y)\right) :=\partial(\nabla h(y))+\sigma \mathcal{B}^{} \partial \operatorname{Prox}_{\sigma p^{}}\left(\sigma\left(\mathcal{B} y+\frac{x}{\sigma}\right)\right) \mathcal{B}\\=\partial(\nabla h(y))+\sigma \mathcal{B}^{*}\left(I-\partial \operatorname{Prox}_{\frac{p}{\sigma}}\left(\mathcal{B} y+\frac{x}{\sigma}\right)\right) \mathcal{B}​$ 求解$V_{j} d=-\nabla \psi\left(y^{j}\right)​$ 方法一：求逆 其中, $\partial \operatorname{Prox}_{\sigma p^{}}\left(\sigma\left(\mathcal{B} y+\frac{x}{\sigma}\right)\right)​$的稀疏性随着图像的平滑减弱，因此直接利用该稀疏矩阵求逆，*计算立马随着噪声的减弱而变慢。相反，$\partial \operatorname{Prox}_{\frac{p}{\sigma}}\left(\mathcal{B} y+\frac{x}{\sigma}\right)​$的稀疏性随着图像的平滑变稀疏。 $\left(A^TA+\sigma \mathcal{B}^\mathcal{B}-\sigma \mathcal{B}^{}\partial \operatorname{Prox}_{\frac{p}{\sigma}}\left(\mathcal{B} y+\frac{x}{\sigma}\right)\mathcal{B}\right)d=-\nabla \psi\left(y^{j}\right) ​$ Denote $U =\partial \operatorname{Prox}_{\frac{p}{\sigma}}\left(\mathcal{B} y+\frac{x}{\sigma}\right)​$ , $b =-\nabla \psi\left(y^{j}\right) ​$ we have $\left(A^TA+\sigma \mathcal{B}^\mathcal{B}-\sigma \mathcal{B}^{}U\mathcal{B}\right)d=-\nabla \psi\left(y^{j}\right) ​$ 如果没有U，就可以使用FFT快速变化，因为B可以当作[-1 1]构成的卷积 如果对可以对$A^TA+\sigma \mathcal{B}^*\mathcal{B}$进行$LL^T$的分解。可将原问题转化成以下形式后求逆 \left(I_{m}+\sigma\left(L^{-1} A\right) U\left(L^{-1} A\right)^{T}\right)\left(L^{T} d\right)=-L^{-1} \nabla \psi(y)例如在TV denoising的情况下：$A=I$，$B=[B1, B2]$, 则需对$I+\sigma B^*B$进行分解 (目前没有实现这个分解, 所以使用pcg求解该线性等式) 方法二：Pcg 循环（第三层循环） Do $V_jd^j​$ Until $\left|V_{j} d^{j}+\nabla \psi\left(y^{j}\right)\right| \leq \min \left(\overline{\eta},\left|\nabla \psi\left(y^{j}\right)\right|^{1+\tau}\right)​$ 尝试过的加速办法： 将该循环写成c语言 加速的办法，每次牛顿迭代从上一次d的结果开始。在算法快收敛的时候可以加速 实验结果与实验设置 循环一：lnexact Lagrangian methd 循环二：牛顿法迭代求解问题（22） 设置循环二的中止条件为: $\left|\nabla \psi_{k}\left(y^{k+1}\right)\right|$ &lt;{0.005} ps：如果在小于0.005前达到收敛，则终止循环二，进入循环一的下一轮迭代 该循环的收敛速度与循环三中pcg的精度有关: \left\|y^{j+1}-\overline{y}\right\|=O\left(\left\|y^{j}-\overline{y}\right\|^{1+\tau}\right) 循环三(pcg)： 影响循环三迭代次数的因素: $\tau​$的大小 该循环终止条件：$\left|V_{j} d^{j}+\nabla \psi\left(y^{j}\right)\right| \leq \min \left(\overline{\eta},\left|\nabla \psi\left(y^{j}\right)\right|^{1+\tau}\right),\tau=(0,1]$ $\tau=1$:循环三循环(大概20)次，循环二收敛需要*次，循环一需要*次 $\tau=2$:循环三循环*次，循环二收敛需要*次，循环一需要*次 (实验结果需要补充) 实验中存在的问题 循环二和循环三之间的收敛速度没有满足理论值： \left\|y^{j+1}-\overline{y}\right\|=O\left(\left\|y^{j}-\overline{y}\right\|^{1+\tau}\right) ​ 具体的说，循环1中的中间几次迭代中，循环二的收敛速度很慢，没有达到理论值，甚至比线性收敛还要慢。 ​ （循环二的收敛图需要补充）​ Non-convex model1. General Model A nonconvex problem (square-root regression problem) (3) \min _{\beta \in \Re^{n}}\{g(\beta) :=\underbrace{h(X \beta)}_{f(\beta)}+\underbrace{p(\beta)-q(\beta)}_{r(\beta)}\}​ $p : \Re^{n} \rightarrow(-\infty,+\infty] \text { is a proper closed convex function }​$ ​ $q : \Re^{n} \rightarrow \Re \text { is a finite-valued (smooth, not essential) convex function. }$ ​ $ \text {The proximal functions of h and p to be (strongly) semismooth.}$ Examples （等待补充） 因为原general model (3) 转化成模型 (10)/(P): $\text { Given } \sigma&gt;0, \tau&gt;0, \tilde{\beta} \in \mathbb{R}^{n}, \tilde{v} \in \mathbb{R}^{n}, \text { and }\tilde{b} \in \mathbb{R}^{m}$ \begin{aligned} \min _{\beta \in \mathbb{R}^{n}}\{h(\beta ; \sigma, \tau, \tilde{\beta}, \tilde{v}, \tilde{b}) :=&\|X \beta-b\|+\lambda p(\beta)-q(\tilde{\beta})-\langle\tilde{v}, \beta-\tilde{\beta}\rangle \\ &+\frac{\sigma}{2}\|\beta-\tilde{\beta}\|^{2}+\frac{\tau}{2}\|X \beta-\tilde{b}\|^{2} \} \end{aligned}(1) Linearize the concave term: $-q(\beta)$. 因为此项nonconvex (2) Add the proximal term：$\frac{\tau}{2}|X \beta-X \tilde{\beta}|^{2}$. 因为$h(\cdot)$和$p(\beta)$是nonsmooth $\beta^{k+1}=\operatorname{Prox}_{\lambda p / \sigma^{2, k}}\left(\beta^{k}+\left(\nabla q\left(\beta^{k}\right)-X^{T} u^{k+1}\right) / \sigma^{2, k}\right)​$ 问题 [x] $\frac{\sigma}{2}|\beta-\tilde{\beta}|$的作用：因为是PPA算法 [ ] $\frac{\tau}{2}|X \beta-\tilde{b}|^{2}$这个Proximal项的由来 [ ] 针对该非凸模型的算法，解的唯一性和算法的收敛性能否被证明？只要是非凸问题，都无法保障解的唯一性？那么该算法的优点？ [ ] 该类问题本来的解法 2. SSN-based PPM Algorithm PPM algorithm： Proximal majorization-minimization Initialize: $\beta^{0} \approx \underset{\beta \in \mathbb{R}^{n}}{\operatorname{argmin}}\left\{h\left(\beta ; \sigma^{1}, \tau^{1}, 0,0, b\right)\right\}​$ Iteration: step 1 $\beta^{k+1}=\underset{\beta \in \mathbb{R}^{n}}{\operatorname{argmin}}\left\{h\left(\beta ; \sigma^{2, k}, \tau^{2, k}, \beta^{k}, \nabla q\left(\beta^{k}\right), X \beta^{k}\right)+\left\langle\delta^{k}, \beta-\beta^{k}\right\rangle\right\}$ ​ step 2 $\sigma^{2, k+1}=\rho_{k} \sigma^{2, k}, \tau^{2, k+1}=\rho_{k} \tau^{2, k}\text { with } \rho_{k} \in(0,1)​$ Until: If $β_{k+1}​$ satisﬁes a prescribed stopping criterion, terminate; Step 1: solve the dual of the (10) which is（12） \begin{aligned} \min _{u \in \mathbb{R}^{m}} &\left\{\varphi(u) :=\langle u, b\rangle+\frac{\tau}{2}\left\|\tau^{-1} u+\tilde{b}-b\right\|^{2}-\left\|\operatorname{Prox}_{\tau^{-1}\|\cdot\|}\left(\tau^{-1} u+\tilde{b}-b\right)\right\|\right.\\ &-\frac{1}{2 \tau}\left\|\operatorname{Prox}_{\tau \delta_{B}}(u+\tau(\tilde{b}-b))\right\|^{2}+\frac{\sigma}{2}\left\|\tilde{\beta}+\sigma^{-1}\left(\tilde{v}-X^{T} u\right)\right\|^{2} \\ &-\lambda p\left(\operatorname{Prox}_{\sigma^{-1} \lambda p}\left(\tilde{\beta}+\sigma^{-1}\left(\tilde{v}-X^{T} u\right)\right)\right)-\frac{1}{2 \sigma}\left\|\operatorname{Prox}_{\sigma(\lambda p) *}\left(\sigma \tilde{\beta}+\tilde{v}-X^{T} u\right)\right\|^{2} \} \end{aligned} (12)with $\overline{y}=\operatorname{Prox}_{\tau^{-1}| |}\left(\tau^{-1} \overline{u}+\tilde{b}-b\right), \quad \overline{\beta}=\operatorname{Prox}_{\sigma^{-1} \lambda p}\left(\tilde{\beta}+\sigma^{-1}\left(\tilde{v}-X^{T} \overline{u}\right)\right)​$. and $\beta^{k+1}=\operatorname{Prox}_{\lambda p / \sigma^{2, k}}\left(\beta^{k}+\left(\nabla q\left(\beta^{k}\right)-X^{T} u^{k+1}\right) / \sigma^{2, k}\right)​$ (12) 的推导过程 令$y=X \beta-b$将(10)转化成约束问题(11) \min _{\beta \in \mathbb{R}^{n}, y \in \mathbb{R}^{m}}\left\{\|X\beta-b\|+\lambda p(\beta)-\langle\tilde{v}, \beta-\tilde{\beta}\rangle+\frac{\sigma}{2}\|\beta-\tilde{\beta}\|^{2}+\frac{\tau}{2}\|y+b-\tilde{b}\|^{2} \right\}\\ \text{subject to }X \beta-b=y\\ 将约束问题转化成Lagrangian： $L(y,\beta; u) = \left\{|y|+\lambda p(\beta)-\langle\tilde{v}, \beta-\tilde{\beta}\rangle+\frac{\sigma}{2}|\beta-\tilde{\beta}|^{2}+\frac{\tau}{2}|y+b-\tilde{b}|^{2} - \right\}​$ (11) Dual function of (11) g(u):=\inf _{y,\beta} L(y, \beta;u) 对$L(y,\beta; u) ​$进行配方 \begin{aligned} L(y,\beta; u) &= \|y\|+\lambda p(\beta)-\langle\tilde{v}, \beta-\tilde{\beta}\rangle+\frac{\sigma}{2}\|\beta-\tilde{\beta}\|^{2}+\frac{\tau}{2}\|y+b-\tilde{b}\|^{2} - \langle u, y-X \beta+b\rangle\\ &=\{\|y\|+\frac{\tau}{2}\|y+b-\tilde{b}\|^{2}-\langle u,y\rangle\}+\{\lambda p(\beta)-\langle\tilde{v}, \beta-\tilde{\beta}\rangle+\frac{\sigma}{2}\|\beta-\tilde{\beta}\|^{2}+\langle X^T u,\beta\rangle\}-\langle u,b\rangle\\ &=\{\|y\|+\frac{\tau}{2}\|y+b-\tilde{b}-\frac{u}{\tau}\|^{2}\}-\frac{u^2}{2\tau}+\langle \tilde b,u\rangle+\{\lambda p(\beta)+\frac{\sigma}{2}\|\beta-\tilde{\beta}\|^{2}+\langle X^Tu-\tilde{v}, \beta-\tilde{\beta}\rangle+\langle \tilde{\beta},X^Tu\rangle\}\\ &=\{\|y\|+\frac{\tau}{2}\|y+b-\tilde{b}+\frac{u}{\tau}\|^{2}\}+\{\lambda p(\beta) + \frac{\sigma}{2}\|\beta-\tilde{\beta}+\frac{X^Tu-\tilde{v}}{\sigma}\|^{2}\}...\\ &-\frac{u^2}{2\tau}+\langle \tilde b,u\rangle+\langle\tilde{\beta},X^Tu\rangle-\|\frac{X^Tu-\tilde{v}}{\sigma}\|^2 \\ \end{aligned} (12) 的求解：A semismooth Newton‘s method 最小化（12）等价于（12）的一阶条件 \nabla \varphi(u)=\operatorname{Prox}_{\tau^{-1} \|}\left(\tau^{-1} u+\tilde{b}-b\right)-X \operatorname{Prox}_{\sigma^{-1} \lambda p}\left(\tilde{\beta}+\sigma^{-1}\left(\tilde{v}-X^{T} u\right)\right)+b\\ \nabla \varphi(u)=0\qquad(13)\\ Solve nonlinear equation（13）by pcg $\hat{\partial}^{2} \varphi(u) :=\sigma^{-1} X \partial \operatorname{Prox}_{\sigma^{-1} \lambda p}\left(\tilde{\beta}+\sigma^{-1}\left(\tilde{v}-X^{T} u\right)\right) X^{T}+\tau^{-1} \partial \operatorname{Prox}_{\tau^{-1} |}\left(\tau^{-1} u+\tilde{b}-b\right)$ $H:=\sigma^{-1} X U X^{T}+\tau^{-1} V \in \hat{\partial}^{2} \varphi(u)​$ 3. Problem4. Our case For the ROF model: $h(X\beta)=|\nabla \beta|_1$ $p(\beta)=|\beta-b|_2^2,q(\beta)=0​$ 附录 转化成smooth单变量方程的推导过程 \begin{aligned} \Psi(\hat{y}, \hat{z}) &=\inf _{z} \Psi(\hat{u}, z) \\ &=G(\hat{y})+\sigma \inf _{z}\left\{\frac{1}{\sigma} F(z)+\frac{1}{2}\left\|z-K \hat{y}-\frac{x}{\sigma}\right\|^{2}\right\}-\frac{1}{2 \sigma}\|x\|^{2} \\ &=G(\hat{y})+\sigma M(F / \sigma)(K \hat{y}+x)-\frac{1}{2 \sigma}\|x\|^{2} \end{aligned}Using the Moreau-Yosida regularization: M_{\lambda} f(x) :=\min _{u} f(u)+\frac{1}{2 \lambda}\|u-x\|_{2}^{2} \nabla M_{\lambda} f(x)=\frac{1}{\lambda}\left(x-\operatorname{Prox}_{\lambda f}(x)\right) $h(y)+p(By)$对偶问题无法转化成单变量smooth的原因 \begin{array}{c}{\partial \psi(z)=\partial p^{*}(z)-\underline{A x}+\sigma\left(\mathcal{A}^{*} z+y-c\right)} \\ {0 \in \partial \psi(\overline{z})} \\ {0 \in \partial p^{*}(\overline{z})-\mathcal{A} x+\sigma\left(\mathcal{A}^{*} \overline{z}+y-c\right)} \\ {0 \in \sigma\left(\frac{\partial p^{*}(\overline{z})}{\sigma}+\mathcal{A}^{*} z-\left(c-y+\frac{1}{\sigma} \mathcal{A} x\right)\right)}\end{array}$z$无法用proximal表示]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[香港地图]]></title>
    <url>%2Fp%2Facef.html</url>
    <content type="text"><![CDATA[尖沙咀 海港城旁边： DFS，Aqua spirit， 把地铁口也标上 买小熊饼干：在美丽华商厦，是个类似重庆大楼的开放的楼 旺记冰室/一兰拉面： 妈咪鸡蛋仔和薯条/； 冰淇淋；送他们上船； 点心：好运添，唐宫小聚（新式） 火锅：火水爐冰室火鍋 旺角 https://www.openrice.com/zh/hongkong/r-%E7%81%AB%E6%B0%B4%E7%88%90%E5%86%B0%E5%AE%A4%E7%81%AB%E9%8D%8B-%E6%97%BA%E8%A7%92-%E6%B8%AF%E5%BC%8F-%E7%81%AB%E9%8D%8B-r544211/reviews 岛上拍照 购物 看书喝咖啡香港小众有趣的消夏好去处 清单去一个岛徒步/长洲岛/西贡，晚上坐船吃冰淇淋：夏天小清新风 去吃茶餐厅 去坐傍晚的叮叮车，吃shakeshack 去逛香港的超市，买寿司当晚餐，咸蛋黄鸡蛋仔 吃大排档：晚上 搬家+看演唱会 点心：周记点点心， 西安 西安及周边100公里内有哪些冷门但非常值得一去的地方？ 西安值得吃的地方 西安有哪些可以安静地待一下午的地方？ 踩过的雷：top1的六🈴️汤包 市区： 大雁塔：美术馆、回民街、洒金桥、钟鼓楼 西安明城墙： 推荐环墙一周的自行车。西安城墙是中国现存规模最大、保存最完整的古代城垣。现存城墙为眀代建筑,全长13.7千米,位于西安市中心,送我们过去的司机师傅说起城墙那种油然而生发自內心的自豪感满满都是 大明宫遗址公园：这个是热门，但冷门玩法是傍晚去 八仙宫古玩市场，八仙庵，，离永兴坊不远 顺城巷：一侧是巍巍古城墙，一侧是秀丽端庄的明清古建。、食店、酒吧、咖啡屋，更有一些秦腔或相声曲艺社，院门半掩静待听客到访。 环城公园：以护城河为线，在城墙根下围绕着明城墙修建的环城公园，是西安本地人运动、散步、休闲的好去处 较远： 秦岭：东梁山(冷门)；黑河；楼观台；黎元坪。 西岳庙。一般游客都会去华山,很少会去西岳庙。但是西岳庙有着非常精美的明清建筑,而且面积 巨大,文革期间用作军营所以保存的不错,不过这里和华山是联票,如果去华山的话一定要抽时间 去西岳庙。 碑林博物馆。这个根本就不算冷门,不过还是小众一些,书法爱好者的圣地。而且这里面还有昭陵 骏和景云钟。里面的佛像石刻更是精美绝伦。另外碑林旁边有一个卧龙寺,是陕西第一所寺庙, 也可以看看。 关中民俗艺术博物院。坐标南五台 坐标南五台。是一家私人关中民俗博物院,建筑精美,藏品丰富。 很感激有这样的收藏家能够将关中民俗有条理的收藏保存,对外开放, 门票略贵,120元,但还是非常值得去感受一下,尤其是如果身边有外国友人,非常推荐带他们来 这里,绝对会对西安、对中国会有更深一层的认识。 西安吃的马峰小炒（好吃！！但拍得不好看，见谅）： 定家小酥肉 地址：酿皮隔壁 东南亚甑糕 西羊市 面：马虎面馆 连锁店，马虎面馆 连锁店，英子牛肉面 景观路 夜宵：小佐烤肉(开元路总店) 咖啡馆、小情调 曼蒂广场 人不多的一个商场,店其实也不多。但是艺术感很强,经常有一些画或雕塑的小展览。里面的虫儿 咖啡、均记咖啡的装修风格我都很喜欢。就连这里的米家大雨泡馍都是咖啡馆式的木质深色装修风 格,很是雅致,如果请人吃泡馍还不想环境脏乱差的话可以考虑这里。 陕西省图书馆。 曲江书城 书城提供很多座位、小沙发，看书很惬意。]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[模仿胶片色调]]></title>
    <url>%2Fp%2F3acf.html</url>
    <content type="text"><![CDATA[06-14 课程链接：https://m.lizhiweike.com/classroom/12544288 https://pan.baidu.com/s/1QCXJhicXgzQgHirQioIPSw 密码：4yqr 快速模仿胶片色调 @修图师李新颖 时尚博主：小象王国、fashionmodels 摄影师：张家诚 同学们，你们已经看完了这5个摄影师的作品了吧，其实呢，我们除了一些技法上还有思路上的提升，其实我们审美上的一个提升也是非常重要的，那么我们如何去提升审美呢？下面我介绍给大家一些提升自己的方法。 识一些国外的就是国际的，嗯比较厉害的摄影师的名字从认识摄影师的名字开始，然后呢，嗯，如果说你们一开始不知道一些国际摄影师的名字，那么我们可以在国内的一些微博啊，比如说微博博主分享一些时尚大片的微博，博主比如说小象王国啊，还有一个fashion models，然后呢，这两个时尚博主他也是会经常的分享一些国外的大片。 他分享的一些时尚大片的，他会艾特出来摄影师，然后我们找到我们喜欢的摄影师的名字去进行一个标记，然后去给摄影师的作品进行一个分类，比如说嗯，我们首先就是这个摄影师的名字进行一个标记，然后呢，我们再去把他的作品进行一个搜集，比如说那个海边的场景我们给它归类一个文件夹海边，然后呢，我们室内的文件夹就给它归类为室内，然后呢，我们那个草地的，然后就把作品分类为草地，这样子呢，可以有效的去提升我们找片子的效率。 两个时尚博主是我比较经常看的，他不止会分享一些时尚的大片，还会分享一些时尚的资讯，会让我在一个视觉啊，然后呢，还有我的那个审美上面会带给我一些新的灵感 收集摄影师的名字。多看国际标准的摄影作品。如何提高审美，每日不断的阅读，不断的去提升自己的眼界，开拓自己的视野。阅读方式也很重要，要找到自己喜欢的照片，有目的的去欣赏，这很重要。从以下几点进行欣赏。看片的话不是盲目的去看片子，看片子的话要就是有目的的去欣赏去分析 舒服的调子、光影层次、情绪、构图 、肢体语言、有趣的色彩对比、有创意有趣味性的画面或者干净、简洁、统一的画面 王家卫的风格都是色彩比较浓郁的，我们模拟的时候要嗯，要加一些大量的色温，或者是在曲线给他多加红啊，然后加黄就可以模拟出来王家卫那种嗯特别浓郁的色彩，然后记得按不给他，嗯，暗部的话加氢，然后呢加那个黄。 提灰：色阶、可选颜色、曲线都可以提灰 视频中提及的高光、中间调、暗部等小知识，大家可以看下下方这张图~ 胶片照片的特点：高光比较柔，高光比较暗，暗部比较扎实，和中间调比较高亮有对比，线条明显，色彩多乱；胶片会色偏（红色往黄色偏） 数码照片的特点：过渡比较平缓，对比度低，像素高，细节多，丢失光感和立体感；质感退化 数码相机到胶片照片：质感退化，高光调暗，中间调提亮，暗部压实；曝光拉高 中间调提亮 厚重，就是明暗差别大，就像上妆重，高光特别亮，中间调特别暗 1.先调光影 减少对比度 增加曝光 色彩曲线：加深暗部（将曲线底部往右平移），提高中间调（把中间点固定在原来的位置），降低高光(降低) 调色彩，色偏 前期 模仿胶片的照片，前期拍摄的话尽量顺光顺光拍摄，然后呢，尽量曝光稍微亮一点，不要就是曝光稍微要高一档，不要说太低了。]]></content>
      <categories>
        <category>摄影</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[小波域图像去噪]]></title>
    <url>%2Fp%2F2812.html</url>
    <content type="text"><![CDATA[变换变换的作用：方便压缩、计算等 如：方便计算的特征向量基础，Tv=cv。eigenvector basis就是组成T的向量，T就是转换。 变换有：傅立叶变换、小波变换（傅立叶变化不属于小波变化。傅立叶变换和小波变换同属于变换） 傅立叶变换 特点：是正交基。正交基方便求出基的系数。 局限性 最擅长的是把一维的，类三角波连续变量函数信号映射到一维系数序列上，但对于突变信号或任何高维的非三角波信号则几乎无能为力。 如： 小波变换 参考 小波变换完美通俗讲解系列之 （一） 小波变换完美通俗讲解系列之 （二） 波的定义 波：在时间域或者空间域的震荡方程 小波：集中在时域某一点的波；优点，能够分析瞬时时变信号；实现，通过对小波的伸缩平移对函数信号进行多尺度细分。 小波的特点 两两正交，归一化。 小波级数的展开同时在时域和频率上进行，也就是对应伸缩(频域)和平移(覆盖时域)，傅立叶变换只在频域。 小波的构成：父小波和母小波的平移伸缩；scaling function+wavelet function(mother) mother wavelet：母小波 father wavelet：scaling function/父小波/尺度函数 对任意V_j的function可以分解为： （1）scaling fucntion的形式： \begin{array}{c}{\varphi_{j, k}} \end{array}（2）第二种就是它上一个子空间的basis以及上一级子空间的wavelet function \begin{array}{c}{\varphi_{j-1, k}} \\ {\psi_{j-1, k}}\end{array}（3）一直利用上上..级的scaling function，则得到小波展开形式： f(t)=\sum_{k=-\infty}^{\infty} c_{k} \varphi(t-k)+\sum_{k=-\infty}^{\infty} \sum_{j=0}^{\infty} d_{j, k} \psi\left(2^{j} t-k\right)其中，$\varphi(t)$是父小波，$\psi_{j, k}(t)$是母小波。scaling function不是凭空插进去的，而是通过不断的嵌套迭代出来的 为什么使用第3种方式（小波变换）来表达信号 计算 那为什么我们最后选定的是这种选取方式呢?实际上，刚才介绍的这个性质已经告诉我们，对于任何的scale j0，我们都可以给我们的signal space找到一组orthonormal basis，这个basis是通过组合scale j0上的scaling function以及所有在scale j，j&gt;j0上的wavelets得到的。这样，基于这个orthonormal basis，所有信号空间中的信号都可以写成组成这个basis的functions的线性组合： \begin{array}{l}{c_{j_{0}, k}=\left\langle s(n), \varphi_{j_{0}, k}(n)\right\rangle} \\ { d_{j, k}=\left\langle s(n), \psi_{j, k}(n)\right\rangle}\end{array} 两种函数相当于高通滤波和低通滤波的作用 wavelet function和scaling function背后的物理意义了：wavelet function等同于对信号做高通滤波保留变化细节，而scaling function等同于对信号做低通滤波保留平滑的shape! scaling function 和 MRA的关系(scaling function在小波变换中的作用和意义） 在不同的子空间，对于同一个信号就有不同的诠释。诠释最好的当然是V3，完全不损失细节。这就是多解析度的意义。我们可以有嵌套的，由scaling function演变的basis function集合，每一个集合都提供对原始信号的某种近似，解析度越高，近似越精确。 物理意义：做低通滤波 小波变换的计算复杂度(还没理解) 从信号算出展开系数a需要很方便。普遍情况下，小波变换的复杂度是O(Nlog(N))，和FFT相当。有不少很快的变换甚至可以达到O(N)，也就是说，计算复杂度和信号长度是线性的关系。小波变换的等式定义，可以没有积分，没有微分，仅仅是乘法和加法即可以做到，和现代计算机的计算指令完全match。 哈尔小波是小波变换的一种。以哈尔小波为例 如：[9 7 3 5 ]-&gt;[8 4 1 -1]-&gt;[6 2 1 -1] 小波变换的基本流程 选取合适的wavelet function和scaling function，从已有的信号中，反算出系数c和d。 对系数做对应处理 从处理后的系数中重新构建信号。 小波变换的应用：系数处理 应用有压缩、去噪、水印、图像融合等等 例如：比如图像或者视频压缩，就希望选取能将能量聚集到很小一部分系数中的小波，然后抛弃那些能量很小的小波系数，只保留少数的这些大头系数，再反变换回去。这样的话，图像信号的能量并没有怎么丢失，图像体积却大大减小了。 小波去噪的原理 小波分解树(以Haar小波为例) 由高频和低频组成 Matlab的实现（未完成，参考《图像处理中的数学》） wavedec2 小波域去噪综述 问题：是否所有小波域下的去噪方法都利用了稀疏性，所有小波转化都是为了得到稀疏性？ 小波域的图像降噪 基于filter,（利用sparsity） H. Zhang, Aria Nosratinia, and R. O. Wells, Jr., “Image denoising via wavelet-domain spatially adaptive FIR Wiener filtering”, in IEEE Proc. Int. Conf. Acoust., Speech, Signal Processing, Istanbul, Turkey, June 2000. 利用小波奇异检测特性将信号与噪声分开。Mallat, 1992。计算量大，收敛缓慢，产生振荡和不稳定 Mallat, S., &amp; Hwang, W. L. (1992). Singularity detection and processing with wavelets. IEEE transactions on information theory, 38(2), 617-643. 利用小波系数阈值收缩法来分开信号和噪声。Donoho，1992。Gibbs phenomena in the neighborhood of discontinuities – 即不连续点周围的信号能量会在一定尺度的范围上来回波动-to the lack of translation invariance of the wavelet basis。 (都不是MRA-based tight frame，那么是MRA-based tight frame什么又是其他tight frame) 改进Gibbs：R.R. Coifman and D.L. Donoho提出了平移不变量算法可有效地避免这种现象的发生 首先让含有噪声的原始信号进行多次循环平移（比如进行 n 次）,其次运用阈值算法对平移后的信号进行去噪处理,然后再平均去噪的信号,此称为“平移-去噪-平均”的平移不变量算法的原理。 \overline{T}\left(x,\left(S_{h}\right)_{h \in H_{n}}\right)=A v e_{h \in H_{k}} S_{-h}\left(T\left(S_{h}(x)\right)\right)[57] R.R. Coifman and D.L. Donoho, Translation-invariant de-noising, Lecture Notes in Statistics-New York-Springer Verlag (1995), 125–125. 贝叶斯方法去噪(利用sparsity了没有？)，需要利用先验证模型 先验模型是小波系数先验模型：利用联合分布，GGD 先验模型是小波系数的空间局部作用关系（马尔可夫模型）：HMM （基于框架的wavelet frame在图像还原中有很大的应用，谁最先先提出的）: wavelet frame. （还有一种说法）引自基于稀疏表示的小波去噪_朱杰.pdf 基于多尺度分析的紧框架结构 (MRA-based tight frame method): The community’s effort to develop redundant wavelet systems that have sparse approximations for various classes of functions has led to the development of the MRA-based wavelet frames.（those tight wavelet frames generated via a multiresolution analysis）（详见基于小波框架的变分模型） tight wavelet frame: 是一种变分法，因为u = Wt (Wu). u在W下不唯一。所以有三种方法来获取目标图像的稀疏近似值。 Therefore, there are three formulations for the sparse approximation of the underlying images; namely, the analysis based approach, the synthesis based approach and the balanced approach. Analysis based：The analysis based approach was ﬁrst proposed in [84, 170]. \min _{u \in \mathbb{R}^{n}} \frac{1}{2}\|A u-f\|_{D}^{2}+\|\mbox{diag}(\lambda) W u\|_{1}[170] J.L. Starck, M. Elad, and D.L. Donoho, Image decomposition via the combination of sparse representations and a variational approach, IEEE transactions on image processing 14 (2005), no. 10, 1570–1582. [84] M. Elad, J.L. Starck, P. Querre, and D.L. Donoho, Simultaneous cartoon and texture image inpainting using morphological component analysis (MCA), Applied and Computational Harmonic Analysis 19 (2005), no. 3, 340–358. synthesis based: The synthesis based approach was first introduced in [66, 86, 87, 90, 91]. \min _{\alpha \in \mathbb{R}^{m}} \frac{1}{2}\left\|A W^{\top} \alpha-f\right\|_{D}^{2}+\|\mbox{diag}(\lambda) \alpha\|_{1}[90]M.A.T. Figueiredo and R.D. Nowak, An EM algorithm for wavelet-based image restoration, IEEE Transactions on Image Processing 12 (2003), no. 8, 906–916. [91]A bound optimization approach to wavelet-based image deconvolution, Image Processing, 2005. ICIP 2005. IEEE International Conference on, vol. 2, IEEE, 2005, pp. II–782. [86] M.J. Fadili and J.L. Starck, Sparse representations and bayesian image inpainting, Proc. SPARS 5 (2005). [66] I. Daubechies, G. Teschke, and L. Vese, Iteratively solving linear inverse problems under general convex constraints, Inverse Problems and Imaging 1 (2007), no. 1, 29. [87]MJ Fadili, J.L. Starck, and F. Murtagh, Inpainting and zooming using sparse representations, The Computer Journal 52 (2009), no. 1, 64. balanced method：The balanced approach was ﬁrst used in [34, 36] for high resolution image reconstruction. \min _{\alpha \in \mathbb{R}^{m}} \frac{1}{2}\left\|A W^{\top} \alpha-f\right\|_{D}^{2}+\frac{\kappa}{2}\left\|\left(I-W W^{\top}\right) \alpha\right\|_{2}^{2}+\|\mbox{diag}(\lambda) \alpha\|_{1}求解算法：the proximal forward and backward splitting algorithm [34]R.H. Chan, T.F. Chan, L. Shen, and Z. Shen, Wavelet algorithms for high-resolution image reconstruction, SIAM Journal on Scientiﬁc Computing 24 (2003), no. 4, 1408–1432. [36]Tight frame: an eﬃcient way for high-resolution image reconstruction, Applied and Computational Harmonic Analysis 17 (2004), no. 1, 91–115. 小波域去噪综述 小波域去噪是利用信号稀疏表达的一个代表性的方法。此类方法主要就是对图像转化到小波域后的系数进行处理，再将处理后的小系数还原到空间域，从而得到复原后的图像。小波域的去噪方法大致可以分为三类：奇异值检测、阈值收缩以及基于贝叶斯的模型。其中小波阈值去噪由Do在在1992提出来， 是最被广为学习的一种方法。它的工作原理是根据噪声和自然图像在频率段不同的表现形式(噪声呈现出高频小幅值)，通过设定阈值将噪声从噪声图像中区分出来，并将噪声系数还原为0，从而消除噪声。大量论文针对阈值的选择进行了研究。然而，通过阈值收缩的方法，在去噪的同时容易抹去一些图像的高频信息，因此在图像不连续的区域容易产生振铃(Gibbs)的缺陷。R.R. Coifman and D.L. Donoho提出一种平移不变量算法，可有效减少此类缺陷。另一方面，随着框架理论的发展，小波紧框架系统被证明了是一种有效的稀疏逼近分段光滑图像的系统。因此，小波紧框架变换在图像恢复问题中应用十分广泛 [] 。( 基于多尺度分析的小波紧框架开始被成功地用于解决图像复原问题。基于小波框架的变分模型[14-19]被成功应用于图像去噪中，其中稀疏性的特点作为约束项。)然而，需要处理的图像是多种多样的，并没有一个静态的小波紧帧系统能够很好地处理恢复它们。因此Cai在[1]中提出了一种由图片数据驱动的设计tight frame wavelet的方法，来解决一类tight frame只能解决一类图片的问题。 [1] Cai, J. F., Ji, H., Shen, Z., &amp; Ye, G. B. (2014). Data-driven tight frame construction and image denoising. one of the most studied coefficients with small magnitude can be considered as pure noise and should be set to zero. redundant wavelet systems that have sparse approximations for various classes of functions has led to the development of the MRA-based wavelet frames A number of papers were proposed to select the threshold.a number of methods differs in the selection of the threshold parameter. As for the coeﬃcients with small magnitude can be considered as pure noise and should be set to zero The sparsity is later incorporated in the variational method. The most investigated domain in denoising using Wavelet Transform is the non-linear coefficient thresholding based methods. Most of the wavelet shrinkage literature is based on methods for choosing the optimal threshold which can be adaptive or non-adaptive to the image. generates spurious blips, better known as artifacts wavelet thresholding是由Donoho首先在1992提出来的，在这之前在wavelet domain的去噪也是有的。如H. Zhang, Aria Nosratinia, and R. O. Wells, Jr., “Image denoising via wavelet-domain spatially adaptive FIR Wiener filtering”, in IEEE Proc. Int. Conf. Acoust., Speech, Signal Processing, Istanbul, Turkey, June 2000. Wavelet-based denoising aims to decompose the signal into the by high-frequency filter and low-frequency filter. {As for the coeﬃcients with small magnitude can be considered as pure noise and should be set to zero.} As for the detail coeﬃcients of the noise presented as high frequency with small magnitude while a clean image tend to be many zeros. Thus stronger sparse expression: wavelet tight frame (limitation: not adaptive, a class of ) 小波阈值去噪 问题 小波阈值去噪，选择的是哪种小波变换，属于tight frame吗，那么属于MRA-based tight frame吗。 原理 The coefficients of the wavelet transform are usually sparse. That is, most of the coefficients in a noiseless wavelet transform are effectively zero. Therefore, we may reformulate the problem of recovering f as one of recovering the coefficients of f which are relatively ”stronger” than the Gaussian white noise background. That is, coefficients with small magnitude can be considered as pure noise and should be set to zero. The approach in which each coefficient is compared with a threshold in order to decide whether it constitute a desirable part of the original signal or not, is called waveletthresholding. 过程 transform-based thresholding working in three steps: Transform the noisy data into an orthogonal domain. Apply soft or hard thresholding to the resulting coefficients, thereby suppressingthose coefficients smaller than a certain amplitude. Transform back into the original domain. 两种分类方法 全局阈值和自适应阈值 软阈值和硬阈值 soft-thresholding几乎用于所有的算法。Hard-thresholding会产生一种spurious blips的缺陷，as a result of unsuccessful attempts of removing moderately large noise coefficients。 阈值的选择影响很大 Large threshold lead to the details lost. Small threshold lead to the noise unclean. Reviews of literature：(1990s) Wavelet thresholding and wavelet shrinkage：VisuShrink，SureShrink，BayesShrink, NeighBlock [VisuShrink 1994] David L. Donoho and Jain M. Johnstone. Ideal spatial adaptation by wavelet shrinkage. Biometrika, 81(3):425–455, 1994. 3 全局阈值 [SureShrink 1995] David L. Donoho and Iain M. Johnstone. Adapting to unknown smoothness via wavelet shrinkage. Journal of the American Statistical Association, pages 1200–1224, 1995. 第一个adaptive [BayesShrink 2000] Martin Vetterli S Grace Chang, Bin Yu. Adaptive wavelet thresholding for image denoising and compression. IEEE Transactions on Image Processing, 9(9):1532–1546, Sep 2000 [NeighBlock 2001] T.T. Cai and B.W. Silverman. Incorporating information on neighbouring coefficients into wavelet estimation. Sankhya, Series A, 63, 2001 小波变分模型基于小波框架的变分模型[14-19]被成功应用于图像去噪中。 研究表明，基于小波框架的变分模型比其他变分模型 例如 ROF 模型更好，这是因为小波框架的多分辨率结构和冗余。 Chan, R. H., Chan, T. F., Shen, L., &amp; Shen, Z. (2003). Wavelet algorithms for high-resolution image reconstruction. SIAM Journal on Scientific Computing, 24(4), 1408-1432. Cai, J. F., Osher, S., &amp; Shen, Z. (2009). Split Bregman methods and frame based image restoration. Multiscale modeling &amp; simulation, 8(2), 337-369. Dong, B., &amp; Shen, Z. (2010). MRA based wavelet frames and applications. IAS Lecture Notes Series, Summer Program on “The Mathematics of Image Processing”, Park City Mathematics Institute, 19． Chan, R., Shen, L., &amp; Shen, Z. (2005). A framelet-based approach for image inpainting. Res. Rep, 4, 325. tight-frame Cai, J. F., Osher, S., &amp; Shen, Z. (2009). Linearized Bregman iterations for frame-based image deblurring. SIAM Journal on Imaging Sciences, 2(1), 226-252. J.-F. Cai, S. Osher, and Z. Shen, “Split Bregman methods and frame based image restoration,” Multiscale Model. Simul., vol. 8, no. 2, pp. 337–369, Dec. 2010. Cai, J. F., Dong, B., Osher, S., &amp; Shen, Z. (2012). Image restoration: total variation, wavelet frames, and beyond. Journal of the American Mathematical Society, 25(4), 1033-1089. Cai, J. F., Ji, H., Shen, Z., &amp; Ye, G. B. (2014). Data-driven tight frame construction and image denoising. Applied and Computational Harmonic Analysis, 37(1), 89-105. 最近建立了小波框架和变分模型之间的联系，并提出了一种数据驱动紧框架， 该框架比以往的模型更能精确地重构图像。code++ 提出了一种从图片本身设计tight frame wavelet的方法。来解决一类tight frame只能解决一类图片的问题。 代码实现 小波去噪的基本原理 影响去噪的因素 域值的选择，小波的选择，分解层次的选择 [2014 Cai] 《MRA-Based Wavelet Frames and Applications》[cam11-22] on wavelet frame based image restoration [35, 36, 37, 38, 39, 40, 41, 42, 43, 21]. Split Bregman methods and frame based image restoration. Therefore, there are mainly three formulations utilizing the sparseness of the wavelet frame coeﬃcients, namely analysis based approach, synthesis based approach, and balanced approach. Detailed and integrated descriptions of the three approaches can be found in [34]. [34] 一本书 B. Dong and Z. Shen, “MRA Based Wavelet Frames and Applications,” IAS Lecture Notes Series, Summer Program on “The Mathematics of Image Processing”, Park City Mathematics Institute, 2010 The analysis based approach was ﬁrst proposed in [84, 170]. [84] M. Elad, J.L. Starck, P. Querre, and D.L. Donoho, Simultaneous cartoon and texture image inpainting using morphological component analysis (MCA), Applied and Computational Harmonic Analysis 19 (2005), no. 3, 340–358. [170] J.L. Starck, M. Elad, and D.L. Donoho, Image decomposition via the combination of sparse representations and a variational approach, IEEE transactions on image processing 14 (2005), no. 10, 1570–1582. 讲述了一种结合变分和转换与 去噪综述 参考文献： Survey of Image Denoising Techniques 小波域图像降噪概述 B. Dong and Z. Shen, “MRA Based Wavelet Frames and Applications,” IAS Lecture Notes Series, Summer Program on “The Mathematics of Image Processing”, Park City Mathematics Institute, 2010 模极大，域值，平移不变 NLM，BM3D去噪原理 维纳滤波器的时域解 维纳滤波的频域解(尚未找到) 低通滤波 小波变换去噪基础知识整理 综述 根据不同去噪方法的实现原理，我们将去噪方法大致分为以下五类：传统滤波法，基于稀疏表达约束法，基于图像自相似的方法，变分法，基于马尔可夫模型的法，以及目前的深度学习方法。每一类方法都可以在空间域以及转化域来实现。 传统滤波法包括均值滤波，中值滤波 [3] ，卡尔曼滤波 [4] ，维纳滤波[5]等。 均值滤波(Mean)是一种最简单的滤波器，其利用邻域内像素的平均值作为替换值来消除图像的孤立点噪声。通常伴有过模糊的问题。另一种均值滤波通过，通过对样本多次观察取平均来降噪。 维纳滤波(Weiner)是由Weinar在二十世纪四十年代提出来，通过最小均方差寻找准则，从噪声图像中提取最佳线性滤波的方法，因此也叫最小均方滤波/最佳线性滤波器。 \begin{array}{c}{ y(n)=\hat{s}(n)=\sum_{m=0}^{+\infty} h(m) x(n-m)} \\ \min_{x}{E\left[e^{2}(n)\right]=E\left[\left(s(n)-\sum_{m=0}^{+\infty} h(m) x(n-m)\right)^{2}\right]}\end{array}其中h(m)是无污染信号，x(n)是线性滤波。 Wiener, Norbert (1949), Extrapolation, Interpolation, and Smoothing of Stationary Time Series. New York: Wiley. 比均值滤波的效果好，在高斯噪声中的效果最好，计算量大。也不能用于噪声为非平稳的随机过程 中值滤波器(Median)是一种非线性滤波，最早由Tukey和 Pratt在1974和1978年提出 [1] [2]，并被广泛用于去噪问题[3]。这个方法，用领域内像素的中值作为像素值。最显著的优点是在临近像素显著不同区域内，让像素的灰度值与其邻近像素更加接近。缺点，在纹理复杂的图像中依然有模糊的问题。在点线尖的纹理较多的地方表现不好。 The one-dimensional median filter was devised by Tukey [1]. Some discussion of it, and an extension to two dimensions, is given by Pratt [2]. [1] Tukey. J.W. Exploratory Data Analysis. Addison-Wesley, Reading, Mass., 1974. [2] William K. Pratt, Digital image processing, John Wiley &amp; Sons, Inc., New York, NY, 1978 [3] Brownrigg D R K, “The Weighted Median Filter,” Communications of the Acm,1984, 27(8):807-818. 其他的滤波还有在转化域的低通滤波法，通过去掉傅立叶变换后中的高频成分，逆转化后得到复原图像。 Adel Sedra &amp; Peter Brackett， Filter Theory and Design, Active and Passive,Matrix Publisher, Oregon 1978 小波域去噪见小波域去噪综述 变分法是另外一大类重要的分支。它将病体问题转化成一个最小化方程问题。目标方程由一个包真项和正则项组成，保真项是 ，正则项，去噪的结果就是通过优化算法获得的方程解。在不同的正则项中，全变分最受欢迎。 空间域：全变分 基于小波框架的变分模型(是不是都属于wavelet frame) NLM是第一个基于相似块的较为现代的方法。是在2005年由Baudes首先提出用来去噪的方法[1]。他通过搜寻与目标像素具有相似块的相似点作为参考值，对这些选择的像素点均值，权重由相似度决定，对目标像素进行去噪。与传统的滤波法不同，该方法利用了整幅图像的冗余性，比较好地去掉图像中存在的高斯噪声。 [1] Buades, A., Coll, B., &amp; Morel, J. M. (2005, June). A non-local algorithm for image denoising. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05) (Vol. 2, pp. 60-65). IEEE.（BM3D） [2] Dabov, K., Foi, A., Katkovnik, V., &amp; Egiazarian, K. (2007). Image denoising by sparse 3-D transform-domain collaborative filtering. Image Processing, IEEE Transactions on 16 (8), pp. 2080-2095. patch-based methods 为了完备性，其他去噪方法还有基于马尔可夫模型的等。现在的方法大多是将这几类方法进行结合使用，来获得更好的效果。例如，最先进的BM3D主要就是NLM，小波滤波和维尼滤波的组合。整个过程有两步组成，第一个流程首先通过NLM相似块分组，第对每一个相似块组进行小波硬阈值处理后逆转换回图像块，将这些块进行加权后融合到原来的位置，得到初步去噪结果。第二个流程对噪声图和初步去噪结果分别进行相思的分组, 从离散余弦变换后的噪声块中提取维纳滤波系数对初步去噪块进行滤波处理后，逆转化后融合成最后的去噪结果。 补充 基于稀疏表达（sparsity approximation）的去噪方法https://wenku.baidu.com/view/ed8fc817c5da50e2524d7fe6.html 基于全局图像稀疏去噪 基于稀疏字典的图像去噪方法 见小波域的去噪综述 MGA 去噪方法中的稀疏性应用 ICA 去噪方法中的稀疏性应用 2010 年， Anjali 等人对 ICA 技术去噪进行了综述，指出 Fourier 方法局限于频率，小波变换虽能同时在空间域与频率 域，但都不具有数据的自适应性； 而 ICA 方法能从高阶去分析 多方向数据内在的适应性，噪声被认为是高斯随机变量，而图 ［61］ 像数据则是非高斯随机变量 马尔可夫模型 MRA，Wavelet frame, MRA-based wavelet frame，MRA-based tight wavelet frame(generalization) 的发展过程 小波分析已用于多个领域，如信号处理，图像分析等方面，而框架理论是小波分析的一个重要工具。框架理论最初是由 Duffin 和 Schaffcf 在 1952 年研究非调和 Fourier 级数时提出来的，在最开始提出的时 候，框架并没有广泛地引起其他学者的研究兴趣。直到 1986 年，Daubechies、Grossmann 和 Meyer 对框架理论有了突破性的研究，至此框架理论才开始吸引了大批学者的关注。近些年来，在框架理论的研究 过程中，用到了算子理论以及 Banach 空间理论。直到 D. R. I. Arson、Deguang Han 和 Xingde Dai 等人把 算子代数理论运用到框架的研究中，框架理论研究才更上了一个层次，并从整体上把握和研究了框架和 基的性质 。至此，结合了算子理论的框架理论快速发展，其性质以及应用得到更加广泛地研究与推广。 Tight frame,1952, Duffin [80] R.J. Duﬃn and A.C. Schaeﬀer, A class of nonharmonic Fourier series, Transactions of the American Mathematical Society 72 (1952), no. 2, 341–366. Wavelet frames (without a multiresolution structure) (see e.g. [61, 132]), [61]Ten lectures on wavelets, vol. CBMS-NSF Lecture Notes, SIAM, nr. 61, Society for Industrial Mathematics, 1992. [132] A wavelet tour of signal processing, vol. 2nd ed. New York: Academic, Academic press, 1999 and Applications: eg. Tight wavelet frames derived from over sampled orthonormal wavelet basis are already used in noise removal by [57, 77]. [57] R.R. Coifman and D.L. Donoho, Translation-invariant de-noising, Lecture Notes in Statistics-New York-Springer Verlag (1995), 125–125. [77] De-noising by soft-thresholding, IEEE transactions on information theory 41 (1995), no. 3, 613–627. De-Noising using the traditional orthogonal wavelet transform. MRA这个概念由Mallat在1989提出。最常用來分析離散小波變換〈DWT〉或是驗證快速小波轉換〈FWT〉理論的方法 [131] S.G. Mallat, Multiresolution approximations and wavelet orthonormal bases of L 2 (R), Transactions of the American Mathematical Society 315 (1989), no. 1, 69–87. MRA-based compactly supported orthonormal wavelet systems, Daubechies [60]. MRA-based compactly supported orthonormal wavelets of [60]. [60] Daubechies, Orthonormal bases of compactly supported wavelets, Commun. Pure Appl. Math. 41 (1988), no. 7, 909–996. MRA-based tight wavelet frames.(a generalization) , 所以也有不是MRA-based [158] Aﬃne Systems in L 2 (R d ): The Analysis of the Analysis Operator, Journal of Functional Analysis 148 (1997), no. 2, 408–447. MRA-based wavelet 论文中在第五章介绍了不同应用中的具体模型 We discuss the model proposed in [18] on blind deblurring (motion deblurring to be speciﬁc) problems. we present a frame based image segmentation model with a fast algorithm for the general image segmentation problems of [71]. we discuss the model proposed by [112] on reconstruction of scenes (visible surfaces) from scattered, noisy and possibly sparse range data (point clouds). 问题 [ ] 在做wavelet domain denoising的综述中，从来没有提到变分模型；wavelet frame的综述中，从来没有提到过thresholding的方法。 框架理论最初是由 Duffin 和 Schaffcf 在 1952 年研究非调和 Fourier 级数时提出来的，在最开始提出的时 候，框架并没有广泛地引起其他学者的研究兴趣。直到 1986 年，Daubechies、Grossmann 和 Meyer 对框 架理论有了突破性的研究，至此框架理论才开始吸引了大批学者的关注。近些年来，在框架理论的研究 过程中，用到了算子理论以及 Banach 空间理论。直到 D. R. I. Arson、Deguang Han 和 Xingde Dai 等人把 算子代数理论运用到框架的研究中，框架理论研究才更上了一个层次，并从整体上把握和研究了框架和 基的性质 。至此，结合了算子理论的框架理论快速发展，其性质以及应用得到更加广泛地研究与推广。 [ ] 基于稀疏表示的小波去噪，是否有不基于稀疏的小波去噪，我感觉小波就是利用了稀疏 。那么稀疏除了小波域，还有没有其他方法 [ ] 傅立叶变化,余弦变化,小波变化同属于转化，都是在搞基。其中小波的特点是稀疏。 紧框架也是一种转化，特点是冗余基，系属性更好。框架理论：包括傅立叶变化，小波变化。 基于多解析度的紧框架：具有快速重构和分解的优点。 [ ] MRA，Wavelet frame, MRA-based wavelet frame，MRA-based tight wavelet frame(generalization) 的发展过程 [ ] 图像有哪些wavelet transformation有哪些，tight frame有哪些，怎么样算属于MRA(Multi-resolution Analysis) [ ] 如何选择这些不同的变换 [ ] (2D)DT-CWT是什么、(1D)DT-WT 这两个是什么，如何用matlab实现，应该是MRA-based tight frame的一种 [ ] 基于小波框架的变分模型哪篇论文中最先提出。参照MRA-Based Wavelet Frames and Applications的文献回顾即可 \left(\mathrm{P}_{1}\right) \quad \min _{x \in \mathbb{R}^{n}}\|x\|_{\ell_{1}} \quad \text { subject to } \quad y=\Phi x [x] 未选中什么是振铃现象，什么是Gibbs现象，图像中如何表现 两者形容的是同一种现象。振铃效应（Ringingeffect）是影响复原图像质量的众多因素之一，其典型表现是在图像灰度剧烈变化的邻域出现类吉布斯（Gibbs）分布 所谓“振铃”，就是指输出图像的灰度剧烈变化处产生的震荡，就好像钟被敲击后产生的空气震荡。如下图： [ ] CWT是什么：continuous wavelet transform [ ] 图像有哪些wavelet transformation，去噪应该选择哪种转变（Haar是一种） [ ] 是否所有小波域下的去噪方法都利用了稀疏性，所有小波转化都是为了得到稀疏性？ 实验记录利用non-convex tight frame去噪记录 参考文献：Please cite as: A. Parekh and I. W. Selesnick. Convex Denoising Using Non-convex Tight Frame Regularization, IEEE Signal Processing Letters, 22(10): 1786-1790, Oct. 2015. 参考代码：https://github.com/aparek/cncTightFrame/blob/master/Documentation/demo2D.pdf 结论1: tf在噪声50的情况下，不能必过TV，会有模糊的效果。 L1-tightframe 0.8/0.9/1.0 nonconvex tightframe 1.0/1.1/1.2 结论2: 在噪声低15的情况下 nonconvex的正则项可以超过TV 结论3: 在噪声35的情况下，复杂的图像很模糊，但是psnr依旧可以超过TV， PSNR_x = 25.9576 26.2444 26.1463 25.9175 25.6507 25.3786 换个W个会好些吗 参考文献 小波、框架 A short introduction to frames, Gabor systems, and wavelet systems 一个较好理解小波的ppt]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
      <tags>
        <tag>denoising</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[健身清单]]></title>
    <url>%2Fp%2F4221.html</url>
    <content type="text"><![CDATA[健身list 臀部 天鹅颈 双下巴 脖子]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[电影清单]]></title>
    <url>%2Fp%2Fab3e.html</url>
    <content type="text"><![CDATA[32部无论是色彩、构图还是场景都超棒的好电影]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>电影</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[美妆笔记]]></title>
    <url>%2Fp%2F13be.html</url>
    <content type="text"><![CDATA[刷头 基础刷头 眼影刷包括：大号打底铺色(偏硬, 比较宽)，晕染(偏软一点)，眼线刷(偏硬)。 眼妆 眼妆的步骤 眼影-眼线-假睫毛-夹睫毛-睫毛液 防晒-隔离(修正肤色、保湿等不同作用)-遮瑕-粉底液-眼妆-鼻影-修容-定妆 (眉毛在哪一步) 眼线的画法 假睫毛的粘贴 睫毛夹 遮泪沟 工具，innisfree遮瑕刷，橘色套装也包含 一支遮瑕膏层层叠加法 歌剧魅影6色遮瑕，橘色刷子 遮瑕评测 发色2019流行“不饱和发色” 2019年最夯最显白的TOP10发色！ 面膜推荐CNP黄色和蓝色 墨镜选择不同脸型选择墨镜]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[理财知识]]></title>
    <url>%2Fp%2Fe897.html</url>
    <content type="text"><![CDATA[信用卡 信用卡周期 statement balance是上个周期的欠款 account balance是目前总欠款 = 上个周期的欠款+上个周期-到当日的欠款 我的还款方式。月尾付account balance 付完之后account balance何时更新。假设6.3号付了account balance，6.3号之前信用卡还有一笔支出，还未显示在account balance中。都是按照出帐日期算的。 current balance是按照出帐日期算的，并不是按照消费当天的日期算的。如果消费当日在上一个周期，而出帐日是在下个周期，则属于下个周期。一般出帐时间需要两天。 保险知识 some links 对保险的初步认识 医疗险0免赔额比1w免赔额好吗 社保和其他保险 香港保险和内地保险 我们家的保险 老爸的保险 公司的社会养老保险：五险一金 新华保险：寿险（ 交完了） 老妈的保险 生日： 社会养老保险：包括养老保险+医疗保险 我 我的中国平安的人身保险 我妈考虑想买的 新华康健华贵B：这款百万医疗很平常 新华康健华贵B保障范围和投保须知(案例)]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[少女私房照]]></title>
    <url>%2Fp%2F7d48.html</url>
    <content type="text"><![CDATA[拍摄主题港风阴天 香港雨天：文礼阁的雨，地上的水圈 寝室仙女的私房照One year ending Project 1: 留学少女私房照 简介 港风留学少女相（私房照），体现和屋子的这个互动，作为我们这一年生活来的记录和留念，体现港风生活。 要求 力求拍出性感不色情的感觉。并且包含生活中最典型的场景。有港风味。不做作 场景设计 菲：做牛排（厨房），寝室准头搞怪那一下（看剧少女） 圈：晾衣服，回家的时候 玥：床上起身或梳妆台前。 大王：厨房，蒜蓉，和她的泡面锅。（拍摄的时候要注意，不要显得生活太过心酸和贫瘠，要有高大上的部分，如维尼饼干） Idea 一张生活照，一张出门前的照，形成对比。内容比较有趣。 前一秒晾衣的邋遢女孩，出门的精致女孩，反差萌。 步骤（1）找摄影师作品，看别人拍摄的作品，找到与自己符合或者相近的照片。前期可以用来模仿。 （2）了解私房照以及港风照的拍摄技巧，需要与后期调色符合。 （3）解决屋子里光线不足的情况 （4）设计方案：场景、服饰、妆容 （5）约拍 作品日系生活照 最美私房照 人物和情绪 拍摄调色vsco操作攻略 vsco调色拍摄学习 vsco滤镜推荐 摄影书籍10 本好看又实用的摄影书]]></content>
      <categories>
        <category>摄影</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数学基础知识]]></title>
    <url>%2Fp%2Fccc1.html</url>
    <content type="text"><![CDATA[正则 Tikhonov正则 问题 lower semicontinuous这个性质的作用在哪里。好像通常用在证明收敛性。 PCA是什么，有什么应用 不同的空间。把图像函数f定义在某个空间上是为了measure，为什么要measure，有哪些typical space SVD去噪 欧拉方程：欧拉方程是泛函极值条件的微分表达式 tensor Norm不同的norm [semi-norm和norm] 证明解的存在性 一个函数在某个空间内有界的意思 $0&lt;c_{1} \leq u_{n} \leq c_{2}, \text{which implies that u n is bounded in} L^{1}(\Omega)$ $\left\{u_{n}\right\} \text { is bounded in } B V(\Omega)$ 泛函， is bounded below, we can choose a minimizing sequence $\left\{u_{n} : n=1,2, \cdots\right\} \in \overline{S}(\Omega)$ 序列的强收敛和弱收敛 Fatous’ lemma 各类函数空间$l_p$空间 $W^{1, 1}(\Omega)$usually defined as all functions $v \in L^{1}(\Omega)$, with weak derivatives of first order and these derivatives shall belong to $L^{1}(\Omega)$. The space $W^{1, \infty}(\Omega)$ is usually defined as all functions $v \in L^{\infty}(\Omega)$with weak derivatives of first orderand these derivatives shall belong to $L^{\infty}(\Omega) .$ BV空间The space $[\mathrm{BV}(\Omega)]^{m}$ with $|u|_{\mathrm{BV}(\Omega)} :=\int_{\Omega}|u| d x+|D u|(\Omega)$ is a Banach space. BH空间证明收敛速度 converge sublinearly $\lim _{k \rightarrow \infty} \frac{\left|x_{k+1}-L\right|}{\left|x_{k}-L\right|}=1$ converge linearly $\lim _{k \rightarrow \infty} \frac{\left|x_{k+1}-L\right|}{\left|x_{k}-L\right|}=\mu, \mu \in(0,1)$ converge superlinearly $\lim _{k \rightarrow \infty} \frac{\left|x_{k+1}-L\right|}{\left|x_{k}-L\right|}=0$ Q-linear convergence: distinguish superlinear rates of convergence. $\lim _{k \rightarrow \infty} \frac{\left|x_{k+1}-L\right|}{\left|x_{k}-L\right|^{q}}&lt;M$ Monotone operator 傅立叶变换傅立叶的平移不变性 Sup/Inf 上确界和下确界A lower bound a of S is called an infimum (or greatest lower bound, or meet) of S if lim sup/lim inf 上极限和下极限 定义 例子 lim inf Semi-continuous Upper continuous Lower continuous Minimizing sequence 基本拓扑]]></content>
      <categories>
        <category>数学</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python语法学习]]></title>
    <url>%2Fp%2Fe35e.html</url>
    <content type="text"><![CDATA[执行参数 store_true 是指带触发action时为真，不触发则为假，2L说的代码去掉default初始化，其功能也不会变化 parser.add_argument(‘-c’, action=’store_true’) #python test.py -c =&gt; c是true（触发） #python test.py =&gt; c是false（无触发） 链接：https://www.zhihu.com/question/56692630/answer/358222352 数据结构类的定义类的定义、类的继承 下划线的含义 变量 前带_的变量: 标明是一个私有变量, 只用于标明, 外部类还是可以访问到这个变量 _ _ 前带两个_ ,后带两个_ 的变量: 标明是内置变量, 大写加下划线的变量: 标明是 不会发生改变的全局变量 函数: 前带_的变量: 标明是一个私有函数, 只用于标明,_ _ 前带两个_ ,后带两个_ 的函数: 标明是特殊函数 参数的定义Positional argument v.s. keyword argument In other words, keyword arguments are only “optional” because they will be set to their default value if not specifically supplied. 多参数的输入 list 列表创建列表： 12files = []files.append(os.path.splitext(i)) 理解Python中列表，元组，字典，集合的使用 列表的读取方式： files[1] numpy数组Python Numpy 数组的初始化和基本操作 numpy设置输出精度 numpy 中的深浅复制 “等于赋值”相当于标签 1）当浅复制的值是不可变对象（数值，字符串，元组）时和“等于赋值”的情况一样，对象的id值与浅复制原来的值相同。 2）当浅复制的值是可变对象（列表和元组）时会产生一个“不是那么独立的对象”存在。有两种情况 不同数据类型 查看数据类型：type(object) 不同的数据结构 列表 可重复，类型可不同，可以遍历 extend (扩展) ：以列表增加 append (追加)：不同类型的数据 [‘a’, ‘b’, ‘c’, 1, 2, [1, 2]] 元组 可重复，类型可不同；可以遍历 只读的，不能修改 tuple1 = (1,2,’a’,4,’5’,6) numpy的数组 类型一样 b = np.array([6, 7, 8]) 字典 键和值，可以不同类型，无序存储 可变；从字典中删除元素 del dict1[‘sex’]；清除所有元素dict1.clear()；增加元素 集合 键，无序组合 可以增加删除元素set2.add(10)，set2.remove(6)，set2.discard(6)(可以删除空元素); 不同的集合支持union(联合), intersection(交), difference(差)和sysmmetric difference(对称差集)等数学运算； 不支持 索引, 分片, 或其它类序(sequence-like） 可将元祖和列表转化为集合set2 = set(list1) 可变：列表、字典、集合（存在固定集合frozenset） 不可变：元组、int、 string、 float 当传过来的是可变类型(list,dict)时，我们在函数内部修改就会影响函数外部的变量。而传入的是不可变类型时在函数内部修改改变量并不会影响函数外部的变量，因为修改的时候会先复制一份再修改。 查看不同数据类型的长度 Torch.tensor 数据类型的转化https://blog.csdn.net/A632189007/article/details/77989287 例如：字符串数组转换为数值型 1numeric_strings = np.array(['1.2','2.3','3.2141'], dtype=np.string_) 文件操作 获取文件内的文件名字 123456import ospath = "d:\\data" # 设置路径dirs = os.listdir(path) # 获取指定路径下的文件for i in dirs: # 循环读取路径下的文件并筛选输出 if os.path.splitext(i)[1] == ".csv": # 筛选csv文件 print i 组成路径名字 库Import 模块和包：包是为了解决模块重命名的问题。 模块：就是一些.py文件，可以包含函数、变量、类等符号； 包：由模块及子包组成 在模块A中 from math import sqrt，那么sqrt函数会直接导入到当前的命名空间中来，并没有创建新的命名空间 。所以就可以在A直接使用sqrt()函数了。在某模块A中import math时，会在A中创建一个命名空间，并在这个命名空间中执行math.py当中的代码，并且在A中创建了math这个名称来引用这个命名空间。 单独导入包名(import package)不会导入包中所包含的所有子模块。 包的init.py文件为空时，导入包名没法使用包内的子包及模块 包的init.py并不为空 ，该文件可以初始化，导入一些常用的包** 库的升级1pip install --upgrade requests // mac,linux,unix 在命令前加 sudo -H 字符串处理+/：https://blog.csdn.net/qq_878799579/article/details/74279842 os.path.join： 双引号和单引号的区别： 小写处理 lower() 方法转换字符串中所有大写字符为小写。 1str.lower() 矩阵处理Python 迭代器与生成器生成器和迭代器的关系: 生成器是迭代器的一种 生成器通过一个函数创建列表，但是不一次性创建完毕，因而节省内存，并且表现得却像是迭代器。 可迭代对象（可以用在 for 语句进行循环的对象） 1.数据类型（列表、元组、字符串、字典等） 123451.for i in [1, 2, 3]: print(i)2.obj = &#123;"a": 123, "b": 456&#125;for k in obj: print(k) 2.自己创建的迭代器 迭代器的创建方法: 为容器对象添加 iter() 和 next() 方法 12345678910111213141516class Container: def __init__(self, start = 0, end = 0): self.start = start self.end = end def __iter__(self): print("[LOG] I made this iterator!") return self def __next__(self): print("[LOG] Calling __next__ method!") if self.start &lt; self.end: i = self.start self.start += 1 return i else: raise StopIteration()c = Container(0, 5) 内置函数 iter() 将可迭代对象转化为迭代器 1234567ita = iter([1, 2, 3])print(type(ita))ita = iter([1, 2, 3])print(type(ita))print(type([1, 2, 3]))&lt;class 'list_iterator'&gt;&lt;class 'list'&gt; 生成器（generator） 1234567def container(start, end): while start &lt; end: yield start start += 1c = container(0, 5)Python&lt;class 'generator'&gt; 函数split通过指定分隔符对字符串进行切片，如果参数num 有指定值，则仅分隔 num 个子字符串 1234#!/usr/bin/pythonstr = "Line1-abcdef \nLine2-abc \nLine4-abcd";print str.split( );print str.split(' ', 1 ); find检测字符串中是否包含子字符串 str ，如果指定 beg（开始） 和 end（结束） 范围，则检查是否包含在指定范围内，如果包含子字符串返回开始的索引值，否则返回-1。 1str.find(str, beg=0, end=len(string)) print1print('t=&#123;&#125;,loss=&#123;:.6f&#125;'.format(t,loss)) numpy的输出1234567891011# 使用set_printoptions设置输出的精度import numpy as npx=np.random.random(10)np.set_printoptions(precision=3)print(x)# 抑制使用对小数的科学记数法y=np.array([1.5e-10,1.5,1500])print(y)# [ 1.500e-10 1.500e+00 1.500e+03]np.set_printoptions(suppress=True)print(y) 合并两个矩阵1234#hstack()在行上合并 这个实验结果不对 np.hstack((a,b)) #vstack()在列上合并 np.vstack((a,b)) squeeze语法：numpy.squeeze(a,axis = None) 1）a表示输入的数组；2）axis用于指定需要删除的维度，但是指定的维度必须为单维度，否则将会报错；3）axis的取值可为None 或 int 或 tuple of ints, 可选。若axis为空，则删除所有单维度的条目； 从数组的形状中删除单维度条目，即把shape中为1的维度去掉 作用：从数组的形状中删除单维度条目，即把shape中为1的维度去掉 例子： 123456789101112131415161718e = np.arange(10).reshape(1,10,1)e: array([[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]]])np.squeeze(e)e: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])#正常显示图示案例#通过np.squeeze()函数转换后，要显示的数组变成了秩为1的数组，即（5，）plt.plot(np.squeeze(squares)) plt.show() getattr123456789101112131415161718# 获得一个标量&gt;&gt;&gt;class A(object):... bar = 1... &gt;&gt;&gt; a = A()&gt;&gt;&gt; getattr(a, &apos;bar&apos;) # 获取属性 bar 值1# 获得一个函数&gt;&gt;&gt; class A(object): ... def set(self, a, b):... x = a ... a = b ... b = x ... print a, b &gt;&gt;&gt; a = A() &gt;&gt;&gt; c = getattr(a, &apos;set&apos;)&gt;&gt;&gt; c(a=&apos;1&apos;, b=&apos;2&apos;)2 1 逻辑处理 条件语句 1 x = int(input(“Please enter an integer: “)) Please enter an integer: 42if x &lt; 0: … x = 0 … print(‘Negative changed to zero’) … elif x == 0: … print(‘Zero’) … elif x == 1: … print(‘Single’) … else: … print(‘More’) … 12&gt;&gt;&gt; &gt;&gt;&gt; python的注释格式python的注释格式 Python 图像 【pytorch】图像基本操作 不同的读取方式: cv2 ski.. PIL库的使用 import matplotlib Image读出来的是PIL的类型，而skimage.i的对比 PIL拆分、合并、合成视频 用PIL显示图像 对图像加噪声、滤波复原 https://www.cnblogs.com/lynsyklate/p/8047510.html Debug [x] 文件阅读时，出现无法创建文件的问题。（该文件被另一进程阅读中） why can the same file not opened by several processes? @wqn628 Because that requires the hdf5-library to be built with parallel extensions. You can open an HDF5-file for read-only in multiple processes, but you cannot open it for read/write in more than one process unless the library has been built with parallel enabled.The reason for this is because HDF5-files write to the file the moment you close them, not the moment you tell them to write. Therefore, in serial, corruption would be very likely. The best way to deal with this is by simply opening the HDF5-file for read-only in both Python scripts. That should work perfectly fine. [x] NoneType之所以出现，该参数没有被创建或者赋值，只被取了名字。 要理解这个，首先要理解Python对象，python对象具有三个特性：身份、类型、值。 这三个特性在对象创建时被赋值。只有值可以改变，其他只读。类型本身也是对象。 Null与None是Python的特殊类型，Null对象或者是None Type，它只有一个值None. 它不支持任何运算也没有任何内建方法. None和任何其他的数据类型比较永远返回False。 None有自己的数据类型NoneType。你可以将None复制给任何变量，但是你不能创建其他NoneType对象。 一句话总结：Null对象是python对象，又叫做NoneType，None是这个对象的值。 看过了NoneType的解释，之所以出现None就很好理解了。 要正确定位bug出现的位置，之前那个logwrite没有定位准确，导致找不到原因。 要关注错误信息。 矩阵操作 Numpy求最大值、 最小值) args和*kwargs如何在Python3中使用args和*kwargs]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[去噪传统方法论文综述]]></title>
    <url>%2Fp%2F8245.html</url>
    <content type="text"><![CDATA[期刊会议 计算机视觉期刊 A类 TPAMI: IEEE Trans on Pattern Analysis and Machine Intelligence IJCV: International Journal of Computer Vision TIP: IEEE Transactions on Image Processing B类 CVIU: Computer Vision and Image Understanding Pattern Recognition C类 IET-CVI: IET Computer Vision IVC: Image and Vision Computing IJPRAI: International Journal of Pattern Recognition and Artificial Intelligence Machine Vision and Applications PRL: Pattern Recognition Letters 计算机视觉会议 A类 ICCV: International Conference on Computer Vision CVPR: International Conference on Computer Vision and Pattern Recognition AAAI: AAAI Conference on Artificial Intelligence ICML: International Conference on Machine Learning NIPS: Annual Conference on Neural Information Processing Systems ACM MM: ACM International Conference on Multimedia B类 ECCV: European Conference on Computer Vision C类 ACCV: Asian Conference on Computer Vision ICPR: International Conference on Pattern Recognition BMVC: British Machine Vision Conference 应用数学期刊 JMIV: Journal of Mathematical Imaging and Vision 应用数学会议 检索工具 期刊影响因子查询网站：中文、 英文网站 出版商、数据库、检索工具 IEEE和Springer是出版商，并且拥有自己的数据库和检索工具。 如：IEEE有IEEExplorer检索。 其他数据库自己（比如EI, EBSCOhost等等）可以抓取出版商数据库里面的文章。比如EngineeringVillage (EI)在IEEExplorer抓取文章的内容；这时候作者的文章就能在EI里面查到了。 Google/Google Scholar/Scirus/ScienceDirect/IEEExplorer/ISI这些都是检索工具 SCI(Science Citation Index)是影响因子，也是ISI（Institute Scientific Information）做的数据库。 SCI不是出版商，只是数据库，不是具体某篇文章内容版权的拥有者；所以在SCI里面，能看到只是题目+摘要+参考文献。SCI的内容不是原始文献全文，卖点是每年推出JCR，里面给出影响因子。 SIAM简介：国际工业与应用数学协会(SIAM), 旗下出版有应用数学相关的许多国际著名期刊杂志 文献链接2019 CVPR 汇总 变分算法 total variation TV-based image restoration: Rudin Osher and Fatermi in 1992. 优点：preserves edges well, but has sometimes undesirable staircase effect, namely the trans- formation of smooth regions into piecewise constant regions (stairs), which implied that the finer details in the original im- age may not be recovered satisfactorily. 缺点：staircasing 什么是staircasing。如何发生。在哪些情况下更容易发生。 除了non-local mean和BM3D，都是oversmoothed。 改进：几大类方法 高阶全变分模 型 [8] 、广义全变分模型 [9] 和自适应全变分模型 [5] 等 [7] L.I. Rudin, S.Osher, and E. Fatemi. Nonlinear total variation based noise removal algorithms. Physica D(Nonlinear phenomena), 1992, 60(1 /2 /3 /4) : 259-268. [8] Y. You and M. Kaveh. Fourth-Order partial differential equations for noise removal. IEEE trans on image processing, 2000, 9(10) : 1723-1730. [9] B. Kristian, K. Karl, and P. Thomas. Total generalized variation. SIAM Journal on imaging sciences, 2010, 3(3):492-526. [5] P. Blomgren, T. Chan, P. Mulet, and C. Wong. Total variation image restoration: numerical methods and extensions. In IEEE international conferance on image processing, pages III, 384-387, 1997. 基于小波框架的变分模型 [14-19] 14、Chan, Raymond H., et al. “Wavelet algorithms for high-resolution image reconstruction.” SIAM Journal on Scientific Computing 24.4 (2003): 1408-1432 17、Chan, Raymond, Lixin Shen, and Zuowei Shen. “A framelet-based approach for image inpainting.” Res. Rep 4 (2005): 325. 16、Dong, Bin, and Zuowei Shen. “MRA based wavelet frames and applications.” IAS Lecture Notes Series, Summer Program on “The Mathematics of Image Processing”, Park City Mathematics Institute 19 (2010). 15、Cai, Jian-Feng, Stanley Osher, and Zuowei Shen. “Split Bregman methods and frame based image restoration.” Multiscale modeling &amp; simulation 8.2 (2009): 337-369. 19、Cai, Jian-Feng, et al. “Image restoration: total variation, wavelet frames, and beyond.” Journal of the American Mathematical Society 25.4 (2012): 1033-1089. 20、Cai, Jian-Feng, et al. “Data-driven tight frame construction and image denoising.” Applied and Computational Harmonic Analysis 37.1 (2014): 89-105. 最近建立了小波框架和 变分模型之间的联系。 这种联系给出了基于小波框 架的变分模型优于其他某些变分模型的理论依据， 即基于小波框架的变分模型可以根据潜在的解的奇 点的顺序，在给定图像的不同区域中自适应地选择微 分算子。又基于图像数据结构特征， 提出了一种数据驱动紧 框架， 该框架比以往的模型更能精确地重构图像。 去噪 不同的滤波器用于不同的噪声，某一个降噪滤波器很难符所有的噪声。 首先，说一下噪声的类型，噪声的分类和该噪声的分布符合什么模型有关，常见的噪声有高斯白噪声、椒盐噪声、泊松分布噪声、指数分布噪声等。 其次，采用的滤波器有空域滤波器，比如均值滤波器、中值滤波器、低通滤波器、高斯滤波等；频域滤波器，比如小波变换、傅里叶变换、余弦变换等；形态学滤波器，主要是通过膨胀和腐蚀等形态学操作进行去噪。 第三，对应场合。一般平时见的比较多是是高斯白噪声，像用均值滤波、中值滤波、高斯滤波可以去噪。还有在低照度下，比如晚上拍照时的图像，一般属于泊松分布的噪声，可以采用一些3d去噪算法，比如效果不错的BM3D算法。像椒盐噪声，一般用中值滤波基本可以去噪。 文献回顾以及代码github去噪技术的方法和代码实现总结 滤波法：空间域和变换域 (空间域) Gaussian filter (转化域) Wavelet-based(详见Tight-frame): tight frame 小波变化文献回顾 马尔可夫场 Markov denoising 理解 全变分模型（变分法的一种） ROF模型, 1992 L. Rudin, S. Osher, E. Fatemi, Nonlinear Total Variation based noise removal algorithm, Physica D 60 259-268, 1992. ++paper \min _{u} \int_{\Omega}\left(\alpha|\nabla u|+\frac{1}{2}(u-z)^{2}\right) 与ROF相关联的偏微分方程以及推导 目标方程： \begin{array}{l}{J[u(x, y)]=\iint_{\Omega}|\nabla u(x, y)| \mathrm{d} x \mathrm{d} y+\quad \frac{\lambda}{2} \iint_{\Omega}\left(u(x, y)-u^{0}(x, y)\right)^{2} \mathrm{d} x \mathrm{d} y}\end{array}该泛函是 J[u(x, y)]=\iint_{\Omega} F\left(x, y, u, \frac{\partial u}{\partial x}, \frac{\partial u}{\partial y}\right) \mathrm{d} x \mathrm{d} y型的泛函，其中(式1) \begin{aligned} F=&|\nabla u(x, y)|+\frac{\lambda}{2}\left(u(x, y)-u^{0}(x, y)\right)^{2}= \\&\sqrt{\left(\frac{\partial u(x, y)}{\partial x}\right)^{2}+\left(\frac{\partial u(x, y)}{\partial y}\right)^{2}}+\frac{\lambda}{2}\left(u(x, y)-u^{0}(x, y)\right)^{2} \end{aligned} $$ {222} 该类函数求极值的必要条件，即欧拉-拉格朗日方程(PDE):F_{H}-\frac{\partial}{\partial x}\left\{F_{p}\right\}-\frac{\partial}{\partial y}\left\{F_{q}\right\}=0 其中, $p=\frac{\partial u(x, y)}{\partial x}, q=\frac{\partial u(x, y)}{\partial y}$. 对于式1，有$F_{H}=\lambda\left(u-u^{0}\right), F_{p}=\frac{\frac{\partial u}{\partial x}}{|\nabla u|}, F_{q}=\frac{\frac{\partial u}{\partial y}}{|\nabla u|}$, 代入后有：\begin{array}{l}{\lambda\left(u-u^{0}\right)-\left\{\frac{\partial}{\partial x}\left\{\left(\frac{\frac{\partial u}{\partial x}}{|\nabla u|}\right)\right\}+\frac{\partial}{\partial y}\left\{\frac{\frac{\partial u}{\partial y}}{|\nabla u|}\right\}\right\}=0} \\ {\Rightarrow \lambda\left(u-u^{0}\right)-\left(\frac{\partial}{\partial x}, \frac{\partial}{\partial y}\right) \cdot\left(\frac{\frac{\partial u}{\partial x}}{|\nabla u|}, \frac{\frac{\partial u}{\partial y}}{|\nabla u|}\right)=0} \\ {\Rightarrow \lambda\left(u-u^{0}\right)-\left(\frac{\partial}{\partial x}, \frac{\partial}{\partial y}\right) \cdot\left(\frac{1}{|\nabla u|}\left(\frac{\partial u}{\partial x}, \frac{\partial u}{\partial y}\right)\right)=0} \\ {\Rightarrow \lambda\left(u-u^{0}\right)-\nabla \cdot\left(\frac{\nabla u}{|\nabla u|}\right)=0}\end{array} 其中，$\nabla=\left(\frac{\partial}{\partial x}, \frac{\partial}{\partial y}\right)$为梯度算子。 TV复原模型的欧拉一拉格朗日方程为 ：-\nabla \cdot\left(\frac{\nabla u}{|\nabla u|}\right)+\lambda\left(u-u^{0}\right)=0$$ using a gradient projection method. 缺点：阶梯块效应 表现形式： 产生原因： 改进方法 可以分为三类：(1) 对l_1进行改进 (2) 高阶变分 (3) 高阶全变分模型[8]、广义全变分模型[9]和自适应全变分模型[5] (在谋篇中文期刊看到过) 二阶(高阶)方法改进staircasing 局限性：通常需要更复杂的边界条件以及结果很可能会过光滑 (二阶) G. Geman and G. Reynolds, 1992, IEEE Trans. Pattern Anal. Mach. Intel. 《Constrained restoration and the recovery of discontinuities》 (二阶) Chambolle and Lions, 1997, Numer. Math. 《Image recovery via total variation minimization and related problems》: Chambolle and Lions do this by minimizing the inf-convolution of the TV norm and a second order functional. \begin{array}{l}{\min _{u_{1}, u_{2}} \int_{\Omega}\left|\nabla u_{1}\right|+\alpha\left|d^{2} u_{2}\right|+\lambda\left|u_{1}+u_{2}-u_{0}\right|^{2}} \\ {=\min _{u, v} \int_{\Omega}|\nabla u-\nabla v|+\alpha|\nabla(\nabla v)|+\lambda\left|u-u_{0}\right|^{2}}\end{array} \min _{u} \int_{\Omega}\left(\alpha \left|d^{2} u\right|+|\nabla u|+\frac{1}{2}(u-z)^{2}\right) (母鸡) P. Blomgren, T. F. Chan, and P. Mulet, 1997 《Extensions to total variation denoising》: The approach is performed by redefining the Total Variation functional R(u) in view of the properties of TV-norm and H1-seminorm. However, it is not completely clear how to choose a function Φ, which makes the regularizing functional R(u) being convex. (四阶) Chan T, Marquina A, Mulet P., 2000, SIAM J Sci Comput, 《High-order total variation-based image restoration》: ”In this paper we present an improved model, constructed by adding a nonlinear fourth order diffusive term to the Euler–Lagrange equations of the variational TV model. “ \int_{\Omega}\left(\alpha|\nabla u|_{\beta}+\mu \Phi(|\nabla u|)(\mathcal{L}(u))^{2}+\frac{1}{2}(u-z)^{2}\right)where, $\mathcal{L}(u)$ is an elliptic operator and $\Phi(|\nabla u|)$ is the adaptive function. This model retain the good properties of the TV functional and penalize “wrong” edges created in regions which “should” be smooth. The adaptive functional, in which the action of the second order term is lessened where the gradient is large(avoid oversmooth). (二阶) O. M. Lysaker and X.-C. Tai, Int. J. Comp. Vis., 2006 《Iterative image restoration combining total variation minimization and a second-order functional》:Instead of combing TV norm and second order derivatives within one regularization functional, Lysaker and Tai [5] use two regularization functionals. 两个式子，单独的两个regularization (四阶）Li F, Shen C, Fan J, 2007, J Vis Commun Image R 《Image restoration combining a total variational filter and a fourth-order filter》 (母鸡，higher-order)Liu G, Huang T, Liu J, 2014, Comput Math Appl 《High-order TVL1-based images restoration and spatially adapted regularization parameter selection》 TGV, Bredies, K., Kunisch, K., &amp; Pock, T. (2010), SIAM Journal on Imaging Sciences \mathrm{TGV}_{\alpha}^{k}=\left(\sum_{l=0}^{k-1} I_{K_{\alpha_{l}}^{l}}\right)^{*} \operatorname{TGV}_{\alpha}^{k}(u)=\underset{w_{0}+\ldots+u_{k-1}=u \atop l=0, \ldots, k-1}{\inf } \sum_{l=0}^{k-1} \alpha_{l}\left\|\nabla^{k-l} u_{l}+w_{l}\right\|_{1}《Total generalized variation.》 code link here, paper link limitation: converge slowly 一个加速版本：K. Shirai, M. Okuda, (2014), “FFT based solution for multivariable l2 equations using KKT system via FFT and efficient pixel-wise inverse calculation,” IEEE ICASSP, paper link A. Beck, and M. Teboulle, A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems, SIAM J. Imaging Sci., Vol. 2, 183–202, 2009. inﬁmal convolution Euler’s elastica K Papafitsoros, (2014), JMIV 《A combined first and second order variational approach for image reconstruction. 》 J(u)=\frac{1}{2}\left\|u_{0}-T u\right\|_{2}^{2}+\alpha\|\nabla u\|_{1}+\beta\left\|\nabla^{2} u\right\|_{1} 从MAP的角度出发 C. Louchet and L. Moisan， 2008 《Total variation denoising using posterior expectation》: Louchet and Moisan proposed an alternative to the minimization of the total variation by considering the TV-LSE filter. C. Louchet and L. Moisan， 2014 《Total Variation Denoising using Iterated Conditional Expectation》 分析了staircasing产生的原因 M. Nikolova, 2000, SIAM J. Appl. Math 《Local strong homogeneity of a regularized estimator》：Nikolova proves that the staircasing effect is related to the non-differentiability of the total variation term 增加细节的改进：non-local+TV的方法. NLTV, Gilboa and Osher, 2004, Multiscale Model. Simul. 《Nonlocal operators with applications to image processing》 J_{\mathrm{NLTV}}(u)+\lambda\|u-I\|^{2} \int_{\mathcal{P}} \sqrt{\int_{\mathcal{B}}(u(p)-u(p+q))^{2} v(p, q) \mathrm{d} q \mathrm{d} p} NLTVG, Peyré, G., Bougleux, S., Cohen, L.D, 2011, Inverse Probl. Imaging 《Non-local regularization of inverse problems.》 RNLTV, Z Li, F Malgouyres, T Zeng，2017, JMIV 《Regularized Non-local Total Variation and Application in Image Restoration》 基于图像的自相似性: 两种图像降噪方法论文解读以及代码实现 Non-local means, 2005, code link here Buades, A., Coll, B., &amp; Morel, J. M. (2005, June). A non-local algorithm for image denoising. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05) (Vol. 2, pp. 60-65). IEEE.（BM3D） Exploit the inter-patch correlations BM3D, 2007, code link Dabov, K., Foi, A., Katkovnik, V., &amp; Egiazarian, K. (2007). Image denoising by sparse 3-D transform-domain collaborative filtering. Image Processing, IEEE Transactions on 16 (8), pp. 2080-2095. inter-patch correlations；wavelet shrinkage LSSC(Learned Simultaneous Sparse Coding), 2009 Mairal, J., Bach, F. R., Ponce, J., Sapiro, G., &amp; Zisserman, A. (2009, September). Non-local sparse models for image restoration. In ICCV (Vol. 29, pp. 54-62). 这个算法是NL-means，LSC(Learned Sparse Coding), Block matching 3D的相结合 基于稀疏表达（在变换域） There exists a transform T such that applying T to patches will admit a sparse representation. Wavelet transform 稀疏表达 Wavelet thresholding 属于一种滤波法 基于马尔可夫场 基于深度学习 论文总结 高斯噪声均值为0，平方差为$\sigma^2$的高斯白噪声。 1234567891011121314# 调用imnoiseg = imnoise(I, (noise_level/255)^2)g = I + sqrt((noise_level/255)^2)*randn(M,N);其中的I是经过im2double(I) % I = I/255归一化的。case 'gaussian' % Gaussian white noise b = a + sqrt(p4)*randn(sizeA) + p3; # 直接g = I + sigma * randn(M,N);其中I是归一化后的，sigma = noise_level/255;# 如果I是[0,255]之间的原图。g = I + alpha * randn(size(S)); 那么，alpha = sigma*255; 斑点噪声（乘性噪声） 噪声模型诶 f=u n p(f | u, L)=\frac{2 L^{L}}{\Gamma(L) u^{2 L}} f^{2 L-1} e^{-\frac{L f^{2}}{u^{2}}} TV去噪模型 （AA模型）G. Aubert and J. Aujol. (2008). A variational approach to remove multiplicative noise. SIAM J. Appl. Math., 68:925–946, . \inf _{u \in S(\Omega)} \int_{\Omega}\left(\log (A u)+\frac{f}{A u}\right) d x+\lambda \int_{\Omega}|D u| Dong, Y., &amp; Zeng, T. (2013). A convex variational model for restoring blurred images with multiplicative noise. SIAM Journal on Imaging Sciences, 6(3), 1598-1625. \inf _{u \in \overline{S}(\Omega)} E(u) :=\int_{\Omega}\left(\log u+\frac{f}{u}\right) d x+\alpha \int_{\Omega}\left(\sqrt{\frac{u}{f}}-1\right)^{2} d x+\lambda \int_{\Omega}|D u| Fang, F., Fang, Y., &amp; Zeng, T. (2018). On the Convex Model of Speckle Reduction: IVLOPDE, Bergen, Norway, August 29 – September 2, 2016. https://doi.org/10.1007/978-3-319-91274-5_6 泊松噪声 泊松噪声既不是加性噪声，也不是乘性噪声，而是一种信号依赖噪声。 加噪模型：f= 10*poissrnd(u/10)，noiselevel=10 \operatorname{Pr}(\boldsymbol{g} | \boldsymbol{f})=\prod_{i, j} \frac{\left[\boldsymbol{f}_{i, j}\right]^{g_{i, j}} \exp \left(-\boldsymbol{f}_{i, j}\right)}{\left(\boldsymbol{g}_{i, j}\right) !} TV去噪 Ｃsisｚár [１] 最早提出了 Kullbaｃk－Leibler （ KL )－divergenｃe 保真项用于去除 灰色图像中的泊松噪声Ｃsisｚár I． Wｈy least squares and ｍaxiｍuｍ entrｏpy Ａn Ａxiｏｍatiｃ Ａpprｏaｃｈ tｏ Inferenｃe fｏr linear Inverse Ｐrｏbleｍs[J]． Ａnnals ｏf Statistiｃs ， １９９１ ， １９ （ ４ ） : ２０3２－２０６６ Luisier 等 [２] 在小波变换中构 造了一个 SURE 估计量用于泊松噪声的去除。 Gｏng 等 [3] 提出了一个 l １ +l ２ 保真项去除泊松噪声及一切未 知噪声。 Zｈang 等学者 [４－5] 使用重新赋权的 l ２ 方 法逼近 KL－divergenｃe 保真项 Wen, Y., Chan, R. H., &amp; Zeng, T. (2016). Primal-dual algorithms for total variation based image restoration under Poisson noise. Science China Mathematics, 59(1), 141-160. 描述了ADMM算法和Primal Dual的算法解决以下问题 Using the Bayesian rule, the Poisson image restoration problem can be represented as a minimization problem \min _{f \in S} \Psi(\boldsymbol{f}) \equiv D_{K L}(H \boldsymbol{f}+\boldsymbol{b}, \boldsymbol{g})+\lambda \operatorname{TV}(\boldsymbol{f}) D_{K L}(\boldsymbol{z}, \boldsymbol{g})=\left\langle\boldsymbol{g}, \ln \frac{\boldsymbol{g}}{\boldsymbol{z}}\right\rangle+\langle\mathbf{1}, \boldsymbol{z}-\boldsymbol{g}\rangle code link: BM3D for Poisson 椒盐噪声 噪声模型 讲一部分像素变成0或者255 TV去噪 （Nikolova in 2004） \underset{u \in \mathbb{R}^{\Omega}}{\operatorname{argmin}}\|u-v\|_{1}+\lambda \sum_{i} \phi(\nabla u(i)) $\underset{u \in \mathbb{R}^{\Omega}}{\operatorname{argmin}}|u-v|_{0}+\lambda \sum_{i} \phi(\nabla u(i))$（因为nonconvex，先研究了第一个替代问题，之后出现对该问题求解） 2015 CVPR: A New Method for Image Restoration in the Presence ofImpulse Noise with code:C [2019 JMIV: Mixing Non-Local and TV-Lp methods] Chan提出了先试用noise detector的方法 NL-means approach [2016 JSC] Removing mixture of gaussian and impulse noise by patch-based weighted means 2019 JMIV: Mixing Non-Local and TV-Lp methods 去模糊 文献 An image sharpening Operator Combined with Framelet for Image Deblurring 提出的模型 \min _{u} \mathcal{J}(u) \equiv \frac{\lambda}{2}\|A u-f\|_{2}^{2}+\|W u\|_{1}+\mu\|u-g\|_{1} 采用变分法，正则项选择选择了Tight frame：|WtW|=1 将一个sharpen算子和传统tight frame的方法结合一起：用一个方法计算出有助于sharp的图像g, 并用L1-nirm去靠近。（和去模糊的构造正解之间的关系？待解决） 超分辨 文献 [2019 CVPR] Deep Plug-and-Play Super-Resolution for Arbitrary Blur Kernels 提出了一个新的SR degradation模型：ADMM和DNN的结合 待读文章 Fooling automated surveillance cameras: adversarial patches to attack person detection（对抗性补丁） 微信推送：包括论文地址和代码地址 对抗性补丁”(adversarial patch)，正是这块补丁 “欺骗” 了 AI 系统 如何写一篇优秀的论文1.紧跟细分领域研究前沿，确定研究角度 要做到这一点，最简单的方法是找一篇权威的综述类文章，可以是中文的，最好是英文的。根据综述中提到的作者、引用的文献，不断抽丝拨茧，去搜索下载原文进行研读，并且形成你自己对这个领域的文献综述。 2.熟练掌握一套实证研究方法和工具 3.不断模仿优秀的论文 如果你想在A类中文核心期刊上发表论文，那么你就必须要在《管理世界》《管理评论》等A类期刊上下载优秀论文，模仿他们的遣词用句，假设推导、假设检验、研究方法、结果探讨等等。 4.永远记住：一切初稿都是狗屎，修改一篇文章远比写一篇新的文章付出的心血要多。 一篇文章写完后，千万不要急需发表，修改工作十分重要。首先，打印出来，自己逐句逐字认认真真通读一遍，把不通顺的语句，错别字，标点符号等全部改正，保证不出现低级错误。 5.保持足够的耐心 论文表达摘录论文句式摘要 英文科技写作句型和词汇表达总结(2010-) 插入语 有必要提一句 It’s necessary to mention that；For completion, we remark that… 值得注意的是 It should be noted that; It is noteworthy that;It’s worth nothing that; It must be noted that;Significantly 说贡献 To the best of our knowledge, the deﬁnition of discrete TV-seminorm (3) as well as the role of the Raviart–Thomas ﬁnite element space to establish dual representation (4) are novel contributions of the present work. 描述一个方法 改造性强 The primal-dual algorithm proposed in this paper can beeasily adapted to different problems, is easy to implement and can be effectively accelerated on parallel hardware such as graphics processing units (GPUs). 评论一个方法的影响力大/效果好 achieve/provide/reach remarkable performance on the .. problem is an efficient network that provide an end-to-end mapping/estimation between the .. 很多方法被提出 To avoid the heuristic edge selection step, numerous algorithms based on natural image prior have been proposed, including normalized sparsity [16], L0 gradients [38] and dark channel prior [27]. Recent years have witnessed significant advances in sin-gle image deblurring. We focus our discussion on recent optimization-based and learning-based methods. Thus, it is of great interest to develop a general image prior which is able to deal with different scenarios with the MAP framework 图片名称 Figure 7: Some examples of RCF. From top to bottom: BSDS500 [2], NYUD [49], Multicue-Boundary [41], and Multicue-Edge [41]. From left to right: origin image, ground truth, RCF edge map, origin image, ground truth, and RCF edge map. 图像的合成 The noisy inputs are synthesized by adding i.i.d. Gaussian white noise with diﬀerent standard deviation σ to the original images. 方法的比较 两个方法差不多的情况 There are images on which the K-SVD method performed better and there are some on which our approaches performed better. Overall, the performances of our proposed method and the K-SVD method are comparable in terms of PSNR value, and so is the visual quality. We compared the computational eﬃciency of Algorithm 1 against that of the K-SVD method in terms of the running time on the same hardware conﬁguration.]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
      <tags>
        <tag>denoising</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一个博客搭建]]></title>
    <url>%2Fp%2F9748.html</url>
    <content type="text"><![CDATA[Github搭建博客 解决数学公式的显示问题(文件助手) github搭建结构的基本框架(文件助手) git的使用，廖雪峰 图片显示问题 新浪图片外链在博客中无法显示的原因 新浪图床直接访问没有权限：Access Denied You don’t have permission to access Mac下iPic的使用方法 不同的图床不同的图床 七牛云的API iPic中添加七牛 阿里云：https://oss.console.aliyun.com/overview 图床跑路怎么办 https://github.com/wisp-x/lsky-pro 解决替换图床的脚本 公式的显示问题博客list多端访问iphone/ipad]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[latex使用笔记]]></title>
    <url>%2Fp%2F5904.html</url>
    <content type="text"><![CDATA[latex使用笔记 参考资料学术写作利器——LaTeX入门笔记整理（不定期更新，附加使用心得) 在线工具 在线画图 在线画表格，转化成latex代码 WinEdt快捷键查找，以及查找下一个： ctrl+I 添加注释的快捷键为: Left+Shift+Ctrl+Alt 删除注释的快捷键为: Right+Shift+Ctrl+Alt Atom快捷键和配置本文操作示例基于 Mac OS。 mac air texshop块注释：cmd+shift+{/} 特殊符号$8^{\circ}$: 8^{\circ} (可以通过自定义来简化，\def\degree{${}^{\circ}$} ，90\degree ) $8^{\circ}C$: 8^{\circ}C 自定义命令符 自定义颜色 RGB颜色查询 123\usepackage&#123;xcolor&#125;\definecolor&#123;myorange&#125;&#123;rgb&#125;&#123;1, 0.44, 0.11&#125;\newcommand&#123;\grammar&#125;[1]&#123;\textcolor&#123;myorange&#125;&#123;#1&#125;&#125; `一些数学符号 12345\newcommand &#123;\Argmin&#125;[1] &#123;\underset&#123;#1&#125;&#123;\argmin&#125;&#125;\DeclareMathOperator &#123;\prox&#125; &#123;prox&#125;\newcommand &#123;\Prox&#125;[3] &#123; &#123;\prox&#125;_&#123;#1&#125;^&#123;#2&#125;\left( #3 \right) &#125;\newcommand &#123;\TV&#125;[2] &#123; TV\left( #1, #2\right) &#125;\newcommand &#123;\PS&#125;[2] &#123;\left\langle #1, #2\right\rangle&#125; 画图 在线画图连接： 神经网络 图片 调整图片大小至页面宽度 \includegraphics[width=\textwidth]{images/EdgeNet2.png} 单栏图片插入 123456789% For one-column wide figures use\begin&#123;figure&#125;% Use the relevant command to insert your figure file.% For example, with the graphicx package use \includegraphics&#123;example.eps&#125;% figure caption is below the figure\caption&#123;Please write your figure caption here&#125;\label&#123;fig:1&#125; % Give a unique label\end&#123;figure&#125; 双栏图片插入 123456789% For two-column wide figures use\begin&#123;figure*&#125;% Use the relevant command to insert your figure file.% For example, with the graphicx package use \includegraphics[width=0.75\textwidth]&#123;example.eps&#125;% figure caption is below the figure\caption&#123;Please write your figure caption here&#125;\label&#123;fig:2&#125; % Give a unique label\end&#123;figure*&#125; 多张图片并列插入 使用subfigure，如果想换行，在图片后面加上回车键 1234567891011121314151617181920212223242526272829303132333435\begin&#123;figure&#125;[htbp]\centering\subfigure[pic1.]&#123;\begin&#123;minipage&#125;[t]&#123;0.25\linewidth&#125;\centering\includegraphics[width=1in]&#123;images/MSRB1.png&#125;%\caption&#123;fig1&#125;\end&#123;minipage&#125;%&#125;%\subfigure[pic2.]&#123;\begin&#123;minipage&#125;[t]&#123;0.25\linewidth&#125;\centering\includegraphics[width=1in]&#123;images/MSRB1.png&#125;%\caption&#123;fig2&#125;\end&#123;minipage&#125;%&#125;%[回车]\subfigure[pic3.]&#123;\begin&#123;minipage&#125;[t]&#123;0.25\linewidth&#125;\centering\includegraphics[width=1in]&#123;images/MSRB1.png&#125;%\caption&#123;fig2&#125;\end&#123;minipage&#125;&#125;%\subfigure[pic4.]&#123;\begin&#123;minipage&#125;[t]&#123;0.25\linewidth&#125;\centering\includegraphics[width=1in]&#123;images/MSRB1.png&#125;%\caption&#123;fig2&#125;\end&#123;minipage&#125;&#125;%[回车]\centering\caption&#123; pics&#125;\end&#123;figure&#125; 一列多张图 去掉图片的序列号 利用minipage下的\centerline以及subfigure*。例如： 123456789101112131415161718\documentclass[a4paper,UTF8]&#123;article&#125;\usepackage&#123;ctex&#125;\usepackage&#123;caption&#125;\usepackage&#123;graphicx&#125;\begin&#123;document&#125;\begin&#123;figure&#125;[ht]\begin&#123;minipage&#125;&#123;0.48\linewidth&#125;\centerline&#123;\includegraphics[width=1\textwidth]&#123;a1.jpg&#125;&#125;\centerline&#123;伤心图&#125;\end&#123;minipage&#125;\qquad\begin&#123;minipage&#125;&#123;0.48\linewidth&#125;\centerline&#123;\includegraphics[width=1\textwidth]&#123;a2.jpg&#125;&#125;\centerline&#123;开心图&#125;\end&#123;minipage&#125;\caption*&#123;都是表情图&#125;\end&#123;figure&#125;\end&#123;document&#125; 设置caption中的字体大小 控制图片插到的位置 其中[htbp]就是浮动格式 “h 当前位置。将图形放置在正文文本中给出该图形环境的地方。如果本页所剩的页面不够，这一参数将不起作用。 t 顶部。将图形放置在页面的顶部。 b 底部。将图形放置在页面的底部。 p 浮动页。将图形放置在一只允许有浮动对象的页面上。” 表格 在线画表格，转化成latex代码 参考博客 例子(摘自论文FFDNet) https://ws1.sinaimg.cn/large/006tNc79gy1g37p8ux09hj313c0rgk7f.jpg 三线表 latex和matlab表格的连接 matlab的输出模式规范成[&amp;&amp;\\\]的模式 latex引用包，把csv读取成表格模式 表格溢出 123456789\begin&#123;table*&#125;[!t]\centering\caption&#123;***&#125;\label&#123;***&#125;\resizebox&#123;\textwidth&#125;&#123;!&#125;&#123;\begin&#123;tabular&#125;&#123;***&#125;***\end&#123;tabular&#125;&#125;\end&#123;table*&#125; 双栏表格例子 12345678910111213\begin&#123;table&#125;[htbp]\caption&#123;The different data-fidelity terms used in the experiments.&#125;\begin&#123;center&#125;\begin&#123;tabular&#125;&#123;|l|*&#123;1&#125;&#123;c|&#125;&#125;\hline\textbf&#123;&#125;&amp;&#123;$D(u)$&#125;\\\hlineinpainting and zooming&amp;$\lambda \|Hu-I\|^2$ \\\hlinedenoising (synthetic image)&amp;$\one&#123;\|u-I\|^2\leq \tau&#125;(u)$\\\hlinedenoising (natural image)&amp;$\lambda \|u-I\|^2$ \\\hline\end&#123;tabular&#125;\end&#123;center&#125;\label&#123;tab:data_term&#125;\end&#123;table&#125; 公式 公式溢出的问题 \! 缩小间距 换行 多行公式对齐，并显示一个序号 12345678910\documentclass[review]&#123;elsarticle&#125;\usepackage&#123;amsmath&#125;\begin&#123;document&#125;\begin&#123;equation&#125; \label&#123;eqn2&#125; \begin&#123;split&#125; n&amp;=\left[\frac&#123;b-a&#125;&#123;0.01&#125;\right]+1, \\ S&amp;=\frac&#123;1&#125;&#123;n&#125;\sum\limits_&#123;j=1&#125;^&#123;n&#125;(\lambda_&#123;0j&#125;-\lambda_&#123;j&#125;). \end&#123;split&#125;\end&#123;equation&#125;\end&#123;document&#125; 公式换行 multiline 1234\begin&#123;multline&#125;TV(v+h,u)=\sum_&#123;p\in\PP_1&#125; \Psi_\mu\left( \sqrt&#123;(\Au v)_p + (\Au h)_p&#125; \right) \\+\sum_&#123;p\in\PP_2&#125; \frac&#123; (\Au v)_p + (\Au h)_p &#125;&#123; 2\mu &#125;.\end&#123;multline&#125; aligned结合\qquad 123456789101112\begin&#123;equation&#125; \begin&#123;aligned&#125;&amp;\qquad\sum_&#123;p\in\PP_1&#125; \Psi_\mu\left( \sqrt&#123;(\Au v)_p + (\Au h)_p&#125; \right)\\&amp;=\sum_&#123;p\in\PP_1&#125; \Psi_\mu\left( \sqrt&#123;(\Au v)_p&#125;+ \frac&#123;(\Au h)_p&#125;&#123;2\sqrt&#123;(\Au v)_p&#125;&#125; +o\left(|(\Au h)_p|\right)\right)\\&amp;=\sum_&#123;p\in\PP_1&#125; \Psi_\mu\left( \sqrt&#123;(\Au v)_p&#125;\right) \\&amp;\qquad+ \Psi&apos;_\mu\left( \sqrt&#123;(\Au v)_p&#125; \right)\frac&#123;(\Au h)_p&#125;&#123;2\sqrt&#123;(\Au v)_p&#125;&#125;+o\left(|(\Au h)_p|\right)\\ \end&#123;aligned&#125;\end&#123;equation&#125; split 1234567\begin&#123;equation&#125; \begin&#123;split&#125; (a + b)^3 &amp;= (a + b) (a + b)^2 \\ &amp;= (a + b)(a^2 + 2ab + b^2) \\ &amp;= a^3 + 3a^2b + 3ab^2 + b^3 \end&#123;split&#125;\end&#123;equation&#125; 公式缩进+\!+ 公式大括号12345\begin&#123;equation&#125;\label&#123;&#125;(u^*(v))_p = \left\&#123;\begin&#123;array&#125;&#123;ll&#125;\frac&#123;1&#125;&#123;2 \sqrt&#123;(\Au v)_p&#125;&#125; &amp;\mbox&#123;, if &#125; \sqrt&#123;(\Au v)_p&#125; \geq \mu,\\\frac&#123;1&#125;&#123;2\mu&#125;&amp;\mbox&#123;, if &#125; \mu \geq \sqrt&#123;(\Au v)_p&#125;\geq 0.\end&#123;array&#125;\right.\end&#123;equation&#125; 引用 同一处引用多个参考文献 方法一：加包 \usepackage{cite} ,处理多个文献命令：\cite{name1,name2,…,nameN} 方法二：不加包，\cite{name1},\cite{name2}. 数学公式的引用 加载 amsmath 工具包，使用 \eqref 命令。效果如下： As the choice of parameter in the TV model (3) latex引用各种公式、文献 算法 以下不需要algorithm package 12345678910111213141516171819202122232425%\usepackage&#123;algorithm&#125;%\usepackage&#123;algorithmic&#125;\begin&#123;algorithm&#125;[ht] % \SetLine \KwIn&#123; \\ $I$: Degraded image\\ $H$: Linear operator to invert\\ $\BB$: Support of the weights \\ $\Ne$: Support for the finite differences defining $R$\\ $\gamma$, $\mu$, $\lambda/\tau$: parameters &#125; \KwOut&#123;\\ $u$: Restored image \\ $v$: Weights \\\ \\ &#125; \Begin&#123; Initialize the image $u$ and $v$;\\ \While&#123; not converged &#125; &#123;step 1: Update $u$: \begin&#123;equation&#125;\label&#123;&#125;u = \Prox&#123;L&#125;&#123;D&#125;&#123; u- L^&#123;-1&#125; \nabla_u \TV&#123;v&#125;&#123;u&#125; &#125;\end&#123;equation&#125; \\ step 2: Update $v$:$v=&#123;\prox&#125;^&#123;\one&#123;\UU^&#123;\PP&#125;&#125;&#125;\left(v-\frac&#123;\nabla_&#123;v&#125;\TV&#123;v&#125;&#123;u&#125;+\nabla R(v)&#125;&#123;l+l&apos;&#125;\right)$ &#125;&#125;\caption&#123;Overview of the algorithm&#125;\label&#123;algo:1&#125;\end&#123;algorithm&#125; 定理、推论 proposition 123456789101112131415161718192021222324252627\newtheorem&#123;prop&#125;&#123;Proposition&#125;----------------------------\begin&#123;prop&#125; \label&#123;propTVu&#125;For any $u\in \RP$, we have\begin&#123;equation&#125;\label&#123;&#125;\nabla_u \TV&#123;v&#125;&#123;u&#125; = \Dv^* w^*(u),\end&#123;equation&#125;where $w^*(u)\in\RR^\PB$ is the maximizer of \eqref&#123;TV-Max&#125; and is provided in closed form by:\begin&#123;equation&#125;\label&#123;&#125;w^*(u)_&#123;p,q&#125; = \left\&#123;\begin&#123;array&#125;&#123;ll&#125;\frac&#123;(\Dv u)_&#123;p,q&#125;&#125;&#123;\mu&#125; &amp; \mbox&#123;, if &#125; \|(\Dv u)_p\| \leq \mu,\\\frac&#123;(\Dv u)_&#123;p,q&#125;&#125;&#123;\|(\Dv u)_p\| &#125; &amp; \mbox&#123;, otherwise.&#125;\end&#123;array&#125;\right.\end&#123;equation&#125;Moreover, $u\rightarrow \nabla_u \TV&#123;v&#125;&#123;u&#125;$ is Lipschitz continuous with Lipschitz constant\begin&#123;equation&#125;\label&#123;&#125;l&quot; = \frac&#123;\sqrt&#123;2&#125; \sqrt&#123;|\BB| +1&#125;&#125;&#123;\mu&#125;.\end&#123;equation&#125;As a consequence, we have for any $u$ and $u&apos;\in \RP$\begin&#123;equation&#125;\label&#123;QMTV-prop&#125;\begin&#123;aligned&#125;\TV&#123;v&#125;&#123;u&apos;&#125;&amp;\leq\TV&#123;v&#125;&#123;u&#125; \\&amp;\quad+ \PS&#123;\nabla_u \TV&#123;v&#125;&#123;u&#125;&#125;&#123;u&apos;-u&#125;\\&amp;\quad+\frac&#123;l&quot;&#125;&#123;2&#125;\|u&apos;-u\|^2.\end&#123;aligned&#125;\end&#123;equation&#125;\end&#123;prop&#125; 页面编辑 Latex添加新一页——用“\clearpage” 不要用“\newpage”。通俗点讲就是当你新加的一页内容较多时，两者基本一样，当新加的一页内容较少时，“\newpage”就无法实现你想要的效果，但“\clearpage”可以。 中文编译sharelatex官方文件 点击menu，将编译器转换XeLatex Error和WarningLatex中查看高亮error和warning 命令行注释 多行注释：使用\usepackage{verbatim}宏包，然后在待注释的部分上加入\begin{comment} … \end{comment}\begin{comment} … \end{comment}，那么中间的部分即被注释掉； 使用\iffalse …. \fi ，那么中间被包含的部分就被注释掉了； 双栏公式默认不居中，不要使用.. 选择单栏或者双栏插入：表格、图片等 Debug begin{equation*} 环境不存在的问题 \usepackage{amsmath} \bibliography{reference.bib}没有反应的时候 可能是因为没有定义引用的格式 \bibliographystyle{spbasic} % basic style, author-year citations\bibliographystyle{spmpsci} % mathematics and physical sciences\bibliographystyle{spphys} % APS-like style for physics 图片和表格引用的时候显示不对 Figure \ref{}; Table \ref{} 解决：因为把\label插在了\caption之前，默认需要在\caption之后。 图片插入到了reference中 将htbp转化成H, 不可以是h. 如：\begin{figure}[H] 需要头文件：\usepackage{float} Package Inputenc error : Unicode Char fi(U+FB01) [duplicate] “fi”被打在同一个字符下 subcaption documentclass{}下的\journalname LaTeX Warning: Reference `Split Bregman’ on page 13 undefined on input line 62 ! Undefined control sequence.或者incomplete \ifodd , all text was ignored… 可能是因为不完整的括号 在winedt中，多余的括号编辑时显示是红色的 ! LaTeX Error: Bad math environment delimiter. 数学环境重复：如$$和\(都是表示公式 Overflow: 原因是遇到一些系统无法为他分割的单词，如schemes, 因而溢出。 解决办法：强制分词。如，sche-mes. latexmk: failure in some parts of making files 在编译文件夹内有多余的组建文件，如有三个.bst文件，即便没有调用另外两个，依然报错。]]></content>
      <categories>
        <category>软件</category>
      </categories>
      <tags>
        <tag>latex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习网络学习]]></title>
    <url>%2Fp%2Fb08c.html</url>
    <content type="text"><![CDATA[网络架构学习 课程链接-tensorflow中文版教程 最新资讯和链接近期必读的10篇ACL 2019【图神经网络（GNN）+NLP】相关论文和代码 卷积神经网络CNN 文献 AlexNet VGG ResNet He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. InProceedings of the IEEE conference on computer vision and pattern recognition 2016 (pp. 770-778). Alex netAlex net之前的工作以及AlexNet中的创新点 论文笔记 VGGRESNETResNet, 2015, 微软亚洲研究院的何凯明等人提出 He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. InProceedings of the IEEE conference on computer vision and pattern recognition 2016 (pp. 770-778). 主要贡献：网络层数从32层提到152层 特点： 网络较瘦，控制了参数数量； 存在明显层级，特征图个数逐层递进，保证输出特征表达能力； 使用了较少的池化层，大量使用下采样，提高传播效率； 没有使用Dropout，利用BN和全局平均池化进行正则化，加快了训练速度； 层数较高时减少了3x3卷积个数，并用1x1卷积控制了3x3卷积的输入输出特征图数量，称这种结构为“瓶颈”(bottleneck)。 残差单元： 克服了梯度消失的问题 卷积神经网络中的组成步长和补零 卷积核：三维结构 池化层池化层的输入一般来源于上一个卷积层，主要作用是提供了很强的鲁棒性（例如max-pooling是取一小块区域中的最大值，此时若此区域中的其他值略有变化，或者图像稍有平移，pooling后的结果仍不变），并且减少了参数的数量，防止过拟合现象的发生。池化层一般没有参数，所以反向传播的时候，只需对输入参数求导，不需要进行权值更新。 Inception模块 bottle layer的作用循环神经网络RNN 4.30 阅读 Pytorch Example: A character-level RNN 识别名字的种类 前馈神经网络的局限 节点之间没有联系 输入和输出的维度固定。无法处理变长的序列数据？维度改变的时序问题 时序的长度是什么？一个字符串的长度？ 每次输出只依赖于当前输入 循环神经网络的难点 按照时序反向传播的时候存在梯度爆炸和梯度消失。解决办法：引入门控机制。 A character-level RNN 强化学习对抗神经网络GAN 阅读 什么是生成式对抗网络GAN 2017王飞跃等：生成式对抗网络 GAN 的研究进展与展望https://www.msra.cn/zh-cn/news/features/gan-20170511) 2019论文：图像补丁躲过图像识别（19.05.02）code 生成对抗网络的工作原理：一个是摄影师（男生），一个是摄影师的女朋友（女生）。男生一直试图拍出像众多优秀摄影师一样的好照片，而女生一直以挑剔的眼光找出“自己男朋友”拍的照片和“别人家的男朋友”拍的照片的区别。于是两者的交流过程类似于：男生拍一些照片 -&gt;女生分辨男生拍的照片和自己喜欢的照片的区别-&gt;男生根据反馈改进自己的技术，拍新的照片-&gt;女生根据新的照片继续提出改进意见-&gt;……，这个过程直到均衡出现：即女生不能再分辨出“自己男朋友”拍的照片和“别人家的男朋友”拍的照片的区别。 生成对抗网络的工作原理：以图像生成模型举例。假设我们有一个图片生成模型（generator），它的目标是生成一张真实的图片。与此同时我们有一个图像判别模型（discriminator），它的目标是能够正确判别一张图片是生成出来的还是真实存在的。那么如果我们把刚才的场景映射成图片生成模型和判别模型之间的博弈，就变成了如下模式：生成模型生成一些图片-&gt;判别模型学习区分生成的图片和真实图片-&gt;生成模型根据判别模型改进自己，生成新的图片-&gt;····（训练过程由生成模型和判别模型组成） 最简单的数据生成模型：如果有数据集S={x1，…xn}，假设这些数据的分布P{X}服从g(x;θ)，在观测数据上通过最大化似然函数得到θ的值，即最大似然法：\max _{\theta} \sum_{i=1}^{n} \log g\left(x_{i} ; \theta\right)。当这个生成模型是神经网络的时候，就是生成式对抗网络（GAN） 文献综述 网络绘图画图软件 Visio NN-SVG，适合卷积神经网络, 在线链接 PlotNeuralNet，基于Latex，github的链接，导出pdf 提供很多模版的在线画图网页 神经网络的例子 Edge detection: HED multi-scale: 不同大小的卷积核 multi-level: receptive field: 网络层次越深，越大，越抽象 weighted-fusion layer: MSRB MSRB for SR MSRB for denoising SR: MSRN 常见问题 DNN（Deep Neural Network）和Deep CNN的区别 过去传统的神经网络ANN（Artifical Neural Network），都是层次较少的网络型结构，所以又被称为浅层网络（shallow neural network），DNN与传统SNN的区别就在于其网络层次结构更多，等复杂，因此由于其层次更多，在图论上说就是图的深度更深，所以被冠名为深度神经网络（Deep Neural Network) Deep CNN是使用了卷积的网络。DNN是最基本的网络结构。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>网络结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pytorch学习笔记]]></title>
    <url>%2Fp%2Fef8a.html</url>
    <content type="text"><![CDATA[Pytorch学习笔记——随时更新 Pytorch环境 查看pytorch版本 1234&gt;&gt;&gt; import torch&gt;&gt;&gt; print(torch.__version__)0.4.0# 更新之后 更新pytorch 1conda update pytorch torchvision Backward Propagation 算法 机器学习中的线性代数之矩阵求导 向量，标量对向量求导数 GPU 查看cuda以及pytorch版本 123print(torch.cuda.current_device()) import pytorchprint(torch.__version__) 指定cuda 指定torch的设备 12345678torch.device('cuda',0) or torch.device('cuda:0') orvar = torch.device('cuda:0')a = torch.randn((2,3),device=var)b = torch.randn(2,3).to_sparse().requires_grad_(True)torch.randn((2,3), 'cuda:1'), cuda1= torch.device('cuda:1')， torch.randn((2,3), device=cuda1) 数据迁徙：将Pytorch模型从CPU转换成GPU 数据处理torch.ToTensor：将[0,255]之间的图像或者numpy转化成[0,1]之间的tensor。 torch.from_numpy：将numpy转化成tensor, 并不会改变数据大小。 Tensor张量 通过pytorch创建张量 123x = torch.randn(M,N）.type(dtype)# 原来是通过numpy创建数组, numpy提供了很多矩阵数学计算x = np.random.randn(N, I) 将张量放在GPU上 张量在cpu和gpu之间的转换 从cpu –&gt; gpu，使用data.cuda()从gpu –&gt; cpu，使用data.cpu() 张量之间的转换 tensor的创建 直接创建： 12torch.tensor([..]).type(..)torch.FloatTensor() 创建随机矩阵： 123dtype = torch.Floatensor dtype = torch.cuda.Floatensor torch.randn((M,N),type(dtype) 从其他数据类型: 12data = [[1,2],[3,4]]tensor = torch.FloatTensor(data) 找出其中最大元素 torch.max(input) 矩阵操作 增加维度 减少维度 torch的矩阵操作 Pytorch可视化利用tensorboard画图 通过logger文件可视化训练过程、官网、别人写的tensorflow下比较具体的可视化 出现的问题 端口冲突通过指定端口解决： 12tensorboard --logdir=/tmp --port=8008 #绝对路径tensorboard --logdir=./tmp --port=8008 #相对路径 tensorbard命令无法找到： 1python3 -m tensorboard.main --logdir=~/my/training/dir 进入目录： 123pip show tensorflowcd /home/abc/xy/.local/lib/python2.7/site-packagespython main.py --logdir=/path/to/log_file/ 路径的名称要小心，路径得是根目录, 不需要引号 1yyfang@mai:~$ ge/ERRNet_Code/logs --port=8008 可视化 1234567891011121314151617181920212223242526272829303132if iteration % 100 == 0: print("===&gt; Epoch[&#123;&#125;](&#123;&#125;/&#123;&#125;): Loss: &#123;:.6f&#125;".format(epoch, iteration, len(training_data_loader),loss.data.item())) info = &#123; 'loss': loss.data.item()&#125; itera = (epoch-1)*len(training_data_loader)+iteration for tag, value in info.items(): logger.scalar_summary(tag, value, itera) for tag, value in model.named_parameters(): # print(value.grad) logger.histo_summary(tag, to_np(value), itera) logger.histo_summary(tag+'/grad', to_np(value.grad), iteration) images = input * 255. # ?[0,1]?????[0,255]?? images[images &lt; 0] = 0 images[images &gt; 255.] = 255. a =to_np(images.view(-1, 64, 64)[:8]) # info_input = &#123;'input': to_np(images.view(-1, 64, 64)[:2])&#125; imagelabel = label * 255. # ?[0,1]?????[0,255]?? imagelabel[imagelabel &lt; 0] = 0 imagelabel[imagelabel &gt; 255.] = 255. b = to_np(imagelabel.view(-1, 64, 64)[:8]) c = np.hstack((a,b)) # print(images) info_label = &#123;'inpput/label': c&#125; for tag, images in info_label.items(): logger.image_summary(tag, images, itera) # for tag, images in info_label.items(): # logger.image_summary(tag, images, iteration) if iteration % 5000 == 0: number = opt.number save_checkpoint_iter(model, number) opt.number += 1 数据集 训练集(train set) 验证集(validation set) 测试集(test set） training set： 用来训练模型 validation set : 用来做model selection（往往我们需要对多种模型进行训练，训练完之后就会得到多个模型的结果，我们希望从这些训练好的模型中选择最适合的模型） test set : 用来评估所选出来的model的实际性能 几个常用数据集 BSD500： DIV2K： 函数 矩阵相乘/点乘 123456789data = [[1,2],[3,4]]tensor = torch.FloatTensor(data)# numpynp.matmual(data, data)# tensortorch.mm(tensor, tensor) # 矩阵相乘tensor.mm(tensor.t())torch.mul(tensor,tenor.t()) #点乘tensor.mul(tensor) 将数组截取到一个区间 12h_relu = h.clamp(min=0)h_relu2 = torch.clamp(h, min=0) To_np函数: 转化数据类型 包含在库fastai中 1from fastai.basics import * 网络结构-nn包torch_nn中文文档 torch.nn的线形层 torch.nn的卷积层 卷积核的大小: [out_Channel, in_Channel, kernel_size, kernel_size] bias的大小:[out_Channel] 继承nn.modle自定义模块 问题 如何初始化model中的参数 为什么需要model.zero_grad之后再backward(), 例如 model.zero_grad() optimizer.zero_grad() 优化器optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate) 损失函数 如何自定义损失函数 并行运算Parallelpytorch中如果使用DataParallel，那么保存的模型key值前面会多出’modules.’，这样如果训练的时候使用的是多GPU，而测试的时候使用的是单GPU，模型载入就会出现问题。放到cpu不会有问题 1.nn.para 2..todevice() 图像处理【pytorch】图像基本操作 例子：设置GPU和预训练模型1234567891011121314151617181920212223242526272829 criterion = nn.MSELoss(size_average=False)# 设置GPU print("===&gt; Setting GPU") if cuda: # 调用多个GPU model = nn.DataParallel(model, device_ids=[0, 1, 2, 3]).cuda() # 调用单个GPU model = model.cuda() criterion = criterion.cuda() else: model = model.cpu() # 加载预预训练好的模型及权重 if opt.pretrained: if os.path.isfile(opt.pretrained): print("=&gt; loading model '&#123;&#125;'".format(opt.pretrained)) weights = torch.load(opt.pretrained) model.load_state_dict(weights['model'].state_dict()) else: print("=&gt; no model found at '&#123;&#125;'".format(opt.pretrained)) # 保存模型的参数 # Save checkpoint save_file = os.path.join(TMP_DIR, 'checkpoint_epoch&#123;&#125;.pth'.format(epoch)) save_checkpoint(&#123; 'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict() &#125;, filename=save_file) 自定义LossFunction Pytorch如何自定义损失函数 GPU计算加速01 : AI时代人人都应该了解的GPU知识 GPU加速02:超详细Python Cuda零基础入门教程，没有显卡也能学！ 自定义LossTV 123456789101112131415161718192021222324252627282930import torchimport torch.nn as nnfrom torch.autograd import Variableclass TVLoss(nn.Module): def __init__(self,TVLoss_weight=1): super(TVLoss,self).__init__() self.TVLoss_weight = TVLoss_weight def forward(self,x): batch_size = x.size()[0] h_x = x.size()[2] w_x = x.size()[3] count_h = self._tensor_size(x[:,:,1:,:]) count_w = self._tensor_size(x[:,:,:,1:]) h_tv = torch.pow((x[:,:,1:,:]-x[:,:,:h_x-1,:]),2).sum() w_tv = torch.pow((x[:,:,:,1:]-x[:,:,:,:w_x-1]),2).sum() return self.TVLoss_weight*2*(h_tv/count_h+w_tv/count_w)/batch_size def _tensor_size(self,t): return t.size()[1]*t.size()[2]*t.size()[3]def main(): x = Variable(torch.FloatTensor([[[1, 2, 3], [2, 3, 4], [3, 4, 5]], [[1, 2, 3], [2, 3, 4], [3, 4, 5]]]).view(1, 2, 3, 3),requires_grad=True) addition = TVLoss() z = addition(x) print x print z.data z.backward() print x.grad Debug Anaconda中，import和实际的不符 Anaconda清除历史，否则同一个格子刷新，并不会影响前面已经import的内容 torch版本带来的bug loss.data[0] -&gt;loss.data.item() Mac air(未解决) 载入模型参数时报错（在ubuntu下不报错，ubuntu的版本是1.0.1.post2, air下是0.4.0） 1AttributeError: Can&apos;t get attribute &apos;_rebuild_parameter&apos; on &lt;module &apos;torch._utils&apos; from &apos;/Users/yingyingfang/anaconda3/lib/python3.6/site-packages/torch/_utils.py&apos;&gt; Ubuntu下训练RCF模型时报错（解决） 12345expected object of backend cpu but got backend cuda for argument #2 'weight'# 解决办法input=input.cuda()net = net.cuda()net(input) 如何提高训练速度 并行运算 RuntimeError: Input type (torch.cuda.DoubleTensor) and weight type (torch.cuda.FloatTensor) should be the same 产生原因：创建了一个高斯随机分布之后？ 解决办法：将数据类型转换成FloatTensor即可，如下，加一行代码 123train_label_batch = torch.from_numpy(train_label_batch)train_label_batch = train_label_batch.type(torch.FloatTensor) # 转Floattrain_label_batch = train_label_batch.cuda() # 转cuda RuntimeError: all tensors must be on devices[0] 问题 如何指定gpu训练 如何multi-gpu训练]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu服务器]]></title>
    <url>%2Fp%2F6fe1.html</url>
    <content type="text"><![CDATA[服务器学习 配置终端命令行内关键字回溯历史命令 让你的iTerm更Geek! 快捷键在服务器里切换界面：alt+tab 链接使用命令的命令。 看完这篇Linux基本的操作就会了 https://github.com/jlevy/the-art-of-command-line/blob/master/README-zh.md 一个学习linux的网站 命令行清单 定义变量1your_name=&quot;qinjx&quot; 用引号，等号旁边不能有空格。否则会被看成无法识别的命令 命名只能使用英文字母，数字和下划线，首个字符不能以数字开头。中间不能有空格，可以使用下划线（_）。不能使用标点符号。不能使用bash里的关键字（可用help命令查看保留关键字） 搜索 搜索文件夹内出现的字符串 find和grep的区别 12grep -n "get_spg2lsf" -r ./grep -n "data_test" -r ./ 12345678910111213141516171819202122232425262728293031323334353637383940 该命令会查找当前目录及其子目录下所有包含指定字符串的文件，会列出文件位置、该行的内容以及行号。 # 文件处理## 显示隐藏文件ls -a桌面面可视化窗口，进入ctrl + h ，则显示隐藏文件## 创建目录Mkdir## 复制删除文件cprm##下载文件ftp下载​ 如果下载[ftp服务器](https://www.baidu.com/s?wd=ftp%E6%9C%8D%E5%8A%A1%E5%99%A8&amp;tn=24004469_oem_dg&amp;rsv_dl=gh_pl_sl_csd)上的文件，可以用ftp命令。然后用get命令下载文件网址下载​```bashwget http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip1.Wget常用参数 ◆-b：后台下载，Wget默认的是把文件下载到当前目录。 ◆-O：将文件下载到指定的目录中。 ◆-P：保存文件之前先创建指定名称的目录。 ◆-t：尝试连接次数，当Wget无法与服务器建立连接时，尝试连接多少次。 ◆-c：断点续传，如果下载中断，那么连接恢复时会从上次断点开始下载。 ◆-r：使用递归下载 git下载 123git clone &lt;https://github.com/LimBee/NTIRE2017.git&gt; 解压文件 Unzip https://www.cnblogs.com/chinareny2k/archive/2010/01/05/1639468.html) test 查看动态文件tail命令 12341、tail -f filename说明：监视filename文件的尾部内容（默认10行，相当于增加参数 -n 10），刷新显示在屏幕上。退出，按下CTRL+C。2、tail -n 20 filename说明：显示filename最后20行。 文件编辑vim在vi中按u可以撤销一次操作; Ctrl+r 恢复上一步被撤销的操作 vim常用命令之多行注释和多行删除 文件内搜索linux 查找某目录下包含关键字内容的文件 进程 命令 fg、bg、jobs、&amp;、ctrl + z都是跟系统任务有关的，虽然现在基本上不怎么需要用到这些命令，但学会了也是很实用的 1.&amp; 最经常被用到 这个用在一个命令的最后，可以把这个命令放到后台执行 2.ctrl + z 可以将一个正在前台执行的命令放到后台，并且暂停 3.jobs 查看当前有多少在后台运行的命令 4.fg 将后台中的命令调至前台继续运行 如果后台中有多个命令，可以用 fg %jobnumber将选中的命令调出，%jobnumber是通过jobs命令查到的后台正在执行的命令的序号(不是pid) 5.bg 将一个在后台暂停的命令，变成继续执行 如果后台中有多个命令，可以用bg %jobnumber将选中的命令调出，%jobnumber是通过jobs命令查到的后台正在执行的命令的序号(不是pid) 后台执行进程linux后台执行命令：&amp;和nohup command &amp;：关掉屏幕，进程结束，不占用屏幕而已 nohup commnd &amp;：真正在后台执行 PS：需要用户交互的命令不要放在后台执行。 有大量的输出，就进行重定向。 12yyfang@mai:~/Documents/Deeplearning_edge/ERRNet_edge/ERRNet_Code$ nohup python main.py --cuda --dataset="../../../dataset/data_edge_35.h5" &gt;result.txt 2&gt;&amp;1 &amp;[1] 22223 显示最后的十行 监视进程https://blog.csdn.net/shenhuan1104/article/details/75808146 查看进程 ps -aux | grep xrdp 杀死进程 $ kill -s 9 进程号 管道符“|”用来隔开两个命令，管道符左边命令的输出会作为管道符右边命令的输入。 把ps的查询结果通过管道给grep查找包含特定字符串的进程。 $ ps -ef | grep firefox 监视GPU3.监视GPU的使用情况 12$ nvidia-smi $ watch -n 10 nvidia-smi %10s钟输出一次 指定GPU 启用相关软件和程序matlabhttps://blog.csdn.net/u013066730/article/details/80944063 命令行之行matlab 1234562.运行m文件如果m文件名为matlabfile.m(1)方法一进入m文件所在目录后，运行$ matlab -nodesktop -nosplash -r matlabfile只用文件名matlabfile，不能添加.m 更改快捷键 anaconda在终端输入anaconda-navigator 退出base 执行python参数输入 Positional argument v.s. keyword argument In other words, keyword arguments are only “optional” because they will be set to their default value if not specifically supplied. 多参数输入 远程连接xrdp 服务相关 SSD管理预处理数据的时候，要检查硬盘是否有足够大的空间 df -h GPU由显卡和GPU组成 相当于内存和CPU的区别 当显卡内存不够时，和batchsize有关和数据集的大小没有关系，一块显卡，64*64的batchsize=16而不能设置成32 环境变量配置anaconda环境变量，在终端输入： 1export PATH=~/anaconda/bin:$PATH 显示当前conda版本信息，在终端输入： 1conda --version 之后我们再次输入命令列出Anaconda自带的包，在终端输入： 1conda list 常见问题 远程桌面死机 知道服务器远程桌面的服务所使用的协议，关掉相应进程即可。 不知道的情况下，可猜测对方所使用的进程（例如猜测使用xrdp） 如果是xrdp的话这行命令应该会有超过一个的结果：ps -aux | grep xrdp yyfang@mai:~$ kill 22722 # 对应的进程是启动桌面 查看服务器容量 如何调出这个显示，代表的是什么容量，应该没有包括数据的大小 Linux du 命令 du -sh 查看当前目录的大小 du -h test 方便阅读的格式显示test目录所占空间情况 12345678910111213141516# du log2012.log 300 log2012.log# du -h test608K test/test6308K test/test44.0K test/scf/lib4.0K test/scf/service/deploy/product4.0K test/scf/service/deploy/info12K test/scf/service/deploy16K test/scf/service4.0K test/scf/doc4.0K test/scf/bin32K test/scf8.0K test/test31.3M test# du -sh * 显示一级目录 服务器上文件Dataset网上下载的一些数据集 train: 生成的一些.h5训练数据 test.h5 用ljc的边缘，未归一化，用了400张图 poisson10.h5 noiselevel=10的泊松噪声 Edge_net generate_data: 生成.h5数据 Loggers05 Poisson10_… 10:22PM开始 /python/compare_models: 不同的.pth 保存文件]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>服务器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[富士X100：慢拍的优雅]]></title>
    <url>%2Fp%2Fb371.html</url>
    <content type="text"><![CDATA[相机成像原理小孔成像：进光、倒立 孔太大了无法成像，每个点包含了整个成像物； 小孔变小，图像会更清晰，光线变暗 相机的光圈类比于小孔 闪光灯的作用 在光线充足的地方：增加光效，突出立体感 在光线不足的地方：补光 逆光时：人物的面部光线更为柔和 X100下具有自动曝光功能。菜单的位置：闪光灯的自动模式，闪光灯的力度大小，配合闪光灯的消除红眼 测光模式 X100具有多重、点测光、平均三种测光模式。 多重测光是点和平均的中和。夜晚时，画面容易偏亮。 点测光的特点：适合拍摄主题明显的画面，营造氛围。当测光在亮部时，整体会偏暗；当测光在暗部时，整体会偏亮。(点测光下，可以改变测光点吗：半自动测光） 平均测光的特点：适合拍摄静物和风景。测光平均值比较稳定。画面容易偏暗 注意：MF模式下，测光方式不影响曝光画面。由光圈快门决定。 曝光补偿 增加曝光补偿： 例子： 降低曝光补偿： 白平衡 作用：增加氛围；中和不正常的光线 几种调节方式：自动白平衡、自定义白平衡、白平衡偏移 白平衡偏移： 不同场景的拍摄夕阳的拍摄技巧 色彩的学习 课堂链接 色彩的饱和度：所含彩色强度的浓度。 色彩的明度：色彩的亮度。其中黄色明度最高，紫色明度最低，绿、红、蓝、橙的明度相近，为中间明度。 色彩的搭配方式 单色、对比色、分离补色、冷暖色 问题 全画幅和半画幅的区别]]></content>
      <categories>
        <category>摄影</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>%2Fp%2F0.html</url>
    <content type="text"><![CDATA[读取和写入write 写表格 A = table([‘a’;’b’;’c’;’d’;’e’],PSNR); writetable(results,’results of noise.txt’); 数据保留小数变量名和字符串之间的转化Matlab变量名与字符串的互相转换 matlab 字符串拼接并转换为变量名字 图片显示 获取图片中像素点的坐标 123%1.1 显示图片mainf=imshow(data); %show the picture%1.2 命令框中输入impixelinfo，点击图片即可 标题中添加latex符号 1title('TV \lambda=0.07'); Matlab中插入latex 图像处理 将灰度图转化成[0,1]区间的图像 It = im2double(uint8(I)); j=加噪声 imnoise这个函数输入的图像需要归一化 BM3D: z = y + sigma/255) 加载图像至$[0,1]$ y = im2double(imread(image)) 12345678910111213141516171819202122232425262728# oursnoise_level=15;g = double(imnoise(uint8(ftrue),'gaussian',0, noise_level^2/255^2));psnr(ftrue,g,255) #[0,255]之间的double# BM3Dsigma = 15y = im2double(uint8(image_name));z = y + (sigma/255)*randn(size(y));psnr(ftrue,g) #[0,1]之间的double# Non-localI = imread('cameraman.tif');noisyImage = imnoise(I,'gaussian',0,0.0015);[filteredImage,estDoS] = imnlmfilt(noisyImage);montage(&#123;noisyImage,filteredImage&#125;)title (['Estimated degree of smoothing, ', 'estDoS = ',num2str(estDoS)]);# Ljzhi NLTVsigm_array= [0.1 0.16 0.2]; # fingerprint[in_clean,map] = imread([fileName,'.png']);in_clean = double(in_clean); in_clean = in_clean/ max(in_clean(:));in_polluted = in_clean + sigm*randn(size(in_clean));# TGVn = 0.1I = im2double(imread(fname)); % 输入必须是uint8I = imnoise(I, 'gaussian', 0, n^2); % I = I + n*randn()sigma/255 = sigm = n 文件操作 将文件夹下的文件加到路径下 123addpath(genpath('matlab'))addpath Functionsaddpath Functions/WaveletFunctions 将文件夹当作类 寻找文件夹内指定文件 文件名组合 imgName = [‘img_’, num2str(imgID, ‘%03d’), ‘_SRF_4_LR.png’]; 保存加载123# 加载单个变量load handel.mat yload('handel.mat','y') 变量 MATLAB中控制输出格式中小数点后的位数(未解决) https://blog.csdn.net/xiakexiaohu/article/details/53760946 可以改变到某一精度，但是显示的时候，但是同时产生了3*13 sym 画图Matlab可视化编程 Excel画图 直方图stem、hist、imhist 123x = linspace(-1, 1, (256*2))';y = hist(edge(:),x)'; % 按照x进行统计，将二维数组转化成一维向量figure, stem(x,y, 'Marker', 'none') 三维图像123456789101112x=100:10:2000;y=x;[X,Y]=meshgrid(x,y); %网格化x、yZ=X.*(1-Y./(X+Y)); %计算Zmesh(X,Y,Z); %画出图形zmax=max(max(Z)); %找出Z的最大值zmax[id_ymax,id_xmax]=find(Z==zmax);xmax=x(id_xmax);ymax=y(id_ymax); %找出Z的最大值对应的横纵坐标xmax、ymaxhold onplot3(xmax,ymax,zmax,'k.','markersize',20) %标记一个黑色的圆点text(xmax,ymax,zmax,[' x=',num2str(xmax),char(10),' y=',num2str(ymax),char(10),' z=',num2str(zmax)]); %标出坐标 Debug str数组 str = [‘noise15’,’noise35’,’noise50’] str(1,:) = ‘noise15’而不是str(0) = ‘n’ 问题 [x] 出现’._200.png’类文件. Invisible files with “.” prefix are created on some shared volumes and external disks [ ] 保存 ‘.mat’ 12save test.mat X % command formsave('test.mat','X') % function form 图片：eps格式 12345678fileName = 'FarmerStats'; % your FILE NAME as stringA = imread(fileName,'png');set(gcf,'visible','off') %suppress figureimage(A); axis image % resolution based on imageaxis off % avoid printing axis set(gca,'LooseInset',get(gca,'TightInset')); % removing extra white space in figuresaveas(gcf,fileName,'epsc'); % save as COLOR eps file 或使用：export_fig包]]></content>
  </entry>
  <entry>
    <title><![CDATA[Mac快捷键以及一些快速操作]]></title>
    <url>%2Fp%2F5d54.html</url>
    <content type="text"><![CDATA[快捷打开方式终端 打开Mac下自带的软件 Automator 新建文稿 创建一个服务 修改框内的脚本 123456on run &#123;input, parameters&#125; tell application &quot;Terminal&quot; reopen activate end tellend run 运行：command + R，如果没有问题，则会打开终端 保存：Command + S，将其命名为打开终端或你想要的名字 设置快捷键 在 系统偏好设置 -&gt; 键盘设置 -&gt; 快捷键 -&gt; 服务 选择我们创建好的 ‘打开终端‘，设置你想要的快捷键，比我我设置了⌘+空格 到此，设置完成。 聪明的你也许会发现，这个技巧能为所有的程序设置快捷启动。 将脚本中的 Terminal 替换成 其他程序就可以 123456on run &#123;input, parameters&#125; tell application &quot;Terminal&quot; reopen activate end tellend run 黑技能既然学了 Automator ，那就在附上一个黑技能吧。为你的代码排序。在 Xcode8以前，有个插件能为代码快速排序，不过时过境迁~ 对于没用的插件而且又有患有强迫症的的小伙伴，只能手动排序了（😂）. 首先还是创建一个服务 创建一个Shell脚本， 勾选:用输出内容替换所选文本 输入：sort|uniq 保存： 存为Sort &amp; Uniq 选中你的代代码 -&gt; 鼠标右键 -&gt; Servies -&gt; Sort&amp;Uniq 排序后的代码： Mac的一些快捷操作 增加文件的快捷方式 按下option+cmd，将文件拖到桌面，产生有箭头标记的文件夹。 谷歌浏览器打开刚关闭的页面: cmd+shift+T Terminal 删除文件夹（无论文件夹是否为空），使用 -rf 命令即可。 使用这个rm -rf的时候一定要格外小心，删除之后没办法在垃圾站找回。 RDP复制粘贴问题https://www.technipages.com/unable-to-copy-and-paste-to-remote-desktop-session windows Atom打开控制面板：cmd-shift-P Latex编译：ctrl-option-B Latex清空： http://mazhuang.org/atom-flight-manual/chapter-2-using-atom/autocomplete.html SnipMac 动态壁纸http://teach.apple543.com/download-dynamic-wallpaper-on-mac/ https://www.newmobilelife.com/2019/05/06/macos-mojave-dynamic-wallpaper/ Dynamic Wallpaper ClubGalleryCreate) VSCodehttps://juejin.im/post/5a08d1d6f265da430f31950e 配置tex https://zhuanlan.zhihu.com/p/38178015 https://www.jianshu.com/p/57f8d1e026f5 Mindnode换行：option+return Bug 出现白条 原因：输入法的bug。在英文输入法的时候输入会直接定位到字母开头的文件，如果中文就会出现白条，然后白条输入完会定位到中文开始的文件。 解决：点击白条一下，按键盘esc。或者重启finder/killall Finder。]]></content>
      <categories>
        <category>软件</category>
      </categories>
      <tags>
        <tag>Mac</tag>
        <tag>效率</tag>
      </tags>
  </entry>
</search>
